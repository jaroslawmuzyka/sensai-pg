# Sensai vibe coding workshop

> ## â€œEveryone is a programmer. Now, you just have to say something to the computer.â€ - Jensen Huang

Podczas tego tygodnia dowiesz siÄ™:

*   jak programowaÄ‡ korzystajÄ…c z edytorÃ³w kodu jak Cursor, Windsurf i platform typu [Bolt.new](http://bolt.new/), czy Lovable
*   jak wybraÄ‡ model do kodowania
*   jak pisaÄ‡ dobre prompty
*   jak korzystaÄ‡ z serwerÃ³w MCP do programowania
*   jak uÅ¼ywaÄ‡ GitHuba
*   jak tworzyÄ‡ ui, np. jako interfejs do automatyzacji w n8n czy do skryptÃ³w python
*   jak wystawiÄ‡ nasz kod do internetu, bezpiecznie wszystko skonfigurowaÄ‡ tak Å¼eby nie naraziÄ‡ siÄ™ na utratÄ™ danych czy niespodziewane rachunki
*   jak tworzyÄ‡ dodatki do przeglÄ…darki

**Czemu uczyÄ‡ siÄ™ programowaÄ‡?**

*   NiezaleÅ¼noÅ›Ä‡ w tworzeniu rozwiÄ…zaÅ„
*   Bo fajnie jest tworzyÄ‡ coÅ› swojego
*   Automatyzacja i oszczÄ™dnoÅ›Ä‡ czasu
*   KonkurencyjnoÅ›Ä‡ na rynku pracy
*   Zrozumienie technologii i programistÃ³w
*   MoÅ¼liwoÅ›Ä‡ budowania wÅ‚asnych rozwiÄ…zaÅ„ pod wÅ‚aasne potrzeby
*   wykorzystanie z pracy innych i dostosowanie jej pod wÅ‚asne wymagania

Bariery techniczne znikajÄ…, co sprawia, Å¼e co raz wiÄ™cej osÃ³b zaczyna programowaÄ‡, i Å¼e zaczyna liczyÄ‡ siÄ™ jeszcze bardziej pomysÅ‚. Jak wiadomo od pomysÅ‚u do wykonania czÄ™sto dÅ‚uga droga, ale nawet jeÅ›li nie chcemy samemu wdraÅ¼aÄ‡ naszych pomysÅ‚Ã³w to Å›wiadomoÅ›Ä‡ moÅ¼liwoÅ›ci jak i ograniczeÅ„ technicznych powinna sprawiÄ‡ Å¼e nasze pomysÅ‚y bÄ™dÄ… jeszcze lepsze, a co za tym idzie zyskujemy przewagÄ™ na tym konkurencyjnym rynku jakim jest SEO. Åšwiat zmienia siÄ™ aktualnie tak szybko Å¼e codziennie pojawiajÄ… siÄ™ nowe problemy do rozwiÄ…zania. PrzedstawiÄ™ Ci narzÄ™dzia i koncepty do realizacji Twoich pomysÅ‚Ã³w skutecznie i bezpiecznie.

# Podstawy

### **ğŸ¤¨ Czym jest programowanie?**

**Programowanie to sposÃ³b tworzenia instrukcji, ktÃ³re mÃ³wiÄ… komputerowi, jak rozwiÄ…zaÄ‡ konkretny problem â€” najczÄ™Å›ciej prowadzÄ…c uÅ¼ytkownika z punktu A do punktu B. MoÅ¼na je porÃ³wnaÄ‡ do przepisu kulinarnego albo instrukcji dojazdu: zawierajÄ… krok po kroku, co ma siÄ™ wydarzyÄ‡.**

![image](image.png)

#### N8N

W trakcie tego kursu zetknÄ™Å‚aÅ› siÄ™ juÅ¼ z formÄ… programowania â€“ automatyzacjami w **n8n**. Tam, zamiast pisaÄ‡ kod, uÅ¼ywamy gotowych moduÅ‚Ã³w, ktÃ³re Å‚Ä…czymy graficznie. To podejÅ›cie jest bardzo wygodne, szczegÃ³lnie gdy zaleÅ¼y nam na **szybkim wdroÅ¼eniu i prostocie**.

#### Python

Teraz pÃ³jdziemy o krok dalej i zajmiemy siÄ™ **programowaniem w Pythonie**. To jÄ™zyk tekstowy, ktÃ³ry daje nam **nieograniczone moÅ¼liwoÅ›ci**, ale wymaga nieco wiÄ™cej nauki. Python jest idealny do zadaÅ„, ktÃ³re sÄ… zbyt zÅ‚oÅ¼one dla n8n, na przykÅ‚ad do zaawansowanej analizy danych, uczenia maszynowego czy budowania niestandardowych narzÄ™dzi.

![image](image%201.png)

#### Wizualizacja procesu

MyÅ›lenie o programowaniu jako o procesie pomaga zrozumieÄ‡ jego logikÄ™. KaÅ¼dy krok w procesie to fragment kodu, ktÃ³ry wykonuje okreÅ›lone zadanie.

![image](image%2010.png)

#### PrzykÅ‚adowy skrypt w Pythonie

To jest przykÅ‚ad kompletnego skryptu w Pythonie, ktÃ³ry pobiera wszystkie linki z mapy strony (sitemap) i zapisuje je do pliku CSV. Nie musisz go teraz w peÅ‚ni rozumieÄ‡ â€” chodzi o to, Å¼eby zobaczyÄ‡, jak wyglÄ…da kod, ktÃ³ry realizuje konkretne zadanie.

```python
import requests
import xml.etree.ElementTree as ET
import csv
from urllib.parse import urljoin, urlparse
import time
from typing import List, Set

class SitemapExtractor:
    def __init__(self, timeout: int = 10):
        """
        Inicjalizuje ekstraktor sitemap
        
        Args:
            timeout: Timeout dla HTTP requestÃ³w w sekundach
        """
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_sitemap(self, url: str) -> str:
        """
        Pobiera zawartoÅ›Ä‡ sitemap z podanego URL
        
        Args:
            url: URL do sitemap
            
        Returns:
            ZawartoÅ›Ä‡ XML jako string
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"BÅ‚Ä…d podczas pobierania {url}: {e}")
            return ""
    
    def parse_sitemap_index(self, xml_content: str) -> List[str]:
        """
        Parsuje sitemap index i zwraca listÄ™ URL-Ã³w do sub-sitemaps
        
        Args:
            xml_content: ZawartoÅ›Ä‡ XML sitemap index
            
        Returns:
            Lista URL-Ã³w do sub-sitemaps
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy rÃ³Å¼ne namespace'y
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            sitemap_urls = []
            
            # Szukamy elementÃ³w sitemap w sitemap index
            for sitemap in root.findall('.//ns:sitemap', namespaces):
                loc = sitemap.find('ns:loc', namespaces)
                if loc is not None and loc.text:
                    sitemap_urls.append(loc.text.strip())
            
            # Fallback - szukamy bez namespace
            if not sitemap_urls:
                for sitemap in root.findall('.//sitemap'):
                    loc = sitemap.find('loc')
                    if loc is not None and loc.text:
                        sitemap_urls.append(loc.text.strip())
            
            return sitemap_urls
            
        except ET.ParseError as e:
            print(f"BÅ‚Ä…d parsowania XML sitemap index: {e}")
            return []
    
    def parse_sitemap_urls(self, xml_content: str) -> List[dict]:
        """
        Parsuje sitemap i zwraca listÄ™ URL-Ã³w z metadanymi
        
        Args:
            xml_content: ZawartoÅ›Ä‡ XML sitemap
            
        Returns:
            Lista sÅ‚ownikÃ³w z URL-ami i metadanymi
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy rÃ³Å¼ne namespace'y
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            urls = []
            
            # Szukamy elementÃ³w url w sitemap
            for url in root.findall('.//ns:url', namespaces):
                url_data = {}
                
                # Lokalizacja (wymagana)
                loc = url.find('ns:loc', namespaces)
                if loc is not None and loc.text:
                    url_data['loc'] = loc.text.strip()
                
                # Ostatnia modyfikacja (opcjonalna)
                lastmod = url.find('ns:lastmod', namespaces)
                if lastmod is not None and lastmod.text:
                    url_data['lastmod'] = lastmod.text.strip()
                
                # CzÄ™stotliwoÅ›Ä‡ zmian (opcjonalna)
                changefreq = url.find('ns:changefreq', namespaces)
                if changefreq is not None and changefreq.text:
                    url_data['changefreq'] = changefreq.text.strip()
                
                # Priorytet (opcjonalny)
                priority = url.find('ns:priority', namespaces)
                if priority is not None and priority.text:
                    url_data['priority'] = priority.text.strip()
                
                if 'loc' in url_data:
                    urls.append(url_data)
            
            # Fallback - szukamy bez namespace
            if not urls:
                for url in root.findall('.//url'):
                    url_data = {}
                    
                    loc = url.find('loc')
                    if loc is not None and loc.text:
                        url_data['loc'] = loc.text.strip()
                    
                    lastmod = url.find('lastmod')
                    if lastmod is not None and lastmod.text:
                        url_data['lastmod'] = lastmod.text.strip()
                    
                    changefreq = url.find('changefreq')
                    if changefreq is not None and changefreq.text:
                        url_data['changefreq'] = changefreq.text.strip()
                    
                    priority = url.find('priority')
                    if priority is not None and priority.text:
                        url_data['priority'] = priority.text.strip()
                    
                    if 'loc' in url_data:
                        urls.append(url_data)
            
            return urls
            
        except ET.ParseError as e:
            print(f"BÅ‚Ä…d parsowania XML sitemap: {e}")
            return []
```
```python
def is_sitemap_index(self, xml_content: str) -> bool:
        """
        Sprawdza czy XML to sitemap index czy zwykÅ‚y sitemap
        
        Args:
            xml_content: ZawartoÅ›Ä‡ XML
            
        Returns:
            True jeÅ›li to sitemap index, False jeÅ›li zwykÅ‚y sitemap
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy czy istniejÄ… elementy sitemapindex
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            # Sprawdzamy z namespace
            if root.find('.//ns:sitemapindex', namespaces) is not None:
                return True
            if root.find('.//ns:sitemap', namespaces) is not None:
                return True
            
            # Sprawdzamy bez namespace
            if root.find('.//sitemapindex') is not None:
                return True
            if root.tag == 'sitemapindex' or 'sitemapindex' in root.tag:
                return True
            
            return False
            
        except ET.ParseError:
            return False
    
    def extract_urls_from_domain(self, domain: str) -> List[dict]:
        """
        WyciÄ…ga wszystkie URL-e z sitemap danej domeny
        
        Args:
            domain: Domena (np. 'https://example.com' lub 'https://example.com/')
            
        Returns:
            Lista sÅ‚ownikÃ³w z URL-ami i metadanymi
        """
        # Normalizujemy domenÄ™
        if not domain.startswith(('http://', 'https://')):
            domain = 'https://' + domain
        
        if not domain.endswith('/'):
            domain += '/'
        
        # Konstruujemy URL do sitemap
        sitemap_url = urljoin(domain, 'sitemap.xml')
        
        print(f"Pobieranie sitemap z: {sitemap_url}")
        
        # Pobieramy gÅ‚Ã³wny sitemap
        xml_content = self.fetch_sitemap(sitemap_url)
        if not xml_content:
            return []
        
        all_urls = []
        
        # Sprawdzamy czy to sitemap index
        if self.is_sitemap_index(xml_content):
            print("Wykryto sitemap index, pobieranie sub-sitemaps...")
            
            # Pobieramy URL-e do sub-sitemaps
            sub_sitemap_urls = self.parse_sitemap_index(xml_content)
            print(f"Znaleziono {len(sub_sitemap_urls)} sub-sitemaps")
            
            # Pobieramy kaÅ¼dy sub-sitemap
            for i, sub_url in enumerate(sub_sitemap_urls, 1):
                print(f"Pobieranie sub-sitemap {i}/{len(sub_sitemap_urls)}: {sub_url}")
                
                sub_xml_content = self.fetch_sitemap(sub_url)
                if sub_xml_content:
                    urls = self.parse_sitemap_urls(sub_xml_content)
                    all_urls.extend(urls)
                    print(f"  Znaleziono {len(urls)} URL-Ã³w")
                
                # MaÅ‚a przerwa miÄ™dzy requestami
                time.sleep(0.1)
        
        else:
            print("Wykryto zwykÅ‚y sitemap, parsowanie URL-Ã³w...")
            # To zwykÅ‚y sitemap, parsujemy URL-e bezpoÅ›rednio
            all_urls = self.parse_sitemap_urls(xml_content)
        
        print(f"ÅÄ…cznie znaleziono {len(all_urls)} URL-Ã³w")
        return all_urls
    
    def extract_urls_from_sitemap_url(self, sitemap_url: str) -> List[dict]:
        """
        WyciÄ…ga URL-e z konkretnego URL sitemap
        
        Args:
            sitemap_url: BezpoÅ›redni URL do sitemap
            
        Returns:
            Lista sÅ‚ownikÃ³w z URL-ami i metadanymi
        """
        print(f"Pobieranie sitemap z: {sitemap_url}")
        
        xml_content = self.fetch_sitemap(sitemap_url)
        if not xml_content:
            return []
        
        all_urls = []
        
        if self.is_sitemap_index(xml_content):
            print("Wykryto sitemap index, pobieranie sub-sitemaps...")
            
            sub_sitemap_urls = self.parse_sitemap_index(xml_content)
            print(f"Znaleziono {len(sub_sitemap_urls)} sub-sitemaps")
            
            for i, sub_url in enumerate(sub_sitemap_urls, 1):
                print(f"Pobieranie sub-sitemap {i}/{len(sub_sitemap_urls)}: {sub_url}")
                
                sub_xml_content = self.fetch_sitemap(sub_url)
                if sub_xml_content:
                    urls = self.parse_sitemap_urls(sub_xml_content)
                    all_urls.extend(urls)
                    print(f"  Znaleziono {len(urls)} URL-Ã³w")
                
                time.sleep(0.1)
        else:
            print("Wykryto zwykÅ‚y sitemap, parsowanie URL-Ã³w...")
            all_urls = self.parse_sitemap_urls(xml_content)
        
        print(f"ÅÄ…cznie znaleziono {len(all_urls)} URL-Ã³w")
        return all_urls
    
    def save_to_csv(self, urls: List[dict], filename: str = 'sitemap_urls.csv'):
        """
        Zapisuje URL-e do pliku CSV
        
        Args:
            urls: Lista sÅ‚ownikÃ³w z URL-ami
            filename: Nazwa pliku wyjÅ›ciowego
        """
        if not urls:
            print("Brak URL-Ã³w do zapisania")
            return
        
        # Zbieramy wszystkie moÅ¼liwe klucze
        fieldnames = set()
        for url in urls:
            fieldnames.update(url.keys())
        
        fieldnames = sorted(list(fieldnames))
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(urls)
        
        print(f"Zapisano {len(urls)} URL-Ã³w do pliku: {filename}")
    
    def save_to_txt(self, urls: List[dict], filename: str = 'sitemap_urls.txt'):
        """
        Zapisuje tylko URL-e do pliku tekstowego (jeden URL na liniÄ™)
        
        Args:
            urls: Lista sÅ‚ownikÃ³w z URL-ami
            filename: Nazwa pliku wyjÅ›ciowego
        """
        if not urls:
            print("Brak URL-Ã³w do zapisania")
            return
        
        with open(filename, 'w', encoding='utf-8') as f:
            for url_data in urls:
                if 'loc' in url_data:
                    f.write(url_data['loc'] + '
')
        
        print(f"Zapisano {len(urls)} URL-Ã³w do pliku: {filename}")


def main():
    """
    Funkcja gÅ‚Ã³wna - przykÅ‚ad uÅ¼ycia
    """
    # Inicjalizujemy ekstraktor
    extractor = SitemapExtractor(timeout=10)
    
    # Opcja 1: Podaj domenÄ™ (automatycznie dodaje /sitemap.xml)
    domain = "https://phu.io.vn/"
    urls = extractor.extract_urls_from_domain(domain)
    
    # Opcja 2: Podaj bezpoÅ›redni URL do sitemap
    # sitemap_url = "https://example.com/sitemap.xml"
    # urls = extractor.extract_urls_from_sitemap_url(sitemap_url)
    
    if urls:
        # Zapisujemy wyniki
        extractor.save_to_csv(urls, 'sitemap_urls.csv')
        extractor.save_to_txt(urls, 'sitemap_urls.txt')
        
        # WyÅ›wietlamy przykÅ‚adowe URL-e
        print(f"
PrzykÅ‚adowe URL-e (pierwsze 5):")
        for i, url in enumerate(urls[:5]):
            print(f"{i+1}. {url.get('loc', 'N/A')}")
        
        # Statystyki
        print(f"
Statystyki:")
        print(f"ÅÄ…cznie URL-Ã³w: {len(urls)}")
        
        # Sprawdzamy domeny
        domains = set()
        for url in urls:
            if 'loc' in url:
                parsed = urlparse(url['loc'])
                domains.add(parsed.netloc)
        
        print(f"Unikalne domeny: {len(domains)}")
        for domain in sorted(domains):
            print(f"  - {domain}")
    
    else:
        print("Nie znaleziono Å¼adnych URL-Ã³w")


if __name__ == "__main__":
    main()
```

### ğŸ“ JÄ™zyki programowania

![image](image%202.png)

RozmawiajÄ…c z modelami jÄ™zykowymi moÅ¼emy pisaÄ‡ aplikacje nie tworzÄ…c samemu ani linijki kodu. Dlatego bardziej niÅ¼ skÅ‚adnia jÄ™zyka powinny nas interesowaÄ‡ jego inne cechy, do czego sÅ‚uÅ¼y, jakie ma mocne strony i jakie sÄ… jego ograniczenia. To po prostu wybÃ³r odpowiedniego narzÄ™dzia do zadania.

W praktyce wystarczy nam Python do przetwarzania danych i jako jÄ™zyk serwera, Javascript jako jÄ™zyk obsÅ‚ugujÄ…cy interakcje usera ze stronÄ… napisanÄ… w HTML/CSS.

[https://benjdd.com/languages/](https://benjdd.com/languages/)

#### ğŸ˜ SQL

o tym mÃ³wiÅ‚ juÅ¼ Damian, w module o Supabase.

[ info o SQL]

AlternatywÄ… jest NoSQL stosowany np. w Firebase

#### ğŸ Python

Jest to najpopularniejszy jÄ™zyk programowania. Na GitHub jeÅ›li chodzi o projekty Open Source, wyprzedziÅ‚ w 2024 roku Javascript.

Nazwany na czeÅ›Ä‡ grupy Monty Python ponad 30 lat temu.

Jest to jÄ™zyk wysokopoziomowy (czyli Å‚atwy dla czÅ‚owieka, nie dla komputera), napisany w C.

![image](image%203.png)

Wszechstronny, mnogoÅ›Ä‡ zastosowaÅ„ w AI

Mocne strony:

*   prosta skÅ‚adnia
*   MnogoÅ›Ä‡ bibliotek do przetwarzania i analizy danych (pandas, BeautifulSoup) i sdk (google, supabase), Crawl4AI
*   MaÅ‚o "ceremonii" w kodzie
*   Alternatywa to Node.js ALE:

##### **PeÅ‚ny przykÅ‚ad Node.js - to samo zadanie co Python:**

```javascript
// Najpierw instalacja pakietÃ³w:
// npm install csv-parser exceljs

const fs = require('fs');
const csv = require('csv-parser');
const ExcelJS = require('exceljs');

// Funkcja gÅ‚Ã³wna (bo Node.js wymaga async/await dla Excel)
async function analizujKeywords() {
  const results = [];

  // 1. Wczytanie CSV
  await new Promise((resolve, reject) => {
    fs.createReadStream('keywords.csv')
      .pipe(csv())
      .on('data', (data) => {
        // Konwersja pozycji na liczbÄ™ (csv-parser zwraca stringi)
        data.pozycja = parseInt(data.pozycja);
        results.push(data);
      })
      .on('end', resolve)
      .on('error', reject);
  });

  // 2. Filtrowanie top 10
  const top10 = results.filter(row => row.pozycja <= 10);

  // 3. Zapis do Excel
  const workbook = new ExcelJS.Workbook();
  const worksheet = workbook.addWorksheet('Top 10');

  // Dodanie nagÅ‚Ã³wkÃ³w (zakÅ‚adamy Å¼e CSV ma kolumny: keyword, pozycja, klikniecia)
  worksheet.columns = [
    { header: 'Keyword', key: 'keyword', width: 30 },
    { header: 'Pozycja', key: 'pozycja', width: 10 },
    { header: 'KlikniÄ™cia', key: 'klikniecia', width: 15 }
  ];

  // Dodanie danych
  top10.forEach(row => {
    worksheet.addRow(row);
  });

  // Zapisanie pliku
  await workbook.xlsx.writeFile('raport.xlsx');

  console.log('Gotowe! Zapisano do raport.xlsx');
}

// Uruchomienie z obsÅ‚ugÄ… bÅ‚Ä™dÃ³w
analizujKeywords().catch(error => {
  console.error('BÅ‚Ä…d:', error);
});
```

###### **PorÃ³wnanie kodu:**

**Python (3 linie):**

```python
dane = pd.read_csv("keywords.csv")
top10 = dane[dane['pozycja'] <= 10]
top10.to_excel("raport.xlsx")
```

**Node.js (40+ linii):**

*   Instalacja 2 pakietÃ³w
*   Import 3 moduÅ‚Ã³w
*   Async/await wrapper
*   Promise dla strumienia
*   RÄ™czna konwersja typÃ³w
*   RÄ™czne definiowanie kolumn
*   ObsÅ‚uga bÅ‚Ä™dÃ³w
*   Callback hell (czÄ™Å›ciowo)

###### **Dodatkowe komplikacje w Node.js:**

1.  **Typy danych:**

```javascript
data.pozycja = parseInt(data.pozycja);
// Python - pandas robi to automatycznie
```

1.  **Struktura danych:**

```javascript
// Node.js - musisz rÄ™cznie definiowaÄ‡ kolumny Excel
worksheet.columns = [
  { header: 'Keyword', key: 'keyword', width: 30 },
  // ...
];
// Python - pandas zachowuje strukturÄ™ automatycznie
```

1.  **ObsÅ‚uga asynchronicznoÅ›ci:**

```javascript
// Node.js - wszystko musi byÄ‡ w async/await lub callbackach
async function analizujKeywords() {
  await new Promise((resolve, reject) => {
    // ...
  });
}
// Python - synchroniczny, prosty przepÅ‚yw
```

###### **Realny czas developmentu:**

| Zadanie | Python | Node.js |
| --- | --- | --- |
| Napisanie kodu | 30 sekund | 5-10 minut |
| Debugowanie | Rzadko potrzebne | CzÄ™sto (typy, async) |
| Instalacja bibliotek | pip install pandas openpyxl | npm install csv-parser exceljs |
| Dokumentacja | Prosta, spÃ³jna | RÃ³Å¼na dla kaÅ¼dej biblioteki |

To pokazuje dlaczego Python dominuje w analizie danych - robi wiÄ™cej przy mniejszym nakÅ‚adzie kodu i czasu.

#### ğŸ“’ Javascript/ Typescript

Kluczowe dla aplikacji webowych. Pisze siÄ™ w nim zarÃ³wno obsÅ‚ugÄ™ klikniÄ™Ä‡ w przyciski,

#### ğŸ  Html + CSS

ChoÄ‡ formalnie nie sÄ… to jÄ™zyki.

HTML HTML + CSS

![image](image%204.png)

### ğŸ“š **Podstawowe koncepty w kodzie**

UczÄ…c siÄ™ jednego jÄ™zyka programowania duÅ¼o Å‚atwiej nauczyÄ‡ nam siÄ™ kolejnych. Podobnie jak poznamy jÄ™zyk wÅ‚oski Å‚atwiej nam nauczyÄ‡ siÄ™ hiszpaÅ„skiego. Jest tak dlatego, Å¼e jÄ™zyki te operujÄ… na podobnych konceptach, majÄ… zbliÅ¼onÄ… skÅ‚adniÄ™. Tu uÅ¼yjemy Pythona, ale te same koncepty stosowane sÄ… w innych jÄ™zykach.

#### **1. Zmienne - Pojemniki na dane**

Zmienna to jak komÃ³rka w Excelu - miejsce gdzie przechowujesz informacjÄ™.

```python
# Zmienne tekstowe (string)
keyword = "buty nike"
url = "https://example.com/buty"

# Zmienne liczbowe
pozycja = 11
liczba_wyswietlen = 15420
ctr = 2.5  # procent

# Zmienne logiczne (True/False)
czy_w_top10 = False
czy_zindeksowana = True

# UÅ¼ywanie zmiennych
print(f"Keyword '{keyword}' jest na pozycji {pozycja}")
# Wynik: Keyword 'buty nike' jest na pozycji 11
```

**Analogia SEO**: Zmienna to jak parametr w Google Analytics - przechowuje konkretnÄ… wartoÅ›Ä‡ ktÃ³rÄ… moÅ¼esz pÃ³Åºniej wykorzystaÄ‡.

#### **2. Funkcje - Gotowe przepisy**

Funkcja to zestaw instrukcji ktÃ³re moÅ¼esz uÅ¼ywaÄ‡ wielokrotnie. Jak makro w Excelu!

```python
# Definicja funkcji
def sprawdz_pozycje(keyword, pozycja):
    """Sprawdza czy keyword jest w TOP10"""
    if pozycja <= 10:
        return f"âœ… '{keyword}' jest w TOP10 (poz. {pozycja})"
    else:
        return f"âŒ '{keyword}' poza TOP10 (poz. {pozycja})"

# UÅ¼ycie funkcji
wynik1 = sprawdz_pozycje("buty nike", 5)
wynik2 = sprawdz_pozycje("adidas sneakers", 15)

print(wynik1)  # âœ… 'buty nike' jest w TOP10 (poz. 5)
print(wynik2)  # âŒ 'adidas sneakers' poza TOP10 (poz. 15)
```

**Praktyczny przykÅ‚ad SEO**:

```python
def oblicz_potencjal_ruchu(impressions, pozycja_obecna, pozycja_docelowa=3):
    """Oblicza potencjalny wzrost ruchu po awansie"""
    ctr_obecny = get_ctr_dla_pozycji(pozycja_obecna)
    ctr_docelowy = get_ctr_dla_pozycji(pozycja_docelowa)

    ruch_obecny = impressions * ctr_obecny / 100
    ruch_potencjalny = impressions * ctr_docelowy / 100

    wzrost = ruch_potencjalny - ruch_obecny
    return round(wzrost)

# UÅ¼ycie
potencjal = oblicz_potencjal_ruchu(10000, 11, 3)
print(f"MoÅ¼esz zyskaÄ‡ {potencjal} dodatkowych klikniÄ™Ä‡ miesiÄ™cznie!")
```

#### **3. Listy - Kolekcje danych**

Lista to jak kolumna w Excelu - przechowuje wiele wartoÅ›ci.

```python
# Lista keywords
keywords = ["buty nike", "adidas sneakers", "puma buty", "reebok classic"]

# Dodawanie do listy
keywords.append("new balance")

# DostÄ™p do elementÃ³w (numeracja od 0!)
pierwszy_keyword = keywords[0]  # "buty nike"
ostatni_keyword = keywords[-1]  # "new balance"

# Przetwarzanie caÅ‚ej listy
for keyword in keywords:
    print(f"AnalizujÄ™: {keyword}")
```

**SÅ‚owniki - dane z etykietami**:

```python
# SÅ‚ownik to jak wiersz w Excelu z nazwanymi kolumnami
dane_keyword = {
    "keyword": "buty nike",
    "pozycja": 11,
    "impressions": 15420,
    "clicks": 234,
    "url": "https://example.com/buty-nike"
}

# DostÄ™p do danych
print(dane_keyword["keyword"])  # "buty nike"
print(dane_keyword["pozycja"])  # 11

# Lista sÅ‚ownikÃ³w (jak tabela w Excelu)
wyniki_seo = [
    {"keyword": "buty nike", "pozycja": 11, "impressions": 15420},
    {"keyword": "adidas sneakers", "pozycja": 5, "impressions": 8900},
    {"keyword": "puma buty", "pozycja": 23, "impressions": 3200}
]
```

#### **4. Warunki - Podejmowanie decyzji**

Instrukcje warunkowe pozwalajÄ… programowi reagowaÄ‡ rÃ³Å¼nie w zaleÅ¼noÅ›ci od sytuacji.

```python
pozycja = 11

# Prosty warunek
if pozycja <= 10:
    print("Åšwietnie! JesteÅ› w TOP10")
else:
    print("Trzeba popracowaÄ‡ nad pozycjÄ…")

# ZÅ‚oÅ¼one warunki
if pozycja <= 3:
    status = "ğŸ¥‡ TOP3 - Excellent!"
elif pozycja <= 10:
    status = "âœ… TOP10 - Dobra robota"
elif pozycja <= 20:
    status = "ğŸ“ˆ TOP20 - Jest potencjaÅ‚"
else:
    status = "ğŸ’ª Wymaga pracy"

# Operatory logiczne
if pozycja > 10 and impressions > 1000:
    print("DuÅ¼y potencjaÅ‚ - warto zoptymalizowaÄ‡!")

if ctr < 1 or pozycja > 50:
    print("Pilna interwencja potrzebna!")
```

#### **5. PÄ™tle - Automatyzacja powtÃ³rzeÅ„**

PÄ™tle pozwalajÄ… wykonaÄ‡ te same operacje dla wielu elementÃ³w.

```python
# PÄ™tla for - gdy znasz iloÅ›Ä‡ powtÃ³rzeÅ„
keywords = ["buty nike", "adidas sneakers", "puma buty"]

for keyword in keywords:
    print(f"Sprawdzam pozycjÄ™ dla: {keyword}")
    # Tu normalnie byÅ‚oby wywoÅ‚anie API

# PÄ™tla while - gdy nie wiesz ile razy
strona = 1
while strona <= 10:
    print(f"Pobieram wyniki ze strony {strona}")
    strona = strona + 1
    # Przerwij jeÅ›li nie ma wiÄ™cej wynikÃ³w
    if brak_wynikow:
        break
```

**Praktyczny przykÅ‚ad - analiza wielu URL**:

```python
urls_do_sprawdzenia = [
    "https://example.com/kategoria-1",
    "https://example.com/kategoria-2",
    "https://example.com/produkt-1"
]

for url in urls_do_sprawdzenia:
    status_code = sprawdz_status(url)  # funkcja sprawdzajÄ…ca
    if status_code == 404:
        print(f"âŒ BÅÄ„D 404: {url}")
    elif status_code == 200:
        print(f"âœ… OK: {url}")
```

#### **6. Klasy - Szablony obiektÃ³w**

Klasa to jak formularz - definiuje strukturÄ™ danych i co moÅ¼na z nimi zrobiÄ‡.

```python
class AnalizaKeyword:
    def __init__(self, keyword, url):
        self.keyword = keyword
        self.url = url
        self.pozycja = None
        self.impressions = 0
        self.clicks = 0

    def oblicz_ctr(self):
        if self.impressions > 0:
            return (self.clicks / self.impressions) * 100
        return 0

    def czy_wymaga_optymalizacji(self):
        return self.pozycja > 10 and self.impressions > 1000

# UÅ¼ycie klasy
analiza1 = AnalizaKeyword("buty nike", "/buty-nike")
analiza1.pozycja = 15
analiza1.impressions = 5000
analiza1.clicks = 50

print(f"CTR: {analiza1.oblicz_ctr():.2f}%")
if analiza1.czy_wymaga_optymalizacji():
    print("ğŸ¯ Ten keyword ma duÅ¼y potencjaÅ‚!")
```

#### **7. Import bibliotek - Korzystanie z gotowych narzÄ™dzi**

Biblioteki to gotowe zestawy funkcji ktÃ³re moÅ¼esz uÅ¼ywaÄ‡.

```python
# Import caÅ‚ej biblioteki
import pandas as pd
import requests

# Import konkretnej funkcji
from datetime import datetime
from collections import Counter

# PrzykÅ‚ad uÅ¼ycia
# Pandas - jak Excel w Pythonie
df = pd.read_csv('keywords.csv')
top10 = df[df['pozycja'] <= 10]

# Requests - pobieranie danych z internetu
response = requests.get('https://api.example.com/data')

# Counter - liczenie wystÄ…pieÅ„
keywords = ["nike", "adidas", "nike", "puma", "nike"]
licznik = Counter(keywords)
print(licznik)  # {'nike': 3, 'adidas': 1, 'puma': 1}
```

#### **8. ObsÅ‚uga bÅ‚Ä™dÃ³w - Gdy coÅ› pÃ³jdzie nie tak**

```python
try:
    # PrÃ³buj wykonaÄ‡ kod
    response = requests.get(api_url)
    data = response.json()
except requests.exceptions.Timeout:
    print("âŒ API nie odpowiada - timeout")
except Exception as e:
    print(f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d: {e}")
else:
    # Wykonaj jeÅ›li nie byÅ‚o bÅ‚Ä™du
    print("âœ… Dane pobrane pomyÅ›lnie")
finally:
    # Wykonaj zawsze (np. zamknij poÅ‚Ä…czenie)
    print("ZakoÅ„czono operacjÄ™")
```

### ğŸ› ï¸ NarzÄ™dzia

No code

[https://x.com/Mrcontech/status/1933538141052211478](https://x.com/Mrcontech/status/1933538141052211478)

![image](image%205.png)

[https://bolt.new/](https://bolt.new/)

[https://lovable.dev/](https://lovable.dev/)

[https://v0.dev/](https://v0.dev/)

[https://replit.com/](https://replit.com/)

[https://www.tempo.new/](https://www.tempo.new/)

[https://x.com/Mrcontech/status/1933538141052211478/photo/1](https://x.com/Mrcontech/status/1933538141052211478/photo/1)

Code

Cursor

Windsurf

SuperCode

Claude Code

### ğŸ›¡ï¸ BezpieczeÅ„stwo

## <mark><strong>Security is not a feature, it's a requirement.</strong></mark>

![image](image%206.png)

```markdown
âš ï¸ ZÅOTA ZASADA: Klucze API to jak hasÅ‚a!
- NIGDY nie wrzucaj ich do GitHub
- UÅ¼ywaj pliku .env (pokaÅ¼emy jak)
- Testuj najpierw na maÅ‚ych danych

PrzykÅ‚ad co moÅ¼e pÃ³jÅ›Ä‡ Åºle:
- Wrzucisz klucz API na GitHub = ktoÅ› go ukradnie
- ZapÄ™tlisz skrypt = 10000 requestÃ³w do API = rachunek ğŸ’¸
```

#### ğŸ” PIP AUDIT - Skaner bezpieczeÅ„stwa bibliotek Pythona

`pip audit` to narzÄ™dzie do skanowania zaleÅ¼noÅ›ci Pythona pod kÄ…tem znanych podatnoÅ›ci bezpieczeÅ„stwa. Jak antywirus, ale dla twoich bibliotek!

##### Instalacja:

```shell
bash
pip install pip-audit
```

##### UÅ¼ycie:

```shell
bash
# Skanuj obecne Å›rodowisko
pip-audit

# Skanuj requirements.txt
pip-audit -r requirements.txt

# Automatyczna naprawa (jeÅ›li moÅ¼liwa)
pip-audit --fix

# SzczegÃ³Å‚owy raport
pip-audit --desc
```

##### PrzykÅ‚adowy output:

```text
Found 2 known vulnerabilities in 2 packages
Name       Version  ID             Fix Versions
---------- -------- -------------- ------------
flask      1.0.2    PYSEC-2019-179 >=1.0.3
requests   2.6.0    PYSEC-2021-101 >=2.26.0
```

#### ğŸ” NPM AUDIT - Skaner bezpieczeÅ„stwa bibliotek Node

`npm audit` to wbudowane narzÄ™dzie do skanowania zaleÅ¼noÅ›ci Node.js pod kÄ…tem znanych podatnoÅ›ci bezpieczeÅ„stwa. Jak antywirus, ale dla twoich pakietÃ³w npm!

Instalacja:

```shell
# Wbudowane w npm (6.0+) - nie wymaga instalacji
npm --version
```

UÅ¼ycie:

```shell
# Skanuj projekt
npm audit

# Automatyczna naprawa (jeÅ›li moÅ¼liwa)
npm audit fix

# Wymuszenie naprawy (moÅ¼e wprowadziÄ‡ breaking changes)
npm audit fix --force

# Tylko podatnoÅ›ci o wysokim/krytycznym poziomie
npm audit --audit-level high

# Format JSON dla dalszego przetwarzania
npm audit --json

# SzczegÃ³Å‚owy raport
npm audit --parseable
```

**Kiedy uÅ¼ywaÄ‡ npm audit:**

ğŸ”„ **Regularnie:**

*   Co tydzieÅ„/miesiÄ…c w aktywnych projektach
*   Przed kaÅ¼dym deploymentem produkcyjnym
*   Po dodaniu nowych zaleÅ¼noÅ›ci

âš¡ **Natychmiast:**

*   Gdy otrzymasz alert o nowej podatnoÅ›ci
*   Przed rozpoczÄ™ciem pracy nad starszym projektem
*   Po klonowaniu repozytorium

ğŸ—ï¸ **W procesie CI/CD:**

*   Dodaj do pipeline'u jako gate przed deploymentem
*   Zautomatyzuj sprawdzanie w pull requestach

âš ï¸ **Pilnie:**

*   Gdy twoja aplikacja obsÅ‚uguje wraÅ¼liwe dane
*   W aplikacjach bankowych/finansowych

**PrzykÅ‚adowy output:**

```text
# npm audit report

lodash  <=4.17.10
Severity: high
Prototype Pollution - https://npmjs.com/advisories/782
fix available via `npm audit fix`
node_modules/lodash

2 high severity vulnerabilities

To address all issues, run:
  npm audit fix
```

**Bonus:** Dla wiÄ™kszych projektÃ³w moÅ¼na uÅ¼yÄ‡ `yarn audit` (Yarn) lub `pnpm audit` (pnpm) - dziaÅ‚ajÄ… podobnie!

#### âœ… Security Rules (wklej to do .cursora)

Skopiuj to do cursorrules i waliduj kod przed deployem

authentication:
- ZAWSZE weryfikuj tokeny przed uÅ¼yciem - nigdy nie ufaj inputowi uÅ¼ytkownika
- NIGDY nie wysyÅ‚aj tokenÃ³w w URL/query params - uÅ¼ywaj Authorization headers
- NIGDY nie loguj peÅ‚nych tokenÃ³w - maksymalnie pierwsze/ostatnie 4 znaki
- UÅ¼ywaj krÃ³tkich czasÃ³w wygaÅ›niÄ™cia tokenÃ³w (30 min dla access, 7 dni dla refresh)
- Implementuj token rotation i refresh token flow
- Hashuj hasÅ‚a uÅ¼ywajÄ…c bcrypt/argon2 z odpowiednim salt

database_security:
- ZAWSZE uÅ¼ywaj Row Level Security (RLS) w Supabase/PostgreSQL
- NIGDY nie uÅ¼ywaj service/admin keys w kodzie klienckim
- ZAWSZE uÅ¼ywaj parametryzowanych zapytaÅ„ - nigdy string concatenation
- Szyfruj wraÅ¼liwe dane przed zapisem do bazy (PII, tokeny, klucze API)
- Regularnie rotuj klucze dostÄ™powe do bazy danych
- Ogranicz uprawnienia uÅ¼ytkownikÃ³w bazy do minimum (principle of least privilege)

input_validation:
- ZAWSZE waliduj i sanityzuj wszystkie inputy uÅ¼ytkownika
- Sprawdzaj typy danych, dÅ‚ugoÅ›Ä‡, format, dozwolone znaki
- Odrzucaj podejrzane wzorce (Path Traversal: ../, SQL: '; DROP TABLE)
- UÅ¼ywaj whitelist zamiast blacklist dla dozwolonych wartoÅ›ci
- Escapuj specjalne znaki przed wyÅ›wietleniem (XSS prevention)
- Waliduj zarÃ³wno po stronie klienta jak i serwera

```
# PrzykÅ‚ady
# Autoryzacja
- |
  // âœ… DOBRZE - z headerem
  fetch('/api/endpoint', {
    headers: { 'Authorization': `Bearer ${token}` }
  })

  // âŒ Å¹LE - token w URL
  fetch(`/api/endpoint?token=${token}`)

# Walidacja
- |
  // âœ… DOBRZE - wÅ‚aÅ›ciwa walidacja
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  if (!emailRegex.test(email)) throw new Error('Invalid email');

  // âŒ Å¹LE - brak walidacji
  const email = req.body.email; // uÅ¼ywane bez sprawdzenia

# Logowanie
- |
  // âœ… DOBRZE - bez wraÅ¼liwych danych
  logger.info(`User ${userId} logged in`);

  // âŒ Å¹LE - logowanie tokenÃ³w
  logger.info(`Token: ${accessToken}`);

# BÅ‚Ä™dy
- |
  // âœ… DOBRZE - generyczny bÅ‚Ä…d
  res.status(500).json({ error: 'Internal server error' });

  // âŒ Å¹LE - szczegÃ³Å‚y implementacji
  res.status(500).json({ error: err.stack });

# Zapytania DB
- |
  // âœ… DOBRZE - parametryzowane
  db.query('SELECT * FROM users WHERE id = $1', [userId]);

  // âŒ Å¹LE - konkatenacja
  db.query(`SELECT * FROM users WHERE id = ${userId}`);
```

supabase_specific:
- ZAWSZE wÅ‚Ä…czaj RLS na wszystkich tabelach
- UÅ¼ywaj auth.uid() w politykach RLS
- Preferuj anon key nad service key w aplikacji
- Implementuj polityki dla SELECT, INSERT, UPDATE, DELETE osobno
- Testuj polityki RLS przed deploymentem
- UÅ¼ywaj Supabase Edge Functions dla wraÅ¼liwych operacji

deployment_security:
- Skanuj dependencies pod kÄ…tem vulnerabilities (npm audit)
- UÅ¼ywaj najnowszych wersji bibliotek z patchami bezpieczeÅ„stwa
- Implementuj CSP (Content Security Policy) headers
- UÅ¼ywaj HSTS dla wymuszenia HTTPS
- Regularnie przeprowadzaj pentesty
- Monitoruj logi pod kÄ…tem anomalii

monitoring:
- Loguj wszystkie nieudane prÃ³by logowania
- Monitoruj nietypowe wzorce dostÄ™pu
- Alertuj przy podejrzanych aktywnoÅ›ciach
- Regularnie przeglÄ…daj logi bezpieczeÅ„stwa
- Implementuj audit trail dla krytycznych operacji

compliance:
- Przestrzegaj GDPR/RODO dla danych osobowych
- Implementuj right to be forgotten
- Szyfruj dane PII (Personally Identifiable Information)
- Dokumentuj przepÅ‚yw danych osobowych
- Regularnie audytuj zgodnoÅ›Ä‡ z przepisami

# KRYTYCZNE - NAPRAW NATYCHMIAST:

critical_vulnerabilities:
- Service key uÅ¼ywany zamiast anon key w Supabase
- Brak weryfikacji tokenÃ³w OAuth przed uÅ¼yciem
- Brak RLS na tabelach z wraÅ¼liwymi danymi
- SQL injection przez konkatenacjÄ™ stringÃ³w
- Logowanie haseÅ‚, tokenÃ³w lub kluczy API
- Hardkodowane klucze API w kodzie klienckim

### ğŸ—‚ï¸ Struktura projektu frontendowego

Projekt generowany przez platformy [bolt.new](http://bolt.new) czy lovable.dev jest oparty na Vite + TypeScript, ze zintegrowanym Tailwind CSS i PostCSS oraz ESLint do lintowania. Jego struktura:

![image](image%209.png)

*   **node_modules** â€“ folder z wszystkimi paczkami (zainstalowanymi przez npm/yarn).
*   **src/** â€“ kod ÅºrÃ³dÅ‚owy aplikacji.
*   **index.html** â€“ gÅ‚Ã³wny HTML, w ktÃ³rym Vite â€œwstrzykujeâ€ bundlowane skrypty.
*   **package.json** & **package-lock.json** â€“ deklaracja zaleÅ¼noÅ›ci i skryptÃ³w.
*   **vite.config.ts** â€“ konfiguracja bundlera Vite.
*   **tsconfig.json**, **tsconfig.app.json**, **tsconfig.node.json** â€“ ustawienia kompilatora TypeScript dla rÃ³Å¼nych czÄ™Å›ci projektu.
*   **tailwind.config.js** & **postcss.config.js** â€“ konfiguracja frameworka CSS.
*   **.eslintrc.cjs** â€“ konfiguracja lintera ESLint.

### ğŸ¤– Modele

![image](image%2011.png)

![image](image%2012.png)

![image](image%2013.png)

**Czy najinteligentniejszy model jest najlepszy?**

WybierajÄ…c model musimy przede wszystkim wziÄ…Ä‡ pod uwagÄ™ w jakim Å›rodowisku go uÅ¼ywamy - jeÅ›li potrzebujemy skryptu bez zaleÅ¼noÅ›ci od wiÄ™kszej liczby elementÃ³w systemy to spokojnie moÅ¼emy go napisaÄ‡ z modelem w czacie, ale jeÅ›li chcemy wykorzystaÄ‡ model jako agenta (np. w Cursorze) to jego skutecznoÅ›Ä‡ korzystania z narzÄ™dzi jest moim zdaniem istotniejsza niÅ¼ jego wyniki w testach programistycznych.

![image](image%2014.png)

[https://artificialanalysis.ai](https://artificialanalysis.ai)

#### Testy programistyczne dla modeli

##### **HumanEval**

*   **Najstarszy i najczÄ™Å›ciej uÅ¼ywany** benchmark do oceny generowania kodu
*   SkÅ‚ada siÄ™ z **164 prostych zadaÅ„ programistycznych** w Pythonie
*   Zadania to gÅ‚Ã³wnie **pojedyncze funkcje** z jasno okreÅ›lonymi specyfikacjami
*   **Relatywnie Å‚atwy** - dlatego najlepsze modele osiÄ…gajÄ… tu wyniki 95%+
*   PrzykÅ‚ad: "Napisz funkcjÄ™, ktÃ³ra zwraca n-ty element ciÄ…gu Fibonacciego"

##### **SciCode**

*   Koncentruje siÄ™ na **programowaniu naukowym i badawczym**
*   Zadania wymagajÄ… **gÅ‚Ä™bokiej wiedzy domenowej** z matematyki, fizyki, chemii itp.
*   CzÄ™sto wymaga uÅ¼ycia **specjalistycznych bibliotek** (NumPy, SciPy)
*   **Trudniejszy niÅ¼ HumanEval** - testuje zdolnoÅ›Ä‡ modelu do rozumienia i stosowania zÅ‚oÅ¼onych algorytmÃ³w
*   PrzykÅ‚ad: "Zaimplementuj metodÄ™ Monte Carlo do oszacowania wartoÅ›ci liczby Pi"

##### **LiveCodeBench**

*   **Najbardziej realistyczny i wymagajÄ…cy** benchmark
*   Zadania pochodzÄ… z **prawdziwych konkursÃ³w programistycznych** (LeetCode, Codeforces)
*   Ocena opiera siÄ™ na **przechodzeniu ukrytych testÃ³w**, tak jak w prawdziwym konkursie
*   Testuje **zdolnoÅ›Ä‡ do rozwiÄ…zywania problemÃ³w**, a nie tylko generowania kodu
*   **Najtrudniejszy** - nawet najlepsze modele majÄ… tu problemy
*   PrzykÅ‚ad: "ZnajdÅº najkrÃ³tszÄ… Å›cieÅ¼kÄ™ w grafie z uwzglÄ™dnieniem wag krawÄ™dzi"

### ğŸ–Šï¸ Cursor

Najpopularniejszy edytor kodu wsparty AI

Cursor i Windsurf to â€**zintegrowane Å›rodowisko programistyczneâ€** (IDE - Integrated Development Environment). W praktyce - edytory kodu, majÄ…ce podÅ›wietlanie skÅ‚adni, wbudowany terminal, integracje z systemami zarzÄ…dzanie wersjÄ… i wiele innych miÅ‚ych w pracy z kodem funkcji. Co wiÄ™cej, a moÅ¼e przede wszystkim, majÄ… integracjÄ™ z AI. To daje nam inteligentne podpowiedzi i agenta ktÃ³ry pisze kod. Oba oparte sÄ… o VS Code od Microsoftu, najpopularniejsze IDE na Å›wiecie. Podobnie Windsurf ma swoje modele SWE-1 ale brak Claudeâ€™a Sonnet 4 to duÅ¼y minus aktualnie

#### Setup

##### Privacy mode

ON

##### YOLO mode - auto-run denylist

![image](image%2015.png)

[https://forum.cursor.com/t/cursor-yolo-deleted-everything-in-my-computer/103131](https://forum.cursor.com/t/cursor-yolo-deleted-everything-in-my-computer/103131)

```python
git, rm, opcjonalnie npm, pip, gcloud
```

##### Tab

WÅ‚Ä…czone

##### Modele

[Sensai vibe coding workshop](Sensai%20vibe%20coding%20workshop%20213666c7eacc800498dfd547dd29c4db.html)

##### Rules

[Sensai vibe coding workshop](Sensai%20vibe%20coding%20workshop%20213666c7eacc800498dfd547dd29c4db.html)

#### Prompting

![image](image%2016.png)

![image](image%2017.png)

![image](image%2018.png)

### **Supabase Cheat Sheet**

#### Setup & Installation

```shell
# JS/TS Client
npm install @supabase/supabase-js

# Auth UI (opcjonalne)
npm install @supabase/auth-ui-react @supabase/auth-ui-shared

# SSR helpers (dla framework SSR)
npm install @supabase/ssr

# CLI (global)
npm install -g @supabase/cli
```

#### Environment Variables

```text
# .env.local
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key

# Dla Vite
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-anon-key

# SvelteKit
PUBLIC_SUPABASE_URL=https://your-project.supabase.co
PUBLIC_SUPABASE_ANON_KEY=your-anon-key
```

#### Podstawowy Client Setup

```javascript
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
  'https://your-project.supabase.co',
  'your-anon-key'
)

export default supabase
```

## Authentication

### Podstawowe Auth Operations

```javascript
// Sign Up
const { data, error } = await supabase.auth.signUp({
  email: 'example@email.com',
  password: 'example-password',
})

// Sign In
const { data, error } = await supabase.auth.signInWithPassword({
  email: 'example@email.com',
  password: 'example-password',
})

// Sign Out
const { error } = await supabase.auth.signOut()

// Get Current User
const { data: { user } } = await supabase.auth.getUser()

// Get Session
const { data: { session } } = await supabase.auth.getSession()
```

### OAuth Providers

```javascript
// Sign in z Google
const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'google',
  options: {
    redirectTo: 'http://localhost:3000/auth/callback'
  }
})

// Sign in z GitHub
const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'github'
})
```

### Auth State Listening

```javascript
// Listen for auth changes
supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_IN') {
    console.log('User signed in:', session.user)
  }
  if (event === 'SIGNED_OUT') {
    console.log('User signed out')
  }
})
```

## Database Operations

### Basic CRUD

```javascript
// INSERT
const { data, error } = await supabase
  .from('profiles')
  .insert([
    { username: 'john_doe', website: 'https://johndoe.com' }
  ])

// SELECT
const { data, error } = await supabase
  .from('profiles')
  .select('*')

// SELECT with filtering
const { data, error } = await supabase
  .from('profiles')
  .select('username, website')
  .eq('username', 'john_doe')
  .single()

// UPDATE
const { data, error } = await supabase
  .from('profiles')
  .update({ website: 'https://new-website.com' })
  .eq('id', userId)

// DELETE
const { data, error } = await supabase
  .from('profiles')
  .delete()
  .eq('id', userId)
```

### Advanced Queries

```javascript
// Filtering
const { data, error } = await supabase
  .from('posts')
  .select('*')
  .eq('status', 'published')
  .gte('created_at', '2024-01-01')
  .order('created_at', { ascending: false })
  .limit(10)

// Joins
const { data, error } = await supabase
  .from('posts')
  .select(`
    *,
    profiles (
      username,
      avatar_url
    )
  `)

// Full text search
const { data, error } = await supabase
  .from('posts')
  .select('*')
  .textSearch('title', 'supabase')
```

### RPC Calls

```javascript
// Call stored procedure
const { data, error } = await supabase
  .rpc('match_documents', {
    query_embedding: embedding,
    match_threshold: 0.78,
    match_count: 10
  })
```

## Row Level Security (RLS)

### Enabling RLS

```sql
-- Enable RLS na tabeli
ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;
```

### Basic RLS Policies

```sql
-- Policy dla SELECT - wszyscy mogÄ… czytaÄ‡ profile
CREATE POLICY "Public profiles are viewable by everyone"
ON profiles FOR SELECT
USING (true);

-- Policy dla INSERT - uÅ¼ytkownicy mogÄ… wstawiaÄ‡ tylko swoje profile
CREATE POLICY "Users can insert their own profile"
ON profiles FOR INSERT
WITH CHECK ((SELECT auth.uid()) = id);

-- Policy dla UPDATE - uÅ¼ytkownicy mogÄ… aktualizowaÄ‡ tylko swoje profile
CREATE POLICY "Users can update own profile"
ON profiles FOR UPDATE
USING ((SELECT auth.uid()) = id);

-- Policy dla DELETE - uÅ¼ytkownicy mogÄ… usuwaÄ‡ tylko swoje profile
CREATE POLICY "Users can delete own profile"
ON profiles FOR DELETE
USING ((SELECT auth.uid()) = id);
```

### Advanced RLS Policies

```sql
-- Policy z JWT claims
CREATE POLICY "User is in team"
ON my_table
TO authenticated
USING (team_id IN (SELECT auth.jwt() -> 'app_metadata' -> 'teams'));

-- Policy z joinami (unikaj - wolne)
CREATE POLICY "rls_test_select" ON test_table
TO authenticated
USING (
  team_id IN (
    SELECT team_id
    FROM team_user
    WHERE user_id = (SELECT auth.uid())
  )
);

-- Optimized policy z security definer functions
CREATE OR REPLACE FUNCTION private.get_user_org_role(org_id bigint, user_id uuid)
RETURNS text
SET search_path = ''
AS $$
  SELECT role FROM public.org_members
  WHERE org_id = $1 AND user_id = $2;
$$ LANGUAGE sql SECURITY DEFINER;

CREATE POLICY "Org management restricted to owners"
ON public.organizations FOR ALL USING (
  private.get_user_org_role(id, (SELECT auth.uid())) = 'owner'
);
```

## Realtime

### Basic Subscriptions

```javascript
// Subscribe to table changes
const subscription = supabase
  .channel('profiles')
  .on('postgres_changes',
    {
      event: '*',
      schema: 'public',
      table: 'profiles'
    },
    (payload) => {
      console.log('Change received!', payload)
    }
  )
  .subscribe()

// Subscribe to specific changes
const subscription = supabase
  .channel('todos')
  .on('postgres_changes',
    {
      event: 'INSERT',
      schema: 'public',
      table: 'todos',
      filter: 'user_id=eq.' + userId
    },
    (payload) => {
      console.log('New todo:', payload.new)
    }
  )
  .subscribe()

// Unsubscribe
subscription.unsubscribe()
```

### Broadcast Messages

```javascript
// Send broadcast message
const channel = supabase.channel('room1')

channel
  .on('broadcast', { event: 'test' }, (payload) => {
    console.log(payload)
  })
  .subscribe()

// Send message
await channel.send({
  type: 'broadcast',
  event: 'test',
  payload: { message: 'hello world' }
})
```

### Presence

```javascript
// Track user presence
const channel = supabase.channel('room1')

channel
  .on('presence', { event: 'sync' }, () => {
    const newState = channel.presenceState()
    console.log('sync', newState)
  })
  .on('presence', { event: 'join' }, ({ key, newPresences }) => {
    console.log('join', key, newPresences)
  })
  .on('presence', { event: 'leave' }, ({ key, leftPresences }) => {
    console.log('leave', key, leftPresences)
  })
  .subscribe(async (status) => {
    if (status !== 'SUBSCRIBED') { return }

    await channel.track({
      user: 'user-1',
      online_at: new Date().toISOString(),
    })
  })
```

## Storage

### File Uploads

```javascript
// Upload file
const { data, error } = await supabase.storage
  .from('avatars')
  .upload('public/avatar1.png', file, {
    cacheControl: '3600',
    upsert: false
  })

// Upload z options
const { data, error } = await supabase.storage
  .from('avatars')
  .upload(`${userId}/avatar.png`, file, {
    contentType: 'image/png',
    cacheControl: '3600',
    upsert: true
  })
```

### File Downloads & URLs

```javascript
// Download file
const { data, error } = await supabase.storage
  .from('avatars')
  .download('public/avatar1.png')

// Get public URL
const { data } = supabase.storage
  .from('avatars')
  .getPublicUrl('public/avatar1.png')

// Get signed URL (dla private files)
const { data, error } = await supabase.storage
  .from('private-bucket')
  .createSignedUrl('path/to/file.pdf', 60) // 60 seconds
```

### Storage Policies

```sql
-- Public access policy
CREATE POLICY "Avatar images are publicly accessible"
ON storage.objects FOR SELECT
USING (bucket_id = 'avatars');

-- Upload policy
CREATE POLICY "Anyone can upload an avatar"
ON storage.objects FOR INSERT
WITH CHECK (bucket_id = 'avatars');

-- User-specific policies
CREATE POLICY "Users can upload their own avatars"
ON storage.objects FOR INSERT
WITH CHECK (
  bucket_id = 'avatars'
  AND (storage.foldername(name))[1] = (SELECT auth.uid()::text)
);
```

## Next.js Integration

### App Router Setup

```typescript
// utils/supabase/client.ts
import { createBrowserClient } from '@supabase/ssr'

export function createClient() {
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  )
}
```

```typescript
// utils/supabase/server.ts
import { createServerClient } from '@supabase/ssr'
import { cookies } from 'next/headers'

export async function createClient() {
  const cookieStore = await cookies()

  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        getAll() {
          return cookieStore.getAll()
        },
        setAll(cookiesToSet) {
          try {
            cookiesToSet.forEach(({ name, value, options }) =>
              cookieStore.set(name, value, options)
            )
          } catch {
            // The `setAll` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
      },
    }
  )
}
```

### Middleware (App Router)

```typescript
// middleware.ts
import { createServerClient } from '@supabase/ssr'
import { NextResponse, type NextRequest } from 'next/server'

export async function middleware(request: NextRequest) {
  let supabaseResponse = NextResponse.next({
    request,
  })

  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        getAll() {
          return request.cookies.getAll()
        },
        setAll(cookiesToSet) {
          cookiesToSet.forEach(({ name, value, options }) => {
            request.cookies.set(name, value)
            supabaseResponse.cookies.set(name, value, options)
          })
        },
      },
    }
  )

  // IMPORTANT: Avoid writing any logic between createServerClient and
  // supabase.auth.getUser(). A simple mistake could make it very hard to debug
  // issues with users being randomly logged out.

  const {
    data: { user },
  } = await supabase.auth.getUser()

  if (
    !user &&
    !request.nextUrl.pathname.startsWith('/login') &&
    !request.nextUrl.pathname.startsWith('/auth')
  ) {
    // no user, potentially respond by redirecting the user to the login page
    const url = request.nextUrl.clone()
    url.pathname = '/login'
    return NextResponse.redirect(url)
  }

  // IMPORTANT: You *must* return the supabaseResponse object as it is. If you're
  // creating a new response object with NextResponse.next() make sure to:
  // 1. Pass the request in it, like so:
  //    const myNewResponse = NextResponse.next({ request })
  // 2. Copy over the cookies, like so:
  //    myNewResponse.cookies.setAll(supabaseResponse.cookies.getAll())
  // 3. Change the myNewResponse object instead of the supabaseResponse object

  return supabaseResponse
}

export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * Feel free to modify this pattern to include more paths.
     */
    '/((?!_next/static|_next/image|favicon.ico|.*\.(?:svg|png|jpg|jpeg|gif|webp)$).*)',
  ],
}
```

### Server Actions

```typescript
// app/auth/actions.ts
'use server'

import { revalidatePath } from 'next/cache'
import { redirect } from 'next/navigation'
import { createClient } from '@/utils/supabase/server'

export async function login(formData: FormData) {
  const supabase = await createClient()

  const data = {
    email: formData.get('email') as string,
    password: formData.get('password') as string,
  }

  const { error } = await supabase.auth.signInWithPassword(data)

  if (error) {
    redirect('/error')
  }

  revalidatePath('/', 'layout')
  redirect('/account')
}

export async function signup(formData: FormData) {
  const supabase = await createClient()

  const data = {
    email: formData.get('email') as string,
    password: formData.get('password') as string,
  }

  const { error } = await supabase.auth.signUp(data)

  if (error) {
    redirect('/error')
  }

  revalidatePath('/', 'layout')
  redirect('/account')
}
```

## Edge Functions

### Basic Edge Function

```typescript
// supabase/functions/hello-world/index.ts
import { serve } from 'https://deno.land/std@0.170.0/http/server.ts'

serve(async (req) => {
  const { name } = await req.json()
  const data = {
    message: `Hello ${name}!`,
  }

  return new Response(
    JSON.stringify(data),
    { headers: { "Content-Type": "application/json" } },
  )
})
```

### Edge Function z Supabase Client

```typescript
import { createClient } from 'jsr:@supabase/supabase-js@2'

serve(async (req) => {
  const supabase = createClient(
    Deno.env.get('SUPABASE_URL') ?? '',
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
  )

  const { data, error } = await supabase
    .from('countries')
    .select('*')

  if (error) {
    return new Response(JSON.stringify(error), { status: 500 })
  }

  return new Response(JSON.stringify(data), {
    headers: { 'Content-Type': 'application/json' },
  })
})
```

### OpenAI Integration

```typescript
import OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts'

serve(async (req) => {
  const { query } = await req.json()

  const openai = new OpenAI({
    apiKey: Deno.env.get('OPENAI_API_KEY'),
  })

  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: query }],
    model: 'gpt-3.5-turbo',
    stream: false,
  })

  const reply = chatCompletion.choices[0].message.content

  return new Response(reply, {
    headers: { 'Content-Type': 'text/plain' },
  })
})
```

## AI & Vector Search

### Vector Column Setup

```sql
-- Create table with vector column
CREATE TABLE documents (
  id BIGSERIAL PRIMARY KEY,
  content TEXT,
  embedding VECTOR(1536) -- OpenAI ada-002 size
);

-- Create vector index
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);
```

### Similarity Search Function

```sql
CREATE OR REPLACE FUNCTION match_documents(
  query_embedding VECTOR(1536),
  match_threshold FLOAT,
  match_count INT
)
RETURNS SETOF documents
LANGUAGE sql STABLE
AS $$
  SELECT *
  FROM documents
  WHERE documents.embedding <=> query_embedding < 1 - match_threshold
  ORDER BY documents.embedding <=> query_embedding
  LIMIT match_count;
$$;
```

### Edge Function for Embeddings

```typescript
const model = new Supabase.ai.Session('gte-small')

serve(async (req) => {
  const { input } = await req.json()

  // Generate embedding
  const embedding = await model.run(input, {
    mean_pool: true,
    normalize: true,
  })

  // Search similar documents
  const { data: documents } = await supabase.rpc('match_documents', {
    query_embedding: embedding,
    match_threshold: 0.78,
    match_count: 10,
  })

  return new Response(JSON.stringify({ documents }), {
    headers: { 'Content-Type': 'application/json' }
  })
})
```

## Database Schema Patterns

### User Profiles

```sql
-- User profiles table
CREATE TABLE profiles (
  id UUID REFERENCES auth.users NOT NULL,
  updated_at TIMESTAMP WITH TIME ZONE,
  username TEXT UNIQUE,
  avatar_url TEXT,
  website TEXT,
)
```
