# Sensai vibe coding workshop

> ## ‚ÄúEveryone is a programmer. Now, you just have to say something to the computer.‚Äù - Jensen Huang

Podczas tego tygodnia dowiesz siƒô:

*   jak programowaƒá korzystajƒÖc z edytor√≥w kodu jak Cursor, Windsurf i platform typu [Bolt.new](http://bolt.new/), czy Lovable
*   jak wybraƒá model do kodowania
*   jak pisaƒá dobre prompty
*   jak korzystaƒá z serwer√≥w MCP do programowania
*   jak u≈ºywaƒá GitHuba
*   jak tworzyƒá ui, np. jako interfejs do automatyzacji w n8n czy do skrypt√≥w python
*   jak wystawiƒá nasz kod do internetu, bezpiecznie wszystko skonfigurowaƒá tak ≈ºeby nie naraziƒá siƒô na utratƒô danych czy niespodziewane rachunki
*   jak tworzyƒá dodatki do przeglƒÖdarki

**Czemu uczyƒá siƒô programowaƒá?**

*   Niezale≈ºno≈õƒá w tworzeniu rozwiƒÖza≈Ñ
*   Bo fajnie jest tworzyƒá co≈õ swojego
*   Automatyzacja i oszczƒôdno≈õƒá czasu
*   Konkurencyjno≈õƒá na rynku pracy
*   Zrozumienie technologii i programist√≥w
*   Mo≈ºliwo≈õƒá budowania w≈Çasnych rozwiƒÖza≈Ñ pod w≈Çaasne potrzeby
*   wykorzystanie z pracy innych i dostosowanie jej pod w≈Çasne wymagania

Bariery techniczne znikajƒÖ, co sprawia, ≈ºe co raz wiƒôcej os√≥b zaczyna programowaƒá, i ≈ºe zaczyna liczyƒá siƒô jeszcze bardziej pomys≈Ç. Jak wiadomo od pomys≈Çu do wykonania czƒôsto d≈Çuga droga, ale nawet je≈õli nie chcemy samemu wdra≈ºaƒá naszych pomys≈Ç√≥w to ≈õwiadomo≈õƒá mo≈ºliwo≈õci jak i ogranicze≈Ñ technicznych powinna sprawiƒá ≈ºe nasze pomys≈Çy bƒôdƒÖ jeszcze lepsze, a co za tym idzie zyskujemy przewagƒô na tym konkurencyjnym rynku jakim jest SEO. ≈öwiat zmienia siƒô aktualnie tak szybko ≈ºe codziennie pojawiajƒÖ siƒô nowe problemy do rozwiƒÖzania. Przedstawiƒô Ci narzƒôdzia i koncepty do realizacji Twoich pomys≈Ç√≥w skutecznie i bezpiecznie.

# Podstawy

### **ü§® Czym jest programowanie?**

**Programowanie to spos√≥b tworzenia instrukcji, kt√≥re m√≥wiƒÖ komputerowi, jak rozwiƒÖzaƒá konkretny problem ‚Äî najczƒô≈õciej prowadzƒÖc u≈ºytkownika z punktu A do punktu B. Mo≈ºna je por√≥wnaƒá do przepisu kulinarnego albo instrukcji dojazdu: zawierajƒÖ krok po kroku, co ma siƒô wydarzyƒá.**

![image](image.png)

#### N8N

W trakcie tego kursu zetknƒô≈Ça≈õ siƒô ju≈º z formƒÖ programowania ‚Äì automatyzacjami w **n8n**. Tam, zamiast pisaƒá kod, u≈ºywamy gotowych modu≈Ç√≥w, kt√≥re ≈ÇƒÖczymy graficznie. To podej≈õcie jest bardzo wygodne, szczeg√≥lnie gdy zale≈ºy nam na **szybkim wdro≈ºeniu i prostocie**.

#### Python

Teraz p√≥jdziemy o krok dalej i zajmiemy siƒô **programowaniem w Pythonie**. To jƒôzyk tekstowy, kt√≥ry daje nam **nieograniczone mo≈ºliwo≈õci**, ale wymaga nieco wiƒôcej nauki. Python jest idealny do zada≈Ñ, kt√≥re sƒÖ zbyt z≈Ço≈ºone dla n8n, na przyk≈Çad do zaawansowanej analizy danych, uczenia maszynowego czy budowania niestandardowych narzƒôdzi.

![image](image%201.png)

#### Wizualizacja procesu

My≈õlenie o programowaniu jako o procesie pomaga zrozumieƒá jego logikƒô. Ka≈ºdy krok w procesie to fragment kodu, kt√≥ry wykonuje okre≈õlone zadanie.

![image](image%2010.png)

#### Przyk≈Çadowy skrypt w Pythonie

To jest przyk≈Çad kompletnego skryptu w Pythonie, kt√≥ry pobiera wszystkie linki z mapy strony (sitemap) i zapisuje je do pliku CSV. Nie musisz go teraz w pe≈Çni rozumieƒá ‚Äî chodzi o to, ≈ºeby zobaczyƒá, jak wyglƒÖda kod, kt√≥ry realizuje konkretne zadanie.

```python
import requests
import xml.etree.ElementTree as ET
import csv
from urllib.parse import urljoin, urlparse
import time
from typing import List, Set

class SitemapExtractor:
    def __init__(self, timeout: int = 10):
        """
        Inicjalizuje ekstraktor sitemap
        
        Args:
            timeout: Timeout dla HTTP request√≥w w sekundach
        """
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_sitemap(self, url: str) -> str:
        """
        Pobiera zawarto≈õƒá sitemap z podanego URL
        
        Args:
            url: URL do sitemap
            
        Returns:
            Zawarto≈õƒá XML jako string
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"B≈ÇƒÖd podczas pobierania {url}: {e}")
            return ""
    
    def parse_sitemap_index(self, xml_content: str) -> List[str]:
        """
        Parsuje sitemap index i zwraca listƒô URL-√≥w do sub-sitemaps
        
        Args:
            xml_content: Zawarto≈õƒá XML sitemap index
            
        Returns:
            Lista URL-√≥w do sub-sitemaps
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy r√≥≈ºne namespace'y
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            sitemap_urls = []
            
            # Szukamy element√≥w sitemap w sitemap index
            for sitemap in root.findall('.//ns:sitemap', namespaces):
                loc = sitemap.find('ns:loc', namespaces)
                if loc is not None and loc.text:
                    sitemap_urls.append(loc.text.strip())
            
            # Fallback - szukamy bez namespace
            if not sitemap_urls:
                for sitemap in root.findall('.//sitemap'):
                    loc = sitemap.find('loc')
                    if loc is not None and loc.text:
                        sitemap_urls.append(loc.text.strip())
            
            return sitemap_urls
            
        except ET.ParseError as e:
            print(f"B≈ÇƒÖd parsowania XML sitemap index: {e}")
            return []
    
    def parse_sitemap_urls(self, xml_content: str) -> List[dict]:
        """
        Parsuje sitemap i zwraca listƒô URL-√≥w z metadanymi
        
        Args:
            xml_content: Zawarto≈õƒá XML sitemap
            
        Returns:
            Lista s≈Çownik√≥w z URL-ami i metadanymi
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy r√≥≈ºne namespace'y
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            urls = []
            
            # Szukamy element√≥w url w sitemap
            for url in root.findall('.//ns:url', namespaces):
                url_data = {}
                
                # Lokalizacja (wymagana)
                loc = url.find('ns:loc', namespaces)
                if loc is not None and loc.text:
                    url_data['loc'] = loc.text.strip()
                
                # Ostatnia modyfikacja (opcjonalna)
                lastmod = url.find('ns:lastmod', namespaces)
                if lastmod is not None and lastmod.text:
                    url_data['lastmod'] = lastmod.text.strip()
                
                # Czƒôstotliwo≈õƒá zmian (opcjonalna)
                changefreq = url.find('ns:changefreq', namespaces)
                if changefreq is not None and changefreq.text:
                    url_data['changefreq'] = changefreq.text.strip()
                
                # Priorytet (opcjonalny)
                priority = url.find('ns:priority', namespaces)
                if priority is not None and priority.text:
                    url_data['priority'] = priority.text.strip()
                
                if 'loc' in url_data:
                    urls.append(url_data)
            
            # Fallback - szukamy bez namespace
            if not urls:
                for url in root.findall('.//url'):
                    url_data = {}
                    
                    loc = url.find('loc')
                    if loc is not None and loc.text:
                        url_data['loc'] = loc.text.strip()
                    
                    lastmod = url.find('lastmod')
                    if lastmod is not None and lastmod.text:
                        url_data['lastmod'] = lastmod.text.strip()
                    
                    changefreq = url.find('changefreq')
                    if changefreq is not None and changefreq.text:
                        url_data['changefreq'] = changefreq.text.strip()
                    
                    priority = url.find('priority')
                    if priority is not None and priority.text:
                        url_data['priority'] = priority.text.strip()
                    
                    if 'loc' in url_data:
                        urls.append(url_data)
            
            return urls
            
        except ET.ParseError as e:
            print(f"B≈ÇƒÖd parsowania XML sitemap: {e}")
            return []
```
```python
def is_sitemap_index(self, xml_content: str) -> bool:
        """
        Sprawdza czy XML to sitemap index czy zwyk≈Çy sitemap
        
        Args:
            xml_content: Zawarto≈õƒá XML
            
        Returns:
            True je≈õli to sitemap index, False je≈õli zwyk≈Çy sitemap
        """
        try:
            root = ET.fromstring(xml_content)
            
            # Sprawdzamy czy istniejƒÖ elementy sitemapindex
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'
            }
            
            # Sprawdzamy z namespace
            if root.find('.//ns:sitemapindex', namespaces) is not None:
                return True
            if root.find('.//ns:sitemap', namespaces) is not None:
                return True
            
            # Sprawdzamy bez namespace
            if root.find('.//sitemapindex') is not None:
                return True
            if root.tag == 'sitemapindex' or 'sitemapindex' in root.tag:
                return True
            
            return False
            
        except ET.ParseError:
            return False
    
    def extract_urls_from_domain(self, domain: str) -> List[dict]:
        """
        WyciƒÖga wszystkie URL-e z sitemap danej domeny
        
        Args:
            domain: Domena (np. 'https://example.com' lub 'https://example.com/')
            
        Returns:
            Lista s≈Çownik√≥w z URL-ami i metadanymi
        """
        # Normalizujemy domenƒô
        if not domain.startswith(('http://', 'https://')):
            domain = 'https://' + domain
        
        if not domain.endswith('/'):
            domain += '/'
        
        # Konstruujemy URL do sitemap
        sitemap_url = urljoin(domain, 'sitemap.xml')
        
        print(f"Pobieranie sitemap z: {sitemap_url}")
        
        # Pobieramy g≈Ç√≥wny sitemap
        xml_content = self.fetch_sitemap(sitemap_url)
        if not xml_content:
            return []
        
        all_urls = []
        
        # Sprawdzamy czy to sitemap index
        if self.is_sitemap_index(xml_content):
            print("Wykryto sitemap index, pobieranie sub-sitemaps...")
            
            # Pobieramy URL-e do sub-sitemaps
            sub_sitemap_urls = self.parse_sitemap_index(xml_content)
            print(f"Znaleziono {len(sub_sitemap_urls)} sub-sitemaps")
            
            # Pobieramy ka≈ºdy sub-sitemap
            for i, sub_url in enumerate(sub_sitemap_urls, 1):
                print(f"Pobieranie sub-sitemap {i}/{len(sub_sitemap_urls)}: {sub_url}")
                
                sub_xml_content = self.fetch_sitemap(sub_url)
                if sub_xml_content:
                    urls = self.parse_sitemap_urls(sub_xml_content)
                    all_urls.extend(urls)
                    print(f"  Znaleziono {len(urls)} URL-√≥w")
                
                # Ma≈Ça przerwa miƒôdzy requestami
                time.sleep(0.1)
        
        else:
            print("Wykryto zwyk≈Çy sitemap, parsowanie URL-√≥w...")
            # To zwyk≈Çy sitemap, parsujemy URL-e bezpo≈õrednio
            all_urls = self.parse_sitemap_urls(xml_content)
        
        print(f"≈ÅƒÖcznie znaleziono {len(all_urls)} URL-√≥w")
        return all_urls
    
    def extract_urls_from_sitemap_url(self, sitemap_url: str) -> List[dict]:
        """
        WyciƒÖga URL-e z konkretnego URL sitemap
        
        Args:
            sitemap_url: Bezpo≈õredni URL do sitemap
            
        Returns:
            Lista s≈Çownik√≥w z URL-ami i metadanymi
        """
        print(f"Pobieranie sitemap z: {sitemap_url}")
        
        xml_content = self.fetch_sitemap(sitemap_url)
        if not xml_content:
            return []
        
        all_urls = []
        
        if self.is_sitemap_index(xml_content):
            print("Wykryto sitemap index, pobieranie sub-sitemaps...")
            
            sub_sitemap_urls = self.parse_sitemap_index(xml_content)
            print(f"Znaleziono {len(sub_sitemap_urls)} sub-sitemaps")
            
            for i, sub_url in enumerate(sub_sitemap_urls, 1):
                print(f"Pobieranie sub-sitemap {i}/{len(sub_sitemap_urls)}: {sub_url}")
                
                sub_xml_content = self.fetch_sitemap(sub_url)
                if sub_xml_content:
                    urls = self.parse_sitemap_urls(sub_xml_content)
                    all_urls.extend(urls)
                    print(f"  Znaleziono {len(urls)} URL-√≥w")
                
                time.sleep(0.1)
        else:
            print("Wykryto zwyk≈Çy sitemap, parsowanie URL-√≥w...")
            all_urls = self.parse_sitemap_urls(xml_content)
        
        print(f"≈ÅƒÖcznie znaleziono {len(all_urls)} URL-√≥w")
        return all_urls
    
    def save_to_csv(self, urls: List[dict], filename: str = 'sitemap_urls.csv'):
        """
        Zapisuje URL-e do pliku CSV
        
        Args:
            urls: Lista s≈Çownik√≥w z URL-ami
            filename: Nazwa pliku wyj≈õciowego
        """
        if not urls:
            print("Brak URL-√≥w do zapisania")
            return
        
        # Zbieramy wszystkie mo≈ºliwe klucze
        fieldnames = set()
        for url in urls:
            fieldnames.update(url.keys())
        
        fieldnames = sorted(list(fieldnames))
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(urls)
        
        print(f"Zapisano {len(urls)} URL-√≥w do pliku: {filename}")
    
    def save_to_txt(self, urls: List[dict], filename: str = 'sitemap_urls.txt'):
        """
        Zapisuje tylko URL-e do pliku tekstowego (jeden URL na liniƒô)
        
        Args:
            urls: Lista s≈Çownik√≥w z URL-ami
            filename: Nazwa pliku wyj≈õciowego
        """
        if not urls:
            print("Brak URL-√≥w do zapisania")
            return
        
        with open(filename, 'w', encoding='utf-8') as f:
            for url_data in urls:
                if 'loc' in url_data:
                    f.write(url_data['loc'] + '
')
        
        print(f"Zapisano {len(urls)} URL-√≥w do pliku: {filename}")


def main():
    """
    Funkcja g≈Ç√≥wna - przyk≈Çad u≈ºycia
    """
    # Inicjalizujemy ekstraktor
    extractor = SitemapExtractor(timeout=10)
    
    # Opcja 1: Podaj domenƒô (automatycznie dodaje /sitemap.xml)
    domain = "https://phu.io.vn/"
    urls = extractor.extract_urls_from_domain(domain)
    
    # Opcja 2: Podaj bezpo≈õredni URL do sitemap
    # sitemap_url = "https://example.com/sitemap.xml"
    # urls = extractor.extract_urls_from_sitemap_url(sitemap_url)
    
    if urls:
        # Zapisujemy wyniki
        extractor.save_to_csv(urls, 'sitemap_urls.csv')
        extractor.save_to_txt(urls, 'sitemap_urls.txt')
        
        # Wy≈õwietlamy przyk≈Çadowe URL-e
        print(f"
Przyk≈Çadowe URL-e (pierwsze 5):")
        for i, url in enumerate(urls[:5]):
            print(f"{i+1}. {url.get('loc', 'N/A')}")
        
        # Statystyki
        print(f"
Statystyki:")
        print(f"≈ÅƒÖcznie URL-√≥w: {len(urls)}")
        
        # Sprawdzamy domeny
        domains = set()
        for url in urls:
            if 'loc' in url:
                parsed = urlparse(url['loc'])
                domains.add(parsed.netloc)
        
        print(f"Unikalne domeny: {len(domains)}")
        for domain in sorted(domains):
            print(f"  - {domain}")
    
    else:
        print("Nie znaleziono ≈ºadnych URL-√≥w")


if __name__ == "__main__":
    main()
```

### üìù Jƒôzyki programowania

![image](image%202.png)

RozmawiajƒÖc z modelami jƒôzykowymi mo≈ºemy pisaƒá aplikacje nie tworzƒÖc samemu ani linijki kodu. Dlatego bardziej ni≈º sk≈Çadnia jƒôzyka powinny nas interesowaƒá jego inne cechy, do czego s≈Çu≈ºy, jakie ma mocne strony i jakie sƒÖ jego ograniczenia. To po prostu wyb√≥r odpowiedniego narzƒôdzia do zadania.

W praktyce wystarczy nam Python do przetwarzania danych i jako jƒôzyk serwera, Javascript jako jƒôzyk obs≈ÇugujƒÖcy interakcje usera ze stronƒÖ napisanƒÖ w HTML/CSS.

[https://benjdd.com/languages/](https://benjdd.com/languages/)

#### üêò SQL

o tym m√≥wi≈Ç ju≈º Damian, w module o Supabase.

[ info o SQL]

AlternatywƒÖ jest NoSQL stosowany np. w Firebase

#### üêç Python

Jest to najpopularniejszy jƒôzyk programowania. Na GitHub je≈õli chodzi o projekty Open Source, wyprzedzi≈Ç w 2024 roku Javascript.

Nazwany na cze≈õƒá grupy Monty Python ponad 30 lat temu.

Jest to jƒôzyk wysokopoziomowy (czyli ≈Çatwy dla cz≈Çowieka, nie dla komputera), napisany w C.

![image](image%203.png)

Wszechstronny, mnogo≈õƒá zastosowa≈Ñ w AI

Mocne strony:

*   prosta sk≈Çadnia
*   Mnogo≈õƒá bibliotek do przetwarzania i analizy danych (pandas, BeautifulSoup) i sdk (google, supabase), Crawl4AI
*   Ma≈Ço "ceremonii" w kodzie
*   Alternatywa to Node.js ALE:

##### **Pe≈Çny przyk≈Çad Node.js - to samo zadanie co Python:**

```javascript
// Najpierw instalacja pakiet√≥w:
// npm install csv-parser exceljs

const fs = require('fs');
const csv = require('csv-parser');
const ExcelJS = require('exceljs');

// Funkcja g≈Ç√≥wna (bo Node.js wymaga async/await dla Excel)
async function analizujKeywords() {
  const results = [];

  // 1. Wczytanie CSV
  await new Promise((resolve, reject) => {
    fs.createReadStream('keywords.csv')
      .pipe(csv())
      .on('data', (data) => {
        // Konwersja pozycji na liczbƒô (csv-parser zwraca stringi)
        data.pozycja = parseInt(data.pozycja);
        results.push(data);
      })
      .on('end', resolve)
      .on('error', reject);
  });

  // 2. Filtrowanie top 10
  const top10 = results.filter(row => row.pozycja <= 10);

  // 3. Zapis do Excel
  const workbook = new ExcelJS.Workbook();
  const worksheet = workbook.addWorksheet('Top 10');

  // Dodanie nag≈Ç√≥wk√≥w (zak≈Çadamy ≈ºe CSV ma kolumny: keyword, pozycja, klikniecia)
  worksheet.columns = [
    { header: 'Keyword', key: 'keyword', width: 30 },
    { header: 'Pozycja', key: 'pozycja', width: 10 },
    { header: 'Klikniƒôcia', key: 'klikniecia', width: 15 }
  ];

  // Dodanie danych
  top10.forEach(row => {
    worksheet.addRow(row);
  });

  // Zapisanie pliku
  await workbook.xlsx.writeFile('raport.xlsx');

  console.log('Gotowe! Zapisano do raport.xlsx');
}

// Uruchomienie z obs≈ÇugƒÖ b≈Çƒôd√≥w
analizujKeywords().catch(error => {
  console.error('B≈ÇƒÖd:', error);
});
```

###### **Por√≥wnanie kodu:**

**Python (3 linie):**

```python
dane = pd.read_csv("keywords.csv")
top10 = dane[dane['pozycja'] <= 10]
top10.to_excel("raport.xlsx")
```

**Node.js (40+ linii):**

*   Instalacja 2 pakiet√≥w
*   Import 3 modu≈Ç√≥w
*   Async/await wrapper
*   Promise dla strumienia
*   Rƒôczna konwersja typ√≥w
*   Rƒôczne definiowanie kolumn
*   Obs≈Çuga b≈Çƒôd√≥w
*   Callback hell (czƒô≈õciowo)

###### **Dodatkowe komplikacje w Node.js:**

1.  **Typy danych:**

```javascript
data.pozycja = parseInt(data.pozycja);
// Python - pandas robi to automatycznie
```

1.  **Struktura danych:**

```javascript
// Node.js - musisz rƒôcznie definiowaƒá kolumny Excel
worksheet.columns = [
  { header: 'Keyword', key: 'keyword', width: 30 },
  // ...
];
// Python - pandas zachowuje strukturƒô automatycznie
```

1.  **Obs≈Çuga asynchroniczno≈õci:**

```javascript
// Node.js - wszystko musi byƒá w async/await lub callbackach
async function analizujKeywords() {
  await new Promise((resolve, reject) => {
    // ...
  });
}
// Python - synchroniczny, prosty przep≈Çyw
```

###### **Realny czas developmentu:**

| Zadanie | Python | Node.js |
| --- | --- | --- |
| Napisanie kodu | 30 sekund | 5-10 minut |
| Debugowanie | Rzadko potrzebne | Czƒôsto (typy, async) |
| Instalacja bibliotek | pip install pandas openpyxl | npm install csv-parser exceljs |
| Dokumentacja | Prosta, sp√≥jna | R√≥≈ºna dla ka≈ºdej biblioteki |

To pokazuje dlaczego Python dominuje w analizie danych - robi wiƒôcej przy mniejszym nak≈Çadzie kodu i czasu.

#### üìí Javascript/ Typescript

Kluczowe dla aplikacji webowych. Pisze siƒô w nim zar√≥wno obs≈Çugƒô klikniƒôƒá w przyciski,

#### üè† Html + CSS

Choƒá formalnie nie sƒÖ to jƒôzyki.

HTML HTML + CSS

![image](image%204.png)

### üìö **Podstawowe koncepty w kodzie**

UczƒÖc siƒô jednego jƒôzyka programowania du≈ºo ≈Çatwiej nauczyƒá nam siƒô kolejnych. Podobnie jak poznamy jƒôzyk w≈Çoski ≈Çatwiej nam nauczyƒá siƒô hiszpa≈Ñskiego. Jest tak dlatego, ≈ºe jƒôzyki te operujƒÖ na podobnych konceptach, majƒÖ zbli≈ºonƒÖ sk≈Çadniƒô. Tu u≈ºyjemy Pythona, ale te same koncepty stosowane sƒÖ w innych jƒôzykach.

#### **1. Zmienne - Pojemniki na dane**

Zmienna to jak kom√≥rka w Excelu - miejsce gdzie przechowujesz informacjƒô.

```python
# Zmienne tekstowe (string)
keyword = "buty nike"
url = "https://example.com/buty"

# Zmienne liczbowe
pozycja = 11
liczba_wyswietlen = 15420
ctr = 2.5  # procent

# Zmienne logiczne (True/False)
czy_w_top10 = False
czy_zindeksowana = True

# U≈ºywanie zmiennych
print(f"Keyword '{keyword}' jest na pozycji {pozycja}")
# Wynik: Keyword 'buty nike' jest na pozycji 11
```

**Analogia SEO**: Zmienna to jak parametr w Google Analytics - przechowuje konkretnƒÖ warto≈õƒá kt√≥rƒÖ mo≈ºesz p√≥≈∫niej wykorzystaƒá.

#### **2. Funkcje - Gotowe przepisy**

Funkcja to zestaw instrukcji kt√≥re mo≈ºesz u≈ºywaƒá wielokrotnie. Jak makro w Excelu!

```python
# Definicja funkcji
def sprawdz_pozycje(keyword, pozycja):
    """Sprawdza czy keyword jest w TOP10"""
    if pozycja <= 10:
        return f"‚úÖ '{keyword}' jest w TOP10 (poz. {pozycja})"
    else:
        return f"‚ùå '{keyword}' poza TOP10 (poz. {pozycja})"

# U≈ºycie funkcji
wynik1 = sprawdz_pozycje("buty nike", 5)
wynik2 = sprawdz_pozycje("adidas sneakers", 15)

print(wynik1)  # ‚úÖ 'buty nike' jest w TOP10 (poz. 5)
print(wynik2)  # ‚ùå 'adidas sneakers' poza TOP10 (poz. 15)
```

**Praktyczny przyk≈Çad SEO**:

```python
def oblicz_potencjal_ruchu(impressions, pozycja_obecna, pozycja_docelowa=3):
    """Oblicza potencjalny wzrost ruchu po awansie"""
    ctr_obecny = get_ctr_dla_pozycji(pozycja_obecna)
    ctr_docelowy = get_ctr_dla_pozycji(pozycja_docelowa)

    ruch_obecny = impressions * ctr_obecny / 100
    ruch_potencjalny = impressions * ctr_docelowy / 100

    wzrost = ruch_potencjalny - ruch_obecny
    return round(wzrost)

# U≈ºycie
potencjal = oblicz_potencjal_ruchu(10000, 11, 3)
print(f"Mo≈ºesz zyskaƒá {potencjal} dodatkowych klikniƒôƒá miesiƒôcznie!")
```

#### **3. Listy - Kolekcje danych**

Lista to jak kolumna w Excelu - przechowuje wiele warto≈õci.

```python
# Lista keywords
keywords = ["buty nike", "adidas sneakers", "puma buty", "reebok classic"]

# Dodawanie do listy
keywords.append("new balance")

# Dostƒôp do element√≥w (numeracja od 0!)
pierwszy_keyword = keywords[0]  # "buty nike"
ostatni_keyword = keywords[-1]  # "new balance"

# Przetwarzanie ca≈Çej listy
for keyword in keywords:
    print(f"Analizujƒô: {keyword}")
```

**S≈Çowniki - dane z etykietami**:

```python
# S≈Çownik to jak wiersz w Excelu z nazwanymi kolumnami
dane_keyword = {
    "keyword": "buty nike",
    "pozycja": 11,
    "impressions": 15420,
    "clicks": 234,
    "url": "https://example.com/buty-nike"
}

# Dostƒôp do danych
print(dane_keyword["keyword"])  # "buty nike"
print(dane_keyword["pozycja"])  # 11

# Lista s≈Çownik√≥w (jak tabela w Excelu)
wyniki_seo = [
    {"keyword": "buty nike", "pozycja": 11, "impressions": 15420},
    {"keyword": "adidas sneakers", "pozycja": 5, "impressions": 8900},
    {"keyword": "puma buty", "pozycja": 23, "impressions": 3200}
]
```

#### **4. Warunki - Podejmowanie decyzji**

Instrukcje warunkowe pozwalajƒÖ programowi reagowaƒá r√≥≈ºnie w zale≈ºno≈õci od sytuacji.

```python
pozycja = 11

# Prosty warunek
if pozycja <= 10:
    print("≈öwietnie! Jeste≈õ w TOP10")
else:
    print("Trzeba popracowaƒá nad pozycjƒÖ")

# Z≈Ço≈ºone warunki
if pozycja <= 3:
    status = "ü•á TOP3 - Excellent!"
elif pozycja <= 10:
    status = "‚úÖ TOP10 - Dobra robota"
elif pozycja <= 20:
    status = "üìà TOP20 - Jest potencja≈Ç"
else:
    status = "üí™ Wymaga pracy"

# Operatory logiczne
if pozycja > 10 and impressions > 1000:
    print("Du≈ºy potencja≈Ç - warto zoptymalizowaƒá!")

if ctr < 1 or pozycja > 50:
    print("Pilna interwencja potrzebna!")
```

#### **5. Pƒôtle - Automatyzacja powt√≥rze≈Ñ**

Pƒôtle pozwalajƒÖ wykonaƒá te same operacje dla wielu element√≥w.

```python
# Pƒôtla for - gdy znasz ilo≈õƒá powt√≥rze≈Ñ
keywords = ["buty nike", "adidas sneakers", "puma buty"]

for keyword in keywords:
    print(f"Sprawdzam pozycjƒô dla: {keyword}")
    # Tu normalnie by≈Çoby wywo≈Çanie API

# Pƒôtla while - gdy nie wiesz ile razy
strona = 1
while strona <= 10:
    print(f"Pobieram wyniki ze strony {strona}")
    strona = strona + 1
    # Przerwij je≈õli nie ma wiƒôcej wynik√≥w
    if brak_wynikow:
        break
```

**Praktyczny przyk≈Çad - analiza wielu URL**:

```python
urls_do_sprawdzenia = [
    "https://example.com/kategoria-1",
    "https://example.com/kategoria-2",
    "https://example.com/produkt-1"
]

for url in urls_do_sprawdzenia:
    status_code = sprawdz_status(url)  # funkcja sprawdzajƒÖca
    if status_code == 404:
        print(f"‚ùå B≈ÅƒÑD 404: {url}")
    elif status_code == 200:
        print(f"‚úÖ OK: {url}")
```

#### **6. Klasy - Szablony obiekt√≥w**

Klasa to jak formularz - definiuje strukturƒô danych i co mo≈ºna z nimi zrobiƒá.

```python
class AnalizaKeyword:
    def __init__(self, keyword, url):
        self.keyword = keyword
        self.url = url
        self.pozycja = None
        self.impressions = 0
        self.clicks = 0

    def oblicz_ctr(self):
        if self.impressions > 0:
            return (self.clicks / self.impressions) * 100
        return 0

    def czy_wymaga_optymalizacji(self):
        return self.pozycja > 10 and self.impressions > 1000

# U≈ºycie klasy
analiza1 = AnalizaKeyword("buty nike", "/buty-nike")
analiza1.pozycja = 15
analiza1.impressions = 5000
analiza1.clicks = 50

print(f"CTR: {analiza1.oblicz_ctr():.2f}%")
if analiza1.czy_wymaga_optymalizacji():
    print("üéØ Ten keyword ma du≈ºy potencja≈Ç!")
```

#### **7. Import bibliotek - Korzystanie z gotowych narzƒôdzi**

Biblioteki to gotowe zestawy funkcji kt√≥re mo≈ºesz u≈ºywaƒá.

```python
# Import ca≈Çej biblioteki
import pandas as pd
import requests

# Import konkretnej funkcji
from datetime import datetime
from collections import Counter

# Przyk≈Çad u≈ºycia
# Pandas - jak Excel w Pythonie
df = pd.read_csv('keywords.csv')
top10 = df[df['pozycja'] <= 10]

# Requests - pobieranie danych z internetu
response = requests.get('https://api.example.com/data')

# Counter - liczenie wystƒÖpie≈Ñ
keywords = ["nike", "adidas", "nike", "puma", "nike"]
licznik = Counter(keywords)
print(licznik)  # {'nike': 3, 'adidas': 1, 'puma': 1}
```

#### **8. Obs≈Çuga b≈Çƒôd√≥w - Gdy co≈õ p√≥jdzie nie tak**

```python
try:
    # Pr√≥buj wykonaƒá kod
    response = requests.get(api_url)
    data = response.json()
except requests.exceptions.Timeout:
    print("‚ùå API nie odpowiada - timeout")
except Exception as e:
    print(f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd: {e}")
else:
    # Wykonaj je≈õli nie by≈Ço b≈Çƒôdu
    print("‚úÖ Dane pobrane pomy≈õlnie")
finally:
    # Wykonaj zawsze (np. zamknij po≈ÇƒÖczenie)
    print("Zako≈Ñczono operacjƒô")
```

### üõ†Ô∏è Narzƒôdzia

No code

[https://x.com/Mrcontech/status/1933538141052211478](https://x.com/Mrcontech/status/1933538141052211478)

![image](image%205.png)

[https://bolt.new/](https://bolt.new/)

[https://lovable.dev/](https://lovable.dev/)

[https://v0.dev/](https://v0.dev/)

[https://replit.com/](https://replit.com/)

[https://www.tempo.new/](https://www.tempo.new/)

[https://x.com/Mrcontech/status/1933538141052211478/photo/1](https://x.com/Mrcontech/status/1933538141052211478/photo/1)

Code

Cursor

Windsurf

SuperCode

Claude Code

### üõ°Ô∏è Bezpiecze≈Ñstwo

## <mark><strong>Security is not a feature, it's a requirement.</strong></mark>

![image](image%206.png)

```markdown
‚ö†Ô∏è Z≈ÅOTA ZASADA: Klucze API to jak has≈Ça!
- NIGDY nie wrzucaj ich do GitHub
- U≈ºywaj pliku .env (poka≈ºemy jak)
- Testuj najpierw na ma≈Çych danych

Przyk≈Çad co mo≈ºe p√≥j≈õƒá ≈∫le:
- Wrzucisz klucz API na GitHub = kto≈õ go ukradnie
- Zapƒôtlisz skrypt = 10000 request√≥w do API = rachunek üí∏
```

#### üîç PIP AUDIT - Skaner bezpiecze≈Ñstwa bibliotek Pythona

`pip audit` to narzƒôdzie do skanowania zale≈ºno≈õci Pythona pod kƒÖtem znanych podatno≈õci bezpiecze≈Ñstwa. Jak antywirus, ale dla twoich bibliotek!

##### Instalacja:

```shell
bash
pip install pip-audit
```

##### U≈ºycie:

```shell
bash
# Skanuj obecne ≈õrodowisko
pip-audit

# Skanuj requirements.txt
pip-audit -r requirements.txt

# Automatyczna naprawa (je≈õli mo≈ºliwa)
pip-audit --fix

# Szczeg√≥≈Çowy raport
pip-audit --desc
```

##### Przyk≈Çadowy output:

```text
Found 2 known vulnerabilities in 2 packages
Name       Version  ID             Fix Versions
---------- -------- -------------- ------------
flask      1.0.2    PYSEC-2019-179 >=1.0.3
requests   2.6.0    PYSEC-2021-101 >=2.26.0
```

#### üîç NPM AUDIT - Skaner bezpiecze≈Ñstwa bibliotek Node

`npm audit` to wbudowane narzƒôdzie do skanowania zale≈ºno≈õci Node.js pod kƒÖtem znanych podatno≈õci bezpiecze≈Ñstwa. Jak antywirus, ale dla twoich pakiet√≥w npm!

Instalacja:

```shell
# Wbudowane w npm (6.0+) - nie wymaga instalacji
npm --version
```

U≈ºycie:

```shell
# Skanuj projekt
npm audit

# Automatyczna naprawa (je≈õli mo≈ºliwa)
npm audit fix

# Wymuszenie naprawy (mo≈ºe wprowadziƒá breaking changes)
npm audit fix --force

# Tylko podatno≈õci o wysokim/krytycznym poziomie
npm audit --audit-level high

# Format JSON dla dalszego przetwarzania
npm audit --json

# Szczeg√≥≈Çowy raport
npm audit --parseable
```

**Kiedy u≈ºywaƒá npm audit:**

üîÑ **Regularnie:**

*   Co tydzie≈Ñ/miesiƒÖc w aktywnych projektach
*   Przed ka≈ºdym deploymentem produkcyjnym
*   Po dodaniu nowych zale≈ºno≈õci

‚ö° **Natychmiast:**

*   Gdy otrzymasz alert o nowej podatno≈õci
*   Przed rozpoczƒôciem pracy nad starszym projektem
*   Po klonowaniu repozytorium

üèóÔ∏è **W procesie CI/CD:**

*   Dodaj do pipeline'u jako gate przed deploymentem
*   Zautomatyzuj sprawdzanie w pull requestach

‚ö†Ô∏è **Pilnie:**

*   Gdy twoja aplikacja obs≈Çuguje wra≈ºliwe dane
*   W aplikacjach bankowych/finansowych

**Przyk≈Çadowy output:**

```text
# npm audit report

lodash  <=4.17.10
Severity: high
Prototype Pollution - https://npmjs.com/advisories/782
fix available via `npm audit fix`
node_modules/lodash

2 high severity vulnerabilities

To address all issues, run:
  npm audit fix
```

**Bonus:** Dla wiƒôkszych projekt√≥w mo≈ºna u≈ºyƒá `yarn audit` (Yarn) lub `pnpm audit` (pnpm) - dzia≈ÇajƒÖ podobnie!

#### ‚úÖ Security Rules (wklej to do .cursora)

Skopiuj to do cursorrules i waliduj kod przed deployem

authentication:
- ZAWSZE weryfikuj tokeny przed u≈ºyciem - nigdy nie ufaj inputowi u≈ºytkownika
- NIGDY nie wysy≈Çaj token√≥w w URL/query params - u≈ºywaj Authorization headers
- NIGDY nie loguj pe≈Çnych token√≥w - maksymalnie pierwsze/ostatnie 4 znaki
- U≈ºywaj kr√≥tkich czas√≥w wyga≈õniƒôcia token√≥w (30 min dla access, 7 dni dla refresh)
- Implementuj token rotation i refresh token flow
- Hashuj has≈Ça u≈ºywajƒÖc bcrypt/argon2 z odpowiednim salt

database_security:
- ZAWSZE u≈ºywaj Row Level Security (RLS) w Supabase/PostgreSQL
- NIGDY nie u≈ºywaj service/admin keys w kodzie klienckim
- ZAWSZE u≈ºywaj parametryzowanych zapyta≈Ñ - nigdy string concatenation
- Szyfruj wra≈ºliwe dane przed zapisem do bazy (PII, tokeny, klucze API)
- Regularnie rotuj klucze dostƒôpowe do bazy danych
- Ogranicz uprawnienia u≈ºytkownik√≥w bazy do minimum (principle of least privilege)

input_validation:
- ZAWSZE waliduj i sanityzuj wszystkie inputy u≈ºytkownika
- Sprawdzaj typy danych, d≈Çugo≈õƒá, format, dozwolone znaki
- Odrzucaj podejrzane wzorce (Path Traversal: ../, SQL: '; DROP TABLE)
- U≈ºywaj whitelist zamiast blacklist dla dozwolonych warto≈õci
- Escapuj specjalne znaki przed wy≈õwietleniem (XSS prevention)
- Waliduj zar√≥wno po stronie klienta jak i serwera

```
# Przyk≈Çady
# Autoryzacja
- |
  // ‚úÖ DOBRZE - z headerem
  fetch('/api/endpoint', {
    headers: { 'Authorization': `Bearer ${token}` }
  })

  // ‚ùå ≈πLE - token w URL
  fetch(`/api/endpoint?token=${token}`)

# Walidacja
- |
  // ‚úÖ DOBRZE - w≈Ça≈õciwa walidacja
  const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  if (!emailRegex.test(email)) throw new Error('Invalid email');

  // ‚ùå ≈πLE - brak walidacji
  const email = req.body.email; // u≈ºywane bez sprawdzenia

# Logowanie
- |
  // ‚úÖ DOBRZE - bez wra≈ºliwych danych
  logger.info(`User ${userId} logged in`);

  // ‚ùå ≈πLE - logowanie token√≥w
  logger.info(`Token: ${accessToken}`);

# B≈Çƒôdy
- |
  // ‚úÖ DOBRZE - generyczny b≈ÇƒÖd
  res.status(500).json({ error: 'Internal server error' });

  // ‚ùå ≈πLE - szczeg√≥≈Çy implementacji
  res.status(500).json({ error: err.stack });

# Zapytania DB
- |
  // ‚úÖ DOBRZE - parametryzowane
  db.query('SELECT * FROM users WHERE id = $1', [userId]);

  // ‚ùå ≈πLE - konkatenacja
  db.query(`SELECT * FROM users WHERE id = ${userId}`);
```

supabase_specific:
- ZAWSZE w≈ÇƒÖczaj RLS na wszystkich tabelach
- U≈ºywaj auth.uid() w politykach RLS
- Preferuj anon key nad service key w aplikacji
- Implementuj polityki dla SELECT, INSERT, UPDATE, DELETE osobno
- Testuj polityki RLS przed deploymentem
- U≈ºywaj Supabase Edge Functions dla wra≈ºliwych operacji

deployment_security:
- Skanuj dependencies pod kƒÖtem vulnerabilities (npm audit)
- U≈ºywaj najnowszych wersji bibliotek z patchami bezpiecze≈Ñstwa
- Implementuj CSP (Content Security Policy) headers
- U≈ºywaj HSTS dla wymuszenia HTTPS
- Regularnie przeprowadzaj pentesty
- Monitoruj logi pod kƒÖtem anomalii

monitoring:
- Loguj wszystkie nieudane pr√≥by logowania
- Monitoruj nietypowe wzorce dostƒôpu
- Alertuj przy podejrzanych aktywno≈õciach
- Regularnie przeglƒÖdaj logi bezpiecze≈Ñstwa
- Implementuj audit trail dla krytycznych operacji

compliance:
- Przestrzegaj GDPR/RODO dla danych osobowych
- Implementuj right to be forgotten
- Szyfruj dane PII (Personally Identifiable Information)
- Dokumentuj przep≈Çyw danych osobowych
- Regularnie audytuj zgodno≈õƒá z przepisami

# KRYTYCZNE - NAPRAW NATYCHMIAST:

critical_vulnerabilities:
- Service key u≈ºywany zamiast anon key w Supabase
- Brak weryfikacji token√≥w OAuth przed u≈ºyciem
- Brak RLS na tabelach z wra≈ºliwymi danymi
- SQL injection przez konkatenacjƒô string√≥w
- Logowanie hase≈Ç, token√≥w lub kluczy API
- Hardkodowane klucze API w kodzie klienckim

### üóÇÔ∏è Struktura projektu frontendowego

Projekt generowany przez platformy [bolt.new](http://bolt.new) czy lovable.dev jest oparty na Vite + TypeScript, ze zintegrowanym Tailwind CSS i PostCSS oraz ESLint do lintowania. Jego struktura:

![image](image%209.png)

*   **node_modules** ‚Äì folder z wszystkimi paczkami (zainstalowanymi przez npm/yarn).
*   **src/** ‚Äì kod ≈∫r√≥d≈Çowy aplikacji.
*   **index.html** ‚Äì g≈Ç√≥wny HTML, w kt√≥rym Vite ‚Äúwstrzykuje‚Äù bundlowane skrypty.
*   **package.json** & **package-lock.json** ‚Äì deklaracja zale≈ºno≈õci i skrypt√≥w.
*   **vite.config.ts** ‚Äì konfiguracja bundlera Vite.
*   **tsconfig.json**, **tsconfig.app.json**, **tsconfig.node.json** ‚Äì ustawienia kompilatora TypeScript dla r√≥≈ºnych czƒô≈õci projektu.
*   **tailwind.config.js** & **postcss.config.js** ‚Äì konfiguracja frameworka CSS.
*   **.eslintrc.cjs** ‚Äì konfiguracja lintera ESLint.

### ü§ñ Modele

![image](image%2011.png)

![image](image%2012.png)

![image](image%2013.png)

**Czy najinteligentniejszy model jest najlepszy?**

WybierajƒÖc model musimy przede wszystkim wziƒÖƒá pod uwagƒô w jakim ≈õrodowisku go u≈ºywamy - je≈õli potrzebujemy skryptu bez zale≈ºno≈õci od wiƒôkszej liczby element√≥w systemy to spokojnie mo≈ºemy go napisaƒá z modelem w czacie, ale je≈õli chcemy wykorzystaƒá model jako agenta (np. w Cursorze) to jego skuteczno≈õƒá korzystania z narzƒôdzi jest moim zdaniem istotniejsza ni≈º jego wyniki w testach programistycznych.

![image](image%2014.png)

[https://artificialanalysis.ai](https://artificialanalysis.ai)

#### Testy programistyczne dla modeli

##### **HumanEval**

*   **Najstarszy i najczƒô≈õciej u≈ºywany** benchmark do oceny generowania kodu
*   Sk≈Çada siƒô z **164 prostych zada≈Ñ programistycznych** w Pythonie
*   Zadania to g≈Ç√≥wnie **pojedyncze funkcje** z jasno okre≈õlonymi specyfikacjami
*   **Relatywnie ≈Çatwy** - dlatego najlepsze modele osiƒÖgajƒÖ tu wyniki 95%+
*   Przyk≈Çad: "Napisz funkcjƒô, kt√≥ra zwraca n-ty element ciƒÖgu Fibonacciego"

##### **SciCode**

*   Koncentruje siƒô na **programowaniu naukowym i badawczym**
*   Zadania wymagajƒÖ **g≈Çƒôbokiej wiedzy domenowej** z matematyki, fizyki, chemii itp.
*   Czƒôsto wymaga u≈ºycia **specjalistycznych bibliotek** (NumPy, SciPy)
*   **Trudniejszy ni≈º HumanEval** - testuje zdolno≈õƒá modelu do rozumienia i stosowania z≈Ço≈ºonych algorytm√≥w
*   Przyk≈Çad: "Zaimplementuj metodƒô Monte Carlo do oszacowania warto≈õci liczby Pi"

##### **LiveCodeBench**

*   **Najbardziej realistyczny i wymagajƒÖcy** benchmark
*   Zadania pochodzƒÖ z **prawdziwych konkurs√≥w programistycznych** (LeetCode, Codeforces)
*   Ocena opiera siƒô na **przechodzeniu ukrytych test√≥w**, tak jak w prawdziwym konkursie
*   Testuje **zdolno≈õƒá do rozwiƒÖzywania problem√≥w**, a nie tylko generowania kodu
*   **Najtrudniejszy** - nawet najlepsze modele majƒÖ tu problemy
*   Przyk≈Çad: "Znajd≈∫ najkr√≥tszƒÖ ≈õcie≈ºkƒô w grafie z uwzglƒôdnieniem wag krawƒôdzi"

### üñäÔ∏è Cursor

Najpopularniejszy edytor kodu wsparty AI

Cursor i Windsurf to ‚Äù**zintegrowane ≈õrodowisko programistyczne‚Äù** (IDE - Integrated Development Environment). W praktyce - edytory kodu, majƒÖce pod≈õwietlanie sk≈Çadni, wbudowany terminal, integracje z systemami zarzƒÖdzanie wersjƒÖ i wiele innych mi≈Çych w pracy z kodem funkcji. Co wiƒôcej, a mo≈ºe przede wszystkim, majƒÖ integracjƒô z AI. To daje nam inteligentne podpowiedzi i agenta kt√≥ry pisze kod. Oba oparte sƒÖ o VS Code od Microsoftu, najpopularniejsze IDE na ≈õwiecie. Podobnie Windsurf ma swoje modele SWE-1 ale brak Claude‚Äôa Sonnet 4 to du≈ºy minus aktualnie

#### Setup

##### Privacy mode

ON

##### YOLO mode - auto-run denylist

![image](image%2015.png)

[https://forum.cursor.com/t/cursor-yolo-deleted-everything-in-my-computer/103131](https://forum.cursor.com/t/cursor-yolo-deleted-everything-in-my-computer/103131)

```python
git, rm, opcjonalnie npm, pip, gcloud
```

##### Tab

W≈ÇƒÖczone

##### Modele

[Sensai vibe coding workshop](Sensai%20vibe%20coding%20workshop%20213666c7eacc800498dfd547dd29c4db.html)

##### Rules

[Sensai vibe coding workshop](Sensai%20vibe%20coding%20workshop%20213666c7eacc800498dfd547dd29c4db.html)

#### Prompting

![image](image%2016.png)

![image](image%2017.png)

![image](image%2018.png)

### **Supabase Cheat Sheet**

#### Setup & Installation

```shell
# JS/TS Client
npm install @supabase/supabase-js

# Auth UI (opcjonalne)
npm install @supabase/auth-ui-react @supabase/auth-ui-shared

# SSR helpers (dla framework SSR)
npm install @supabase/ssr

# CLI (global)
npm install -g @supabase/cli
```

#### Environment Variables

```text
# .env.local
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key

# Dla Vite
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-anon-key

# SvelteKit
PUBLIC_SUPABASE_URL=https://your-project.supabase.co
PUBLIC_SUPABASE_ANON_KEY=your-anon-key
```

#### Podstawowy Client Setup

```javascript
import { createClient } from '@supabase/supabase-js'

const supabase = createClient(
  'https://your-project.supabase.co',
  'your-anon-key'
)

export default supabase
```

## Authentication

### Podstawowe Auth Operations

```javascript
// Sign Up
const { data, error } = await supabase.auth.signUp({
  email: 'example@email.com',
  password: 'example-password',
})

// Sign In
const { data, error } = await supabase.auth.signInWithPassword({
  email: 'example@email.com',
  password: 'example-password',
})

// Sign Out
const { error } = await supabase.auth.signOut()

// Get Current User
const { data: { user } } = await supabase.auth.getUser()

// Get Session
const { data: { session } } = await supabase.auth.getSession()
```

### OAuth Providers

```javascript
// Sign in z Google
const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'google',
  options: {
    redirectTo: 'http://localhost:3000/auth/callback'
  }
})

// Sign in z GitHub
const { data, error } = await supabase.auth.signInWithOAuth({
  provider: 'github'
})
```

### Auth State Listening

```javascript
// Listen for auth changes
supabase.auth.onAuthStateChange((event, session) => {
  if (event === 'SIGNED_IN') {
    console.log('User signed in:', session.user)
  }
  if (event === 'SIGNED_OUT') {
    console.log('User signed out')
  }
})
```

## Database Operations

### Basic CRUD

```javascript
// INSERT
const { data, error } = await supabase
  .from('profiles')
  .insert([
    { username: 'john_doe', website: 'https://johndoe.com' }
  ])

// SELECT
const { data, error } = await supabase
  .from('profiles')
  .select('*')

// SELECT with filtering
const { data, error } = await supabase
  .from('profiles')
  .select('username, website')
  .eq('username', 'john_doe')
  .single()

// UPDATE
const { data, error } = await supabase
  .from('profiles')
  .update({ website: 'https://new-website.com' })
  .eq('id', userId)

// DELETE
const { data, error } = await supabase
  .from('profiles')
  .delete()
  .eq('id', userId)
```

### Advanced Queries

```javascript
// Filtering
const { data, error } = await supabase
  .from('posts')
  .select('*')
  .eq('status', 'published')
  .gte('created_at', '2024-01-01')
  .order('created_at', { ascending: false })
  .limit(10)

// Joins
const { data, error } = await supabase
  .from('posts')
  .select(`
    *,
    profiles (
      username,
      avatar_url
    )
  `)

// Full text search
const { data, error } = await supabase
  .from('posts')
  .select('*')
  .textSearch('title', 'supabase')
```

### RPC Calls

```javascript
// Call stored procedure
const { data, error } = await supabase
  .rpc('match_documents', {
    query_embedding: embedding,
    match_threshold: 0.78,
    match_count: 10
  })
```

## Row Level Security (RLS)

### Enabling RLS

```sql
-- Enable RLS na tabeli
ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;
```

### Basic RLS Policies

```sql
-- Policy dla SELECT - wszyscy mogƒÖ czytaƒá profile
CREATE POLICY "Public profiles are viewable by everyone"
ON profiles FOR SELECT
USING (true);

-- Policy dla INSERT - u≈ºytkownicy mogƒÖ wstawiaƒá tylko swoje profile
CREATE POLICY "Users can insert their own profile"
ON profiles FOR INSERT
WITH CHECK ((SELECT auth.uid()) = id);

-- Policy dla UPDATE - u≈ºytkownicy mogƒÖ aktualizowaƒá tylko swoje profile
CREATE POLICY "Users can update own profile"
ON profiles FOR UPDATE
USING ((SELECT auth.uid()) = id);

-- Policy dla DELETE - u≈ºytkownicy mogƒÖ usuwaƒá tylko swoje profile
CREATE POLICY "Users can delete own profile"
ON profiles FOR DELETE
USING ((SELECT auth.uid()) = id);
```

### Advanced RLS Policies

```sql
-- Policy z JWT claims
CREATE POLICY "User is in team"
ON my_table
TO authenticated
USING (team_id IN (SELECT auth.jwt() -> 'app_metadata' -> 'teams'));

-- Policy z joinami (unikaj - wolne)
CREATE POLICY "rls_test_select" ON test_table
TO authenticated
USING (
  team_id IN (
    SELECT team_id
    FROM team_user
    WHERE user_id = (SELECT auth.uid())
  )
);

-- Optimized policy z security definer functions
CREATE OR REPLACE FUNCTION private.get_user_org_role(org_id bigint, user_id uuid)
RETURNS text
SET search_path = ''
AS $$
  SELECT role FROM public.org_members
  WHERE org_id = $1 AND user_id = $2;
$$ LANGUAGE sql SECURITY DEFINER;

CREATE POLICY "Org management restricted to owners"
ON public.organizations FOR ALL USING (
  private.get_user_org_role(id, (SELECT auth.uid())) = 'owner'
);
```

## Realtime

### Basic Subscriptions

```javascript
// Subscribe to table changes
const subscription = supabase
  .channel('profiles')
  .on('postgres_changes',
    {
      event: '*',
      schema: 'public',
      table: 'profiles'
    },
    (payload) => {
      console.log('Change received!', payload)
    }
  )
  .subscribe()

// Subscribe to specific changes
const subscription = supabase
  .channel('todos')
  .on('postgres_changes',
    {
      event: 'INSERT',
      schema: 'public',
      table: 'todos',
      filter: 'user_id=eq.' + userId
    },
    (payload) => {
      console.log('New todo:', payload.new)
    }
  )
  .subscribe()

// Unsubscribe
subscription.unsubscribe()
```

### Broadcast Messages

```javascript
// Send broadcast message
const channel = supabase.channel('room1')

channel
  .on('broadcast', { event: 'test' }, (payload) => {
    console.log(payload)
  })
  .subscribe()

// Send message
await channel.send({
  type: 'broadcast',
  event: 'test',
  payload: { message: 'hello world' }
})
```

### Presence

```javascript
// Track user presence
const channel = supabase.channel('room1')

channel
  .on('presence', { event: 'sync' }, () => {
    const newState = channel.presenceState()
    console.log('sync', newState)
  })
  .on('presence', { event: 'join' }, ({ key, newPresences }) => {
    console.log('join', key, newPresences)
  })
  .on('presence', { event: 'leave' }, ({ key, leftPresences }) => {
    console.log('leave', key, leftPresences)
  })
  .subscribe(async (status) => {
    if (status !== 'SUBSCRIBED') { return }

    await channel.track({
      user: 'user-1',
      online_at: new Date().toISOString(),
    })
  })
```

## Storage

### File Uploads

```javascript
// Upload file
const { data, error } = await supabase.storage
  .from('avatars')
  .upload('public/avatar1.png', file, {
    cacheControl: '3600',
    upsert: false
  })

// Upload z options
const { data, error } = await supabase.storage
  .from('avatars')
  .upload(`${userId}/avatar.png`, file, {
    contentType: 'image/png',
    cacheControl: '3600',
    upsert: true
  })
```

### File Downloads & URLs

```javascript
// Download file
const { data, error } = await supabase.storage
  .from('avatars')
  .download('public/avatar1.png')

// Get public URL
const { data } = supabase.storage
  .from('avatars')
  .getPublicUrl('public/avatar1.png')

// Get signed URL (dla private files)
const { data, error } = await supabase.storage
  .from('private-bucket')
  .createSignedUrl('path/to/file.pdf', 60) // 60 seconds
```

### Storage Policies

```sql
-- Public access policy
CREATE POLICY "Avatar images are publicly accessible"
ON storage.objects FOR SELECT
USING (bucket_id = 'avatars');

-- Upload policy
CREATE POLICY "Anyone can upload an avatar"
ON storage.objects FOR INSERT
WITH CHECK (bucket_id = 'avatars');

-- User-specific policies
CREATE POLICY "Users can upload their own avatars"
ON storage.objects FOR INSERT
WITH CHECK (
  bucket_id = 'avatars'
  AND (storage.foldername(name))[1] = (SELECT auth.uid()::text)
);
```

## Next.js Integration

### App Router Setup

```typescript
// utils/supabase/client.ts
import { createBrowserClient } from '@supabase/ssr'

export function createClient() {
  return createBrowserClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!
  )
}
```

```typescript
// utils/supabase/server.ts
import { createServerClient } from '@supabase/ssr'
import { cookies } from 'next/headers'

export async function createClient() {
  const cookieStore = await cookies()

  return createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        getAll() {
          return cookieStore.getAll()
        },
        setAll(cookiesToSet) {
          try {
            cookiesToSet.forEach(({ name, value, options }) =>
              cookieStore.set(name, value, options)
            )
          } catch {
            // The `setAll` method was called from a Server Component.
            // This can be ignored if you have middleware refreshing
            // user sessions.
          }
        },
      },
    }
  )
}
```

### Middleware (App Router)

```typescript
// middleware.ts
import { createServerClient } from '@supabase/ssr'
import { NextResponse, type NextRequest } from 'next/server'

export async function middleware(request: NextRequest) {
  let supabaseResponse = NextResponse.next({
    request,
  })

  const supabase = createServerClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY!,
    {
      cookies: {
        getAll() {
          return request.cookies.getAll()
        },
        setAll(cookiesToSet) {
          cookiesToSet.forEach(({ name, value, options }) => {
            request.cookies.set(name, value)
            supabaseResponse.cookies.set(name, value, options)
          })
        },
      },
    }
  )

  // IMPORTANT: Avoid writing any logic between createServerClient and
  // supabase.auth.getUser(). A simple mistake could make it very hard to debug
  // issues with users being randomly logged out.

  const {
    data: { user },
  } = await supabase.auth.getUser()

  if (
    !user &&
    !request.nextUrl.pathname.startsWith('/login') &&
    !request.nextUrl.pathname.startsWith('/auth')
  ) {
    // no user, potentially respond by redirecting the user to the login page
    const url = request.nextUrl.clone()
    url.pathname = '/login'
    return NextResponse.redirect(url)
  }

  // IMPORTANT: You *must* return the supabaseResponse object as it is. If you're
  // creating a new response object with NextResponse.next() make sure to:
  // 1. Pass the request in it, like so:
  //    const myNewResponse = NextResponse.next({ request })
  // 2. Copy over the cookies, like so:
  //    myNewResponse.cookies.setAll(supabaseResponse.cookies.getAll())
  // 3. Change the myNewResponse object instead of the supabaseResponse object

  return supabaseResponse
}

export const config = {
  matcher: [
    /*
     * Match all request paths except for the ones starting with:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * Feel free to modify this pattern to include more paths.
     */
    '/((?!_next/static|_next/image|favicon.ico|.*\.(?:svg|png|jpg|jpeg|gif|webp)$).*)',
  ],
}
```

### Server Actions

```typescript
// app/auth/actions.ts
'use server'

import { revalidatePath } from 'next/cache'
import { redirect } from 'next/navigation'
import { createClient } from '@/utils/supabase/server'

export async function login(formData: FormData) {
  const supabase = await createClient()

  const data = {
    email: formData.get('email') as string,
    password: formData.get('password') as string,
  }

  const { error } = await supabase.auth.signInWithPassword(data)

  if (error) {
    redirect('/error')
  }

  revalidatePath('/', 'layout')
  redirect('/account')
}

export async function signup(formData: FormData) {
  const supabase = await createClient()

  const data = {
    email: formData.get('email') as string,
    password: formData.get('password') as string,
  }

  const { error } = await supabase.auth.signUp(data)

  if (error) {
    redirect('/error')
  }

  revalidatePath('/', 'layout')
  redirect('/account')
}
```

## Edge Functions

### Basic Edge Function

```typescript
// supabase/functions/hello-world/index.ts
import { serve } from 'https://deno.land/std@0.170.0/http/server.ts'

serve(async (req) => {
  const { name } = await req.json()
  const data = {
    message: `Hello ${name}!`,
  }

  return new Response(
    JSON.stringify(data),
    { headers: { "Content-Type": "application/json" } },
  )
})
```

### Edge Function z Supabase Client

```typescript
import { createClient } from 'jsr:@supabase/supabase-js@2'

serve(async (req) => {
  const supabase = createClient(
    Deno.env.get('SUPABASE_URL') ?? '',
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? ''
  )

  const { data, error } = await supabase
    .from('countries')
    .select('*')

  if (error) {
    return new Response(JSON.stringify(error), { status: 500 })
  }

  return new Response(JSON.stringify(data), {
    headers: { 'Content-Type': 'application/json' },
  })
})
```

### OpenAI Integration

```typescript
import OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts'

serve(async (req) => {
  const { query } = await req.json()

  const openai = new OpenAI({
    apiKey: Deno.env.get('OPENAI_API_KEY'),
  })

  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: query }],
    model: 'gpt-3.5-turbo',
    stream: false,
  })

  const reply = chatCompletion.choices[0].message.content

  return new Response(reply, {
    headers: { 'Content-Type': 'text/plain' },
  })
})
```

## AI & Vector Search

### Vector Column Setup

```sql
-- Create table with vector column
CREATE TABLE documents (
  id BIGSERIAL PRIMARY KEY,
  content TEXT,
  embedding VECTOR(1536) -- OpenAI ada-002 size
);

-- Create vector index
CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);
```

### Similarity Search Function

```sql
CREATE OR REPLACE FUNCTION match_documents(
  query_embedding VECTOR(1536),
  match_threshold FLOAT,
  match_count INT
)
RETURNS SETOF documents
LANGUAGE sql STABLE
AS $$
  SELECT *
  FROM documents
  WHERE documents.embedding <=> query_embedding < 1 - match_threshold
  ORDER BY documents.embedding <=> query_embedding
  LIMIT match_count;
$$;
```

### Edge Function for Embeddings

```typescript
const model = new Supabase.ai.Session('gte-small')

serve(async (req) => {
  const { input } = await req.json()

  // Generate embedding
  const embedding = await model.run(input, {
    mean_pool: true,
    normalize: true,
  })

  // Search similar documents
  const { data: documents } = await supabase.rpc('match_documents', {
    query_embedding: embedding,
    match_threshold: 0.78,
    match_count: 10,
  })

  return new Response(JSON.stringify({ documents }), {
    headers: { 'Content-Type': 'application/json' }
  })
})
```

## Database Schema Patterns

### User Profiles

```sql
-- User profiles table
CREATE TABLE profiles (
  id UUID REFERENCES auth.users NOT NULL,
  updated_at TIMESTAMP WITH TIME ZONE,
  username TEXT UNIQUE,
  avatar_url TEXT,
  website TEXT,
)
```
