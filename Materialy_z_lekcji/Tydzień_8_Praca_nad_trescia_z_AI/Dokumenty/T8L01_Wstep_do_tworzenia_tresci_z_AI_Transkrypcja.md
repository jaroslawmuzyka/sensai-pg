# T8L01: Wstęp do tworzenia treści z AI - Transkrypcja

**Tydzień:** 8  
**Lekcja:** 01  
**Temat:** Wstęp do tworzenia treści z AI  
**Link do lekcji:** [SensAI Academy](https://learn.sensai.academy/next/public/lesson/349)

---

## Transkrypcja lekcji

Cześć, witam Cię w kolejnym już ósmym tygodniu. W tym tygodniu będzie bardzo praktycznie i skupimy się głównie na automatyzacjach, na workflowach, na pracy z treścią. Tytułem wstępu i przypomnienia różnica między Knowledge Graphem a Information Graphem. Knowledge Graph, Big Picture i holistyczne spojrzenie i Helicopter View i połączone encje i po prostu struktura. Information Graph, wiedza i głębokość. Pamiętajmy o tym, bo będziemy bardzo dużo używać tego w tym tygodniu zamiennie. W tym tygodniu skupimy się głównie nad jednostką treści. Będziemy zarówno ją generować i w procesie będę omawiać kluczowe momenty generacji, które właśnie w kontekście zarówno semantycznego SEO, jak i AI Overview i wszystkich innych rzeczy, z którymi mamy do czynienia, zaczną się łączyć w kropki, że tutaj na przykład musi być taki język, albo taka struktura zdania, albo taka czytelność, albo taka ilość sylab, albo wiele innych rzeczy, które będą zawarte w promptach. W tym tygodniu zachęcam w szczególności w lekcji dotyczącej i w automatyzacji dotyczących generacji treści, żeby się przyjrzeć, co znajduje się w tych promptach, bo jest tam bardzo dużo. Natomiast na samym początku powinieneś pamiętać o jednej rzeczy. Każda jednostka treści Google tag indeksuje. Treść będzie się składała z makro kontekstu, czyli tego co się znajduje na samej górze oraz tego co się znajduje na samym dole treści. Tak też będziemy budować te jednostki treści, układając nagłówki, przez które będziemy przechodzić content i tak dalej, pamiętając o tym, że właśnie to, co najważniejsze ma być u góry definicja encji. Najważniejsze atrybuty encji i inne historie, a na dole na przykład perspektywy. Tak, żeby z każdej strony na każde pytanie zadane przez użytkownika Twoja treść się znalazła, twoja jednostka treści, ale też na każde możliwe pytanie, gdzie wiemy, że w czatach czy w AI Overview człowiek pyta bardzo długimi pytaniami. Te perspektywy, które znajdują się na dole, w mikro kontekście czy w makro kontekście na samej górze były brane do syntezy AIowej. Ok, także w tym tygodniu generacja treści i na samym końcu to, co mam nadzieję, że Wam się spodoba, bo przygotowałem automatyzację również do optymalizacji i jednostki treści z wykorzystaniem modeli reasoningowych, które skutecznie porównają nam zarówno grafy, informacji, grafy wiedzy. Istniejące jednostki treści dadzą Wam sugestie jak optymalizować treści, czego brakuje i skończy się to wszystko bardzo ciekawym raportem. Do tego tygodnia podeszliśmy absolutnie praktycznie, bo dostaliśmy od Was sygnały, że chcecie mieć więcej makeup, więcej automatyzacji i więcej takich rzeczy, więc każdy proces, który dzisiaj pokazuję, jest zakończony automatyzacją. De facto dostajecie cały proces generowania treści i optymalizacji treści na koniec tego tygodnia, bo myślę, że to jest bardzo, bardzo potrzebne i po co? Bo zarówno dobrze przygotowana treść, mam na myśli wygenerowana z odpowiednią semantyką, strukturą. Pod tą lekcją znajduje się szczegółowy opis wytycznych redakcyjnych jak ta treść powinna być zbudowana. Przede wszystkim będzie brał udział w AI Mode i AI Overview i tak samo, tak dobrze zoptymalizowana, atrakcyjna treść pod względem wiecie, tych trójników semantycznych, wypełnienia treści, wiedzy, która tam się znajduje, będzie brała udział w AI Overview, AI Mode i dowolnym czacie. Dostaliśmy od Was bardzo dużo pytań dotyczących właśnie tych wyzwań, więc właśnie tutaj będziemy sobie w tym tygodniu odpowiadać jak tą treść na poziomie jednostki stosownie wypełnić. Ale powiedzmy sobie jak to działa. Będziemy odtwarzać dokładnie to, no niedokładnie ten sam proces, który działa w AI Overview, w AI Mode, ale bardzo, bardzo, bardzo zbliżony w procesie generacji treści, czyli proces RAG-u. Cały ten tydzień właśnie generowania treści, ale pomyślmy o tym w ten sposób. Ja będę odtwarzał dokładnie ten sam proces w kolejnych lekcjach, natomiast w ten sam sposób będzie działał AI Over, działa AI Overview, działa AI Mode i działa każdy chatbot. Czyli na początku pójdziemy do internetu poszukać wiedzy z AI Overview, ze stron, które są w AI Overview. Ze stron, które znajdują się w top dziesięć wyników. Możemy użyć oczywiście Query Expansion, który poznaliśmy w poprzednim tygodniu, żeby tych stron dodać więcej. Ja aż tak nie rozbudowywałem procesów, bo akurat dzisiaj jesteśmy, w tym tygodniu będziemy w Defile, czyli w środowisku, które nie ma takich możliwości jak Python, więc jakby nie będziemy mieli Query Expansion, ale możecie je dodać, więc poszukujemy całej wiedzy, która się znajduje w danym kontekście i budując po prostu pełne pokrycie. Będziemy robić proces data chunkingu, czyli dokładnie tego samego, co robi AI Overview. Jak wiecie te snippety krótkie to jest właśnie udział konkretnych stron internetowych, więc jakby odtworzymy ten proces w tym tygodniu, dzieląc na pytania, odpowiedzi na konkretne malutkie fragmenciki wiedzowe. Będzie RAG, będzie embedding, będzie kilka innych rzeczy. Pokażę jak to, jak to działa, jak to zbudować w Defile, czyli po raz kolejny to samo co robi Google i bazy wektorowe, które zarówno Damian, jak i Roman pokazywali Wam w poprzednich tygodniach. To również będzie w tym tygodniu. Dokładnie odtworzymy sobie identyczny proces w celu generacji, bo identyczny proces stosuje Google i każdy czat w procesie generacji syntezy odpowiedzi dla Was, więc będziemy robić dokładnie to samo. Jeśli chodzi o bezpośrednią generację, będziemy generować dokładnie w identyczny sposób, jak generowany jest AI Overview, czyli question, Ty zadajesz pytanie do wyników wyszukiwania. Naszym pytaniem będzie nagłówek, na który będziemy budować treść. Będziemy wyszukiwać najlepszej wiedzy i dopasowanych fragmentów, żeby ta treść była pozbawiona halucynacji, najbardziej nasycona, trójniki semantyczne, wypełnienie wiedzowe, semantyka, wszystko będzie zrobione. Wszystkie prompty są udostępnione. Humanizacja, generacja, weryfikacja duplikatów, humanizacja, perpleksity, formatowanie. Będzie to wszystko zrobione modele językowe i odpowiedź. Naszą odpowiedzią po tym tygodniu będzie content. A tak działa AI Overview przy okazji, czyli daje Wam odpowiedzi na samej górze wyników wyszukiwania. Zrobimy dokładnie tak samo.To samo. I coś, co zawsze pokazuję od jakiegoś czasu pokazuje na konferencjach. Pokażę to jeszcze raz, bo to będzie zastosowane, żeby ten kontent nasz w tym tygodniu był najlepiej docięty. Ale też przygotowałem pod tą lekcją taki specjalny collab, gdzie będę bawił się modelami rankingowymi w celu pomocy optymalizacji Ci jeszcze lepiej kontentu. Zaraz sobie o tym powiemy, ale zacznijmy od tego, jak to w ogóle działa, bo Google i każda wyszukiwarka musi mieć pewność, że powiedzmy relevance, czyli dopasowanie odpowiedzi czy dopasowanie fragmentu branego do syntezy jest absolutnie najlepsze. My też to wykorzystamy w naszym procesie generacji treści. Dokładnie taką metodę, czyli będziemy my przeszukiwać naszą bazę wiedzy i szukać fragmentów wiedzowych, które zobaczymy w następnym tygodniu. Google w ten sposób układa ranking do AI Overview, czyli my podążamy cały czas tym samym schematem. Szukamy najlepiej dopasowanych fragmentów. Google też szuka najlepiej dopasowanych fragmentów do syntezy, więc robimy de facto ten sam proces. Jak się dłużej nad nim-- znaczy na koniec zobaczysz, że no to po prostu będzie to samo. Wykonamy wyszukiwanie najlepszych fragmentów, czyli to samo, co robi Google w AI Overview i w AI Mode i rankingowania. Co się znajduje wyżej, co się znajduje niżej. Dzięki tym modelom rerankingowym, strony, które nie znajdują się w top dziesięć wyników w Organic Search potrafią znaleźć się w AI Overview, bo są najlepiej dopasowane do modeli językowych i do syntezy przez modele językowe. W takim procesie odtwarzamy dokładnie to samo. Mamy na to stosowny przykład. Pokazywałem go wiele razy, ale dla osób, które jeszcze nie widziały, pokażemy go jeszcze raz. Pytanie: What's the capital city of the United States? Google stoi przed identycznym problemem. Carlson City Commonwealth, Washington DC. Capital Punishment. Kara śmierci. Google musi wybrać fragment, który będzie brał udział w syntezie AI Overview, AI Mode czy ChatGPT, czy Gemini. Gdziekolwiek szukamy, to jest to samo wyzwanie, co najlepiej odpowiada wynikom i my będziemy mieli dzisiaj, znaczy nie wiem, czy dzisiaj, kiedy będziesz oglądał tę lekcję w tym tygodniu to samo wyzwanie, który fragment pasuje najlepiej do nasycenia naszej treści, żeby ona była po prostu najlepsza. I modele rankingowe właśnie nas poinformują, że prawidłową odpowiedzią jest Washington DC z wartością zero dziewięćdziesiąt osiem i Capital Punishment zero dwadzieścia osiem, mimo że to był keyword staffing. I to jest o tyle ważne, że modele rankingowe pozwalają nam, pozwolą nam w dłuższym procesie, który też omówię, zasypać coś, co się nazywa reasoning gap. I to jest największa szansa obecnie optymalizując treści, znaczy nie wiem, czy największa, jedna z większych szans, optymalizując treści, żeby brać udział w syntezie AI Overview. Ponieważ AI Overview AI Mode bierze treści, jakie są, nie mają wyboru. Po prostu nasza konkurencja ma treści, jakie są i oni biorą treści, jakie są. I te treści, które jakie są, biorą udział w syntezie. No i nie wiemy, jakie one są. Może zajść sytuacja, w której model albo halucynuje, bo mu brakuje wiedzy, albo wiedza jest niekompletna. Czyli mamy taką przerwę reasoningową, gap reasoningowy albo odpisze masło maślane. W odpisach AI Overview w poprzednim tygodniu widzieliście samochód. Jest to w języku polskim samochód. No to właśnie to był wynik takiego reasoning gapu i będziemy w stanie zidentyfikować, znaczy na tyle dobrze poprawić naszą treść, żeby reasoning gap może zachodzić w dużo mniejszym stopniu i nasza treść, w tym relevance była najlepiej dopasowana do udziału w syntezie AI w AI Mode czy w AI Overview. Właśnie grając trochę na ten reasoning gap i to, że nasza konkurencja treści ma jakie ma, a my sobie zoptymalizujemy, to znajduje się pod tą lekcją w collabie. Tam jest za dużo tego, żebym to omawiał w tej lekcji. Ta lekcja jest tylko wstępna. Tam się znajduje dokładny opis co się dzieje, co wprowadzić, jaki jest tego wynik ćwiczenia? Omówienie z wykorzystaniem semantyki leksykalnej, sugestie optymalizacyjne właśnie grając na ten reasoning gap również z wykorzystaniem modeli rerankingowych. Bardzo potrzebne to powiedzmy jest to mikro semantyka. Nie chcę wchodzić w tym tygodniu w ten temat i za bardzo w temat teoretyczny. To znajduje się w collabie poniżej. Natomiast zapraszam Cię do lekcji dotyczącej budowania kontentu AI, gdzie znajduje się ogrom wiedzy jak zbudować kontent i ogrom wytycznych redakcyjnych i wszystkiego. Reszta znajduje się pod tą lekcją także miłej zabawy i do zobaczenia na końcu tego tygodnia przy audycie kontentowym, który myślę, że będzie fajnym prezentem dla Ciebie. Cześć!

---

**Powiązane materiały:**
- Notatka z lekcji: [T8L01_Wstep_do_tworzenia_tresci_z_AI.md](./T8L01_Wstep_do_tworzenia_tresci_z_AI.md)
- README Tygodnia 8: [Materiały tygodnia](../README.md) 