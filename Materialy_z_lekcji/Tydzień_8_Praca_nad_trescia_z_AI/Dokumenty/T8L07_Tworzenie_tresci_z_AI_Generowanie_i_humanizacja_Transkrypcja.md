# T8L07: Tworzenie treści z AI - Generowanie i humanizacja - Transkrypcja

**Tydzień:** 8  
**Lekcja:** 07  
**Temat:** Tworzenie treści z AI - Generowanie i humanizacja  
**Link do lekcji:** [SensAI Academy](https://learn.sensai.academy/next/public/lesson/354)

---

## Transkrypcja lekcji

Cześć! Witam Cię w przedostatniej lekcji z serii generowania treści. Dotarliśmy praktycznie już do końca. W tej lekcji omówimy sobie proces generowania treści oraz humanizacji treści, tak żeby dało się ją przeczytać. Mmm, w kolejnej lekcji omówimy sobie sposoby automatyzacji całości procesu. Eem, no i w tej lekcji będzie dosyć dużo kluczowych informacji z perspektywy optymalizacji kontentu i tego, w jaki sposób z jednej strony copywriterzy mogą pisać kontent, ale w jaki sposób będziemy przymuszać trochę AI do pisania po stronie właśnie semantyki, po stronie języka i po stronie łatwości tego informational retrieval, czyli zrozumienia treści. Przygotowałem oczywiście proces. Jest to aktualizacja moich procesów i po kolei sobie omówimy logikę stojącą za, za, za, za, za generacją treści w miejscu, w którym mamy dosłownie wszystko, czyli mamy rag, mamy brief, mamy nagłówki, mamy wiedzę, mamy dosłownie wszystko. Pierwsza sprawa. Proces generowania treści odbywa się nagłówek po nagłówku z przekazaniem całej struktury. Przyjrzyjmy się, co może-- w ten sposób, co będziemy przekazywać. Obecny nagłówek, który generujemy. Dlaczego? Bo generując tyle, jesteśmy w stanie zarządzić prawdopodobieństwem i jakby dopasowaniem takiego fragmentu, jakbyśmy generowali całą treść na jeden raz, to tu na dole prawdopodobieństwo wynosi zero i halucynacja sto procent. A jeżeli mamy tylko tyle, po raz kolejny zminimalizowaliśmy sobie problem. Więc pierwsza rzecz, którą wprowadziliśmy do procesu, to jest nagłówek i będziemy generować nagłówek po nagłówku. Kolejną rzeczą to wiadomo język, no bo robimy to uniwersalnie. Wiedza, która dotyczy danego nagłówka, czyli to, co widziałeś w procesie briefingu. Frazy kluczowe, które dotyczą danego nagłówka i mają się znaleźć w jakichś oczywiście odmianach wykorzystując semantykę leksykalną i inne historie, więc one na pewno muszą się znaleźć w tym konkretnym nagłówku. Gdybyśmy chcieli generować tak długi tekst i wrzucić np. sto słów czy dwadzieścia słów, żeby tam się znalazły, one tam się nigdy nie znajdą. I to, co jest ważne cały zestaw headingów, czy powiedzmy nagłówków, które będą wygenerowane w całej treści po to, żeby powiedzieć AI: Hej, Ty generujesz ten konkretny nagłówek. Twój plan jest taki, czyli wiesz, co już wygenerowałeś, ale co będziesz generował w przyszłości, żeby ograniczyć zjawisko powtórzeń. Żeby sztuczna inteligencja wiedziała: okej, dobra, to teraz generuje co to jest kortyzol, a później będę generował coś tam, więc nie będę pisał o tym, co będę generował w przyszłości, bo teraz napiszę— wiecie, o co chodzi. Umiejscowienie w kontekście, a nie przypadkowe nagłówki i zlepek, który spowoduje to, że nie wyróżnimy w tym momencie makro kontekstu i mikro kontekstu, tylko będziemy mieli przypadkowe, przypadkowe sytuacje i jakby to nazywając cały plan powiedzmy spójnym wektorem kontekstowym czy wektorem heading wektorem, no to, to się, to się rozjedzie. Dlatego zawsze podajemy cały plan. Ja to nazwałem ładnie Done, czyli wiecie, jeżeli wygenerujemy pierwszy nagłówek, no to jest fajnie, on jest pierwszy, ale jeżeli wygenerujemy piąty, to przekazujemy mu to, co do tej pory wygenerował, czyli te cztery wcześniejsze na zasadzie: masz plan, ale to już wygenerowałeś, nie powtarzaj informacji, nie mów o tym. To już jest. Utrzymaj styl, utrzymaj ton i to ma być jeden spójny artykuł, więc będziemy przekazywać wygenerowaną treść. Keyword, czyli główny temat, w którym będziemy się poruszać, nasz jest ciągle: „Co to jest kortyzol?" Żeby wyznaczyć właśnie ten makro kontekst, że Twoim makro kontekstem tego blog postu, tego artykułu, tej jednostki treści jest „Co to jest kortyzol?" I tak masz wprowadzić cały artykuł, bo to jest makro kontekst, najważniejsza informacja dla artykułu. No i tutaj przemyciłem jeszcze taką rzecz dla Was: instrukcje, jeżeli-- dodałem to już tutaj, że pisz w takim stylu, nie mów tego, nasz brand to...”. Takie dodatkowe instrukcje, które można przemycić. Teraz tak: na celach treningowo-edukacyjnych ja tutaj używam modelu 4o mini dlatego, że jest szybki. Natomiast w tym konkretnym przypadku jakość modelu jest bardzo istotna. Co prawda, nie widzę tutaj wielkiego zastosowania dla Reasoningu, chyba że tutaj w procesie weryfikacji, do którego zaraz dojdziemy. Natomiast jeżeli jest nowy model, dużo lepszy, no to warto tutaj dosyć często się aktualizować, testować i badać stabilność procesu. No ale po kolei. Ten kod powiedzmy jest absolutnie nieistotny. Pierwszy raz wprowadziłem Wam w Defy Pytona. Tak można pisać w Pythonie w Defy, ale, ale on dla was nie jest do końca istotny, uwierzcie mi, on jest potrzebny. Co tu się dzieje? Wchodzimy do procesu tymi wszystkimi parametrami zmiennymi, które wam wylistowałem i powiedziałem dlaczego. I zaczynamy poszukiwać wiedzy, czyli to, co mamy tutaj. Zobaczcie. Czyli mamy heading, wchodzimy headingiem: „Co to jest kortyzol?" Wiedzą o tym headingu. Okej, fantastycznie, ale my sobie jeszcze doszukamy tej wiedzy właśnie w RAGu. I zobaczmy sobie to ćwiczenie. Jeżeli wrzucimy mu pytanie, co to jest kortyzol? O, tutaj możemy sobie uruchomić ten konkretny krok. To zobaczcie, on nam wyciągnie z tego... O, widzicie, tu jest właśnie wiedza, co to jest kortyzol. I to są te odpowiedzi dopasowane ze scorem. Tu gdzieś powinien być score.O, z tym scorem, który sobie już omawialiśmy do danego naszego fragmentu. Czyli mamy to uderzenie z dwóch stron wiedzą. W ten sposób niwelujemy wyzwanie halucynacji. Tutaj jest jeszcze ciekawe ustawienia. Zobaczcie tutaj można punktacji, czyli np. uznać, że jeżeli coś jest daleko, powiedzmy nie wiem, tam było zero siedem, to uznajemy, że zero siedem to jest nasz punkt odcięcia. To jeżeli coś jest daleko i niepowiązane z kortyzolem albo nie wnosi wartości bezpośrednio do danego nagłówka, to możemy sobie to odciąć i w procesie, to się znajduje tutaj ustawienia wyszukiwania i sobie możemy to odciąć. No i najważniejsza rzecz generacja. Chciałbym chwileczkę porozmawiać na temat tego promptu. Oczywiście on jest udostępniony dla Was, ale chciałbym wyliczyć kilka kluczowych momentów, w tym w tym prompcie, bo on jest skonfigurowany do generowania blog postów, co do zasady artykułów. Pewnie macie wiele różnych wyzwań, typu opisy kategorii, czy być może nie wiem, oferty czy produkty. Produkty, akurat to będzie inny proces, ale na przykład coś zupełnie innego, nie blog posty. Ten prompt będzie musiał być odpowiednio dostosowany do tego procesu. Ja Wam daję artykuły i pokazuję Wam, co jest najistotniejsze. Headingi mamy omówione, knowledge mamy omówioną, keywordy mamy omówioną. To jest wszystko jasne. Already written part właśnie. Czyli to jest ta rzecz. Hej, to już napisałeś, nie pisz tego. If the section is empty, you are writing content from the beginning. If the section is not empty and shoot that your new content maintain the same language, tone of voice and style and already written part no nie? Czyli żeby cały artykuł był w tym samym tone of voice w tym samym stylu. Jeżeli w dodatkowych informacjach wpiszecie na przykład, że nasz brand voice czy nasz sposób komunikacji to jest whatever, no to, to zostanie dalej wciągnięte. Makro kontekst. No właśnie, to jest istotna rzecz. Tutaj daje tą informację: "This is the overall subject area of related concepts that give meaning of your specific content. Relevant entities, attributes and values should be added to the macro context." No i to jest ten główny temat artykułu i nam zależy na tym, żeby to było na samej górze oczywiście. Oczywiście nagłówki nam definiują, co jest na samej górze, natomiast ma być tylko o tym, bo to jest najważniejsze i Google to tego poszukuje. A na dole wiemy, że jest makro i mikro kontekst, bo to ten mikro kontekst, to są nagłówki, to takie już rozszerzające dookoła do linkowania wewnętrznego, ale ciągle muszą być powiązane do makro kontekstu. Więc to jest bardzo istotna informacja. O właśnie te additional information, jeżeli wprowadzicie, no to są właśnie detail guides, więc, więc to sobie możecie wprowadzać w procesie. No dobra i teraz jakby oczywiście zapoznajcie się, ja Wam wyliczę kilka najważniejszych elementów, które mogą być dla Was istotne. Pierwsza sprawa language rules. Modele językowe co do zasady skonfigurowane są w taki sposób-- znaczy, że nie są skonfigurowane. One mają bias, czyli stronniczość. Są wytrenowane na danych w dużej większości anglojęzycznych amerykańskich, przez co mają tendencję do stosowania gramatyki amerykańskiej. Tych podwójnych myślników. One jakoś się nazywają ładnie. Nie pamiętam nazwy. Jak sobie przypomnę, to Wam powiem hyphen czy jakoś tak. I innego listowania. Więc na pewno musimy zdefiniować konkretną gramatykę i pisownię w języku, w jakim się znajdujemy. Będziemy podawać informację polską, ale każdy, każdy język charakteryzuje się swoją gramatyką, słownictwem, punctuation, interpunkcją. W języku polskim na przykład jak się listuje, tak są listy. No to pierwszy element lista ma dużą literę, na końcu jest przecinek. Kolejny element listy ma mieć małą literę, a ostatni element listy ma mieć kropkę. To takie pamiętam wytyczne, jakie mi dawali edytorzy językowi, na przykład, że tak jest, a domyślnie AI napisze wszystko z dużej litery bez niczego. No bo tak jest po amerykańsku na przykład. Albo po dwukropku duża litera w Polsce mała. No to właśnie tutaj sobie załatwiamy i w ten sposób najczęściej ludzie właśnie rozpoznają treści AI, bo jeżeli nie są zdefiniowane reguły danego języka, to one będą zawsze amerykańskie. Także to jest chyba najistotniejsza informacja dla was. Dodatkowe informacje znajdują się tutaj. Follow this specific content writing rules. Ja w tym momencie Wam powiem, które są tutaj najistotniejsze. Na pewno lexical lexical semantic concept to pewnie trzeba by troszkę lepiej tutaj rozpisać, natomiast ja mam tak. Czyli semantyka leksykalna, czyli kolejne słownictwa, zależności, słowa powiązane i tego typu historie, bo to bardzo pomaga. Dobra i co tu jest jeszcze takiego istotnego? Ok. Jakie tutaj są najważniejsze reguły? Przed chwilą pozwoliłem sobie jeszcze powiedzmy szczerze je sczytać i co do zasady większość jest bardzo ważnych. I ten fragment promptu, ten cały. Zachęcam, żeby każdy z Was sobie przeczytał. Ja zrobię dosłownie kilka i je można uniwersalnie stosować do wszystkich operacji contentowych, jakie robicie. Bo wiecie, bo AI Overview wyciąga od Was wiedzę w procesie data chunkingu, który sobie trenowaliśmy i potrzebuję mieć nasycenia do udzielenia odpowiedzi człowiekowi, czy to będzie AI mode. On zrobi dokładnie to samo w procesie ragowym, który mam nadzieję, że już każdy rozumie co się dzieje. Też na Serp Data pokazywałem Wam te snippety, czyli one są bezpośrednio odpowiedziami na potrzebę wyszukiwarki. Tutaj w tej części promptu jest dokładnie jakby adresowanie tej potrzeby semantycznej. Czyli pisz zwięźle, jak najmniej słów. Znaczy as few words as necessary while covering all perspectives.Czyli mało słów, które pokryją wszystkie perspektywy, czyli krótkie pytanie, krótka odpowiedź. Niestety tego potrzebuje AI. Place the most important information at the beginning of your content. No jasne, no bo jeżeli co to jest kortyzol? Kortyzol to jest x, y, z. Więc jakby to tak też wiecie, wyszukiwarka będzie ignorować w procesie rerankingu, który też Wam omówiłem na przykład coś, co jest niejasne albo gdzieś w połowie jest jakaś definicja, to ma być na samym początku. Simple language and divide filter words and complex phrasing. No jasne, include facts, statistics number and specific data to increase credibility. No tak, no bo wyszukiwarka będzie szukała konkretnych wartości, że na przykład chcesz zrobić jajecznicę, to weź trzy jajka, a nie weź kilka jajek. Oczywiście to, to tak właśnie o to chodzi i to bazy wiedzy, żeby Wam wychodziły zapewne znacznie wychodzić. User intent. Okej, w moim procesie nie ma jakby bezpośrednio definicji jaka jest intencja użytkownika. Pewnie w tym pierwszym kroku budowania bazy, bazy grafów, wiedzy, bazy wiedzy, jakbyście jeszcze mogli sobie zdefiniować- to jest dosyć trudny proces, więc go nie wprowadzałem w DeFi - jaka jest intencja użytkownika i tutaj ją przekazać jednoznacznie, bo w tym przypadku ona będzie definiowana na podstawie wiedzy i Waszego inputu. Lepiej jednoznacznie to wskazywać, żeby model nie odjechał. No ale tutaj w tym procesie jakby to było bardzo trudne w DeFi, żeby to zrobić. Unikaj niejednoznacznych słów. Many severals, no tak, weź, weź dużo jajek, weź kilkanaście jajek, several, kilka jajek, weź trzy jajka. No jasne, to jest właśnie to. I avoid summary at the end. Tak, bo wszystkie, wszystkie typowe chatboty, jeżeli zadacie mu zadanie generacji treści, to na końcu Wam podsumuje. Tylko że my nie piszemy treści takiej długiej, długiej, długiej, tylko piszemy jeden konkretny nagłówek i my nie chcemy mieć podsumowania, bo to nam maksymalnie rozwodni kontekst, ten context dilution. No bo będziemy mieli to samo, później znowu to samo. No i Google generalnie nie lubi takich historii. Po raz kolejny duplikaty. Przemycamy fakty z bazy wiedzy. O, don't add fluff. Czyli nie dajemy wypełniaczy językowych, bo to nam rozwodni znowu relevance score. Maksymalnie krótko, krótko piszemy, jeśli chodzi o zdania, żeby nie były zbyt złożone. Oczywiście jakby definiując długość AI w takiej formie, no to wiemy, że i tak AI zdecyduje. No ale co do zasady możemy próbować. Dokładna odpowiedź jak najszybciej, bo tego poszukuje AI overview. No i wiecie, jesteśmy tutaj cały czas w intencji blogowej. To troszkę pewnie inaczej będzie wyglądać dla e-commerce. Natomiast w intencji blogowej, w intencji informacyjnej, my właśnie musimy tego udzielać, bo tego w procesie retrieval potrzebuję, AI overview to już mówiłem. Zapoznajcie się z tym. Możecie te wytyczne skopiować sobie, przetworzyć je Geminiem i wysyłać do swoich copywriterów. Wszystkie są zasadne pod optymalizację pod AI overview. No i okej, to jest pierwszy krok tego procesu. No i teraz rozdzielamy na, na dwie gałęzie. No niestety w DeFi jest troszkę ułomny, więc te Perplexity na dole i u góry to są jakby elementy tożsame. Ja po prostu tutaj patrzę, czy to jest pierwszy generator, pierwszy nagłówek czy kolejny. Jeżeli kolejny, no to idziemy na górę zweryfikować, czy nie ma duplikatów. Jeżeli pierwszy to idziemy po Perplexity, więc załóżmy, że, że jest drugi, to pójdziemy górną, górną gałęzią. Pierwszą zostawmy. Czyli jeżeli to jest kolejny element, mamy tutaj, mamy tutaj weryfikację, że staramy się wmówić, że jest proofreaderem z głęboką wiedzą w danym temacie i to jest najistotniejsze: based on already written part and sure your content flows seamlessly from it, maintain consistently in style and tone and heading the content, i tak dalej. AI w pierwszym procesie ma bardzo dużą ilość instrukcji. Nie widzę tam zastosowania reasoningu. Więc w tym momencie po prostu ja sprawdzam, czy on na pewno nie powiela informacji w stosunku do tego, co napisał, czy ma spójność językową, czy jedno wynika z drugiego i inne historie. Musiałem tak to rozwiązać. Nie jest to perfekcyjne, natomiast mamy tutaj właśnie to, co już mówiłem w kontekście jakby heading wektoru i tego, że jedno wynika z drugiego, że ensure transition between headings, logical progression and contextual relevance, żeby wszystko było powiązane i stanowiło całość. Czyli taki dodatkowy krok wprowadzam i dopiero wtedy ten content zaczyna nam się budować w sposób powiedzmy no spójny. Nie jest to perfekcyjne. To wynika z problemów niestety już modeli językowych, no ale najlepsze, jakie możliwe na ten moment, jak tworzyłem ten proces do osiągnięcia. Być może tutaj model reasoningowy by się zastosował i poprawiał skutecznie te, te wyniki, ale tylko w tym miejscu. No i kolejne elementy tego procesu. Perplexity, humanizacja i formatowanie. Co do zasady perplexity, humanizacja i formatowanie stanowi proces już humanizacji treści, czyli powiedzmy po weryfikacji, czy nie ma duplikatów mamy jeden, dwa, trzy nagłówki. Pamiętam, ciągle jesteśmy w jednej jednostce, będziemy jakby ją, tą jedną jednostkę, ten nagłówek poprawiać w trzech krokach, żeby był coraz bardziej najbardziej czytelny dla człowieka. I pierwszym takim krokiem jest właśnie perplexity, tak to nazwałem. Natomiast perplexity łączy się tutaj akurat z trzema rzeczami, bo tyle byłem w stanie przemycić w prompcie. Perplectivity jest to miara tego zakłopotania modelu językowego, jak model dobrze przewiduje, przewiduje... mm,Następny token, następne słowo. Im bardziej zakłopotany, tym bardziej odjeżdża i jest, powiedzmy nie, nie, znaczy no, nienaturalnie po prostu. I w ten sposób też działają detektory treści, one mierzą właśnie perplexity i kolejną rzecz, którą zaraz powiem. No i jest teraz niestety tutaj taki trochę kompromis, bo tutaj się znajduje właśnie informacja o tym perplexity. Jeżeli chcecie wiedzieć więcej, poczytajcie sobie o tym perplexity w kontekście generowania treści, bo niskie perplexity jest to spójny tekst, jedno wynika z drugiego. I tutaj właśnie staramy się, żeby kontent został dostosowany do niskiego perplexity, żeby jedno wynikało z drugiego. Było bardzo proste w procesie information retrieval przez wyszukiwarkę. Jedno wynika z drugiego. Spójne, proste, nie ma odjazdów. Relevance bardzo wysoki. Jak byśmy dalej high perplexity, no to by model odjeżdżał dookoła, bo bym mógł na przykład pisać rzeczy bardzo niespójne, trudne do przetworzenia przez maszynę. Natomiast jest tak, że low perplexity, czyli niskie perplexity charakteryzuje pracę modeli językowych. Czyli jeżeli tutaj mamy detekcję treści, no to ten proces spowoduje, że ta detekcja będzie, niestety to jest kompromis, wysoka przez low perplexity. Jakbyśmy mieli dalej high perplexity, to byśmy, ta detekcja byłaby bardzo trudna. Niestety też przetworzenie tego, tego tekstu byłoby bardzo trudne, czyli byśmy nigdy nie uzyskali rankingu w wyszukiwarce, więc to jest kompromis. Dlatego też jest wprowadzona kolejna, kolejne, kolejne elementy tego, tego procesu. Czyli staramy się, żeby treść była jak najbardziej spójna, przewidywalna i najprostsza w odbiorze przez maszynę, godząc się z tym, że będzie łatwiej wykrywalna, ale też dzięki temu maszyna łatwiej będzie mogła wziąć tą treść i umieścić w AI Overview. No niestety. Także tak to wygląda z Perplexity. Natomiast mamy również Beltisness, to jest kolejna miara, która jest brana pod uwagę w procesie detekcji treści. Bo moja humanizacja nie jest omijaniem detektorów, żeby nie było, bo ominąć detektory, które cały czas są rozwijane, te najlepsze detektory, jest naprawdę bardzo trudno. Moja humanizacja jest na kierunku takim, żeby człowiekowi się jak najlepiej to czytało, ale jednocześnie, żeby maszyna, która przyzna mi ranking czy weźmie mnie pod uwagę w AI Overview, najlepiej umiała mnie przeczytać, żeby mnie wzięła do AI Overview i mam sporo takich fraz, gdzie ja się po prostu tam pojawiam dzięki temu. No ale kolejna dobra Beltisness, jest to wybuchowość modelu językowego. Czyli tak jak Damian zazwyczaj to mówi, że nagle rozmawiamy, nie wiem, o kulinariach, a model na przykład odjeżdża na fizykę kwantową, bo coś tam mu się wydarzyło albo odjeżdża od tematu po prostu. No tak się, tak się, tak się zdarza, więc jakby tutaj to jest kolejna rzecz, czyli High Beltisness spowoduje, że ta treść będzie, będzie, będzie, będzie, będzie po prostu bardziej spójna po raz kolejny. No i kilka tutaj wytycznych. Tutaj staramy się policzyć mu sylaby. To jest kolejna rzecz, że niektóre detektory, liczą ilość sylab. AI ma to do siebie, że często generuje bardzo długie zdania, więc jakby liczymy sylaby. Niektóre detektory my jesteśmy w stanie po ilości sylab oszukać. No niestety nie te najlepsze. I coś, co jest absolutnie fajne. Okazuje się, że to jest Flesch-Kincaid Scale. To jest skala, która jest stosowana w Stanach Zjednoczonych, w uniwersytetach i ona określa mniej więcej jakąś tam osobę dziewiętnasto dwudziestoletnią, czyli łatwość czytania w tej konkretnej skali Flesch-Kincaid, żeby po prostu kontent był bardzo łatwy do przeczytania przez człowieka, ale też przed, przez maszynę. Więc jakby to są takie trzy kluczowe, kluczowe dyrektywy w moim procesie humanizacji pod Google'a. Jakby, więc jeżeli macie tutaj coś się wgłębić w proces i eksplorować go dalej, to na pewno te trzy: Perplexity, Beltisness i Redability Flesch-Kincaid Readings East Score, to warto to sobie z nimi się zapoznać. No dobra, to był pierwszy element. Powiedzmy, mamy już tekst trochę wyprostowany, no ale nie do końca z tym like low perplexity, czyli łatwo wykrywalny. Więc podchodzimy teraz pod, pod jakąś humanizację i troszeczkę semantykę leksykalną. Czyli będziemy ten tekst bardziej dla człowieka jeszcze, jeszcze, jeszcze dostosowywać. Czyli zobaczcie, mamy używanie synonimów, ale nie będziemy ich powtarzać. Różne struktury zdań, czyli krótka, krótka, długa, krótka, długa, krótka, długa. No bo AI ma to do siebie, że niestety na przykład będzie pisać bardzo podobnie, więc chcemy to troszeczkę zróżnicować. Po raz kolejny, jeżeli nam się coś powtórzy, bo tak się może wydarzyć, nie będziemy mówić ciągle o tym samym, bo mógł nastąpić po raz kolejny na przykład jakiś tam reasoning gap czy cokolwiek w modelu językowym. Więc jak mamy dwie te same informacje, a Google potrzebuje raz to samo, no to łączymy to w całość. No i-i-i-i, kilku innych, innych rzeczy. Właśnie to avoid over-explaining, bo Google potrzebuje krótkie odpowiedzi, a nie elaboratu w temacie co to jest kortyzol? No i czyszczonko, tak? No niestety, tak to, tak to wygląda. Więc jakby tak to czyścimy, to są najistotniejsze te instrukcje. Macie te prompty udostępnione. Zachęcam do analizy. Zbliżając się do końca mamy proces formatowania, tak żeby tekst był jak najbardziej czytelny, czyli właśnie wprowadzenie list takie, jakie mają być w języku polskim ul i ol. Tutaj akurat ja mam tabelę, ale chyba nigdy ich nie wykorzystywałem.Jeżeli mamy trzy podobne, podobne informacje, to będziemy je listować. Dlaczego? Bo listy dla wyszukiwa-- listy w niektórych intencjach, może inaczej, w niektórych intencjach listy są potrzebne, bo są łatwiej czytalne dla człowieka. Bo na przykład jakie są potrzebne rzeczy, żeby zrobić jajecznice? No to masz listę: jajka, masło, sól, pieprz, na przykład, nie? Ale jednocześnie, jeżeli coś jest ustrukturyzowane dla wyszukiwarki w formie takiej, to jest dużo łatwiejsze do przeczytania, niż ja bym miał przeczytać to zdanie i wyciągnąć z niego wnioski, to pewnie bym musiał się chwilę zastanowić, by mnie kosztowało powiedzmy nie wiem, jeden procent, znaczy pół procenta mojej dzisiejszej staminy. Przeczytanie tego mnie nic nie kosztuje. To mniej więcej dlatego wprowadzam zawsze listowania do mojego kontentu. No i właśnie te po raz kolejny te reguły polskojęzyczne, o których Wam mówiłem. Dlaczego to ma być zrobione? No i kilka rzeczy typu na przykład tutaj wrzucanie boldów czy strongów dla najważniejszych słów kluczowych. Oczywiście można to sobie ograniczać i dostosowywać, więc ten finalny wynik i formatowanie robimy zawsze na końcu. Jeżeli mamy pokusę, a możecie mieć pokusę omijania detektorów treści promptami to zrobić jest bardzo ciężko, chyba, że pozwolimy tutaj odlatywać mu w Perplexity na bardzo wysokie wartości Perplexity i ogólnie zrobimy... Nie wiem co zrobimy, ale po prostu nie będzie się dało tego czytać, więc będzie uznane, że jest tak nieprzewidywalne. Więc na pewno napisał to człowiek, bo jest tak nieprzewidywalny. Czyli będzie zlepek przypadkowych słów. Natomiast jeśli macie pokusę. Detektory treści AI przewidują następny token, czyli na przykład ok, mają tam miarę, te perplexity, te ??? inne historie, ale też mają taką metrykę, powiedzmy, że ok, że, że tutaj mamy słowo na przykład hot. No i w moim, w moim kontekście jest słowo dog. No to on też wie, że hot najczęściej będzie dog i jeżeli u niego i u Ciebie jest to samo, no to on wie, że on by też tak napisał. Czyli potencjalnie już wie, potencjalnie, że powiedzmy w jakimś tam mówię o gigantycznym uproszczeniu, że to prawdopodobieństwo następnego słowa jest tak wysokie, że on też wie, że będzie takie wysokie, więc on uzna, że to jest tekst AIowy. Jeżeli chcemy tego uniknąć w długim longform, znaczy nagłówek będzie tyle zajmował, to możemy zasterować tu następującą funkcją Presence penalty i frequency penalty. Tymi dwoma funkcjami możemy sterować modeli językowych. Damian zapewne o tym omawiał. To jest jakby model sam się kara za powtarzanie, czyli jeżeli nadużywa tego samego słownictwa, tych samych tokenów, tych samych rzeczy, to już użyje raz, ale drugi raz już nie użyje, czyli będzie dużo mniej przewidywalne. No i teraz jest jeszcze jedna informacja dla was. Tutaj temperaturą to Damian tłumaczył. Ja w tym procesie treningowym nie steruję, ale tutaj powinna być na zero siedem mniej więcej. Możemy zastosować ten trik frequency presence i frequency penalty. W generacji weryfikacji byśmy może zrobili raczej reasoning, więc nie. Perplexity w humanizacji, ale w formatowaniu już nie możemy. Czyli na początku możemy troszeczkę posterować tym, że się nie będzie powtarzać, no ale na końcu niestety nie możemy stosować tej metody, więc i tak nam wyjdzie treść napisana przez AI. Dlaczego? No bo jak mamy formatowanie i nam powtórzy B, powtórzy nam strong, powtórzy nam ul i li i tagi HTML i sam się ukara za to, że nam formatuje tekst do HTMLa. No to po prostu będziemy mieli tekst niesformatowany, bo się przestanie nam uznawać tagi htmlowe. Rozwiązaniem ewentualnie jest to, że rezygnujemy z HTMLa i nie chcemy mieć sformatowanego tekstu. No to wtedy możemy sobie pozwolić jeszcze tutaj na to i wtedy content będzie dużo, dużo ciężej. Być może nie będzie-- detektory nie będą w stanie go wykryć. No ale macie kontent niesformatowany, czyli po raz kolejny kompromis. Ja o tych kompromisach myślę w ten sposób, że ja raczej staram się pisać pod ludzi, pod wyszukiwarkę, żeby było pełne wypełnienie wiedzy, żeby nie było halucynacji, żeby było ładnie sformatowane, żeby było readability, żeby to było dla człowieka, jednocześnie dla maszyny, w procesie retrievalu pod AI Mode czy AI Overview, czy chatboty. I godzę się z tym, że może być treść wykrywana. Natomiast jeżeli też są takie skrajne przypadki, że macie tekst na przykład strasznie nasycony faktami dotyczącego, nie wiem, jakiegoś jeziora na przykład i tam są fakty, sto procent faktów, to był Wikipedia. Mamy takie przypadki, gdzie robiliśmy jakby w języku polskim jest takie piękne słowo trawestacja, to mniej więcej my tak samo to zrobiliśmy, czyli rewrite Wikipedii i jakby ustrukturyzacja Wikipedii i przepisanie Wikipedii, ale tam było tyle mięsa w środku, że, że żaden detektor nie jest w stanie wykryć, że to jest tekst pisany przez AI, bo po prostu nie jest w stanie przewidzieć jakiegoś następnego tokenu, bo to jest jakieś jezioro na Mazurach i on nie wie, co tam będzie następne, bo skąd ma wiedzieć, nie? No to ilość wiedzy, którą dostarczycie też może spowodować, że, że będzie bardzo ciężko Was wykryć. A wiedza to jest bezpośrednio wartość. No dobra, odpalmy sobie ten proces. On jest oczywiście udostępniony. Przeczytajcie sobie te prompty, potrenujcie tutaj reasoning w weryfikacji. Ja ich nie trenuję, bo chcę, żeby ten proces na potrzeby treningowe był szybki. Odpalamy i zaraz będziemy sobie go automatyzować. Tego nie robiliśmy wcześniej. Dostaniecie pełną automatyzację. Dobra, zobaczcie. Wchodzimy do procesu nagłówkiem, razem z H2. Niestety, ale tak, tak jest po prostu łatwiej. Językiem tu będzie Polish. Knowledge, czyli odpalimy sobie dwa razy ten proces.To już się pochwaliłem. Automatyzację, którą Wam zrobiłem. No, licz. Keywordem. Headingami wszystkimi. Dla zrozumienia to robimy. Dlatego zapytam Mateusza, czy działa nam internet, internet w studiu, czy... Bo chyba nam nie działa i zaraz odpalimy i się okaże, że nie działa. To mi to nie działa. Dobra, kontynuujemy. Done. Nie uzupełniamy, ponieważ to jest pierwszy nagłówek keyword, czyli ten nasz makro kontekst. Najważniejszy temat naszego artykułu, czyli co to jest kortyzol. Instrukcje. Nie robimy instrukcji, robimy na żywca i zobaczcie po kolei. Kod po prostu usuwa H2, bo RAG nie chce wiedzieć, że to jest H2, więc je usuwam. Proces musi wiedzieć, że to jest H2, żeby umiejscowić w całym content planie. Dlatego usuwam H2, idę do RAG-a i widzicie, tu się zaczyna generować. Teraz przeszedł nam proces Perplexity, właśnie teraz się humanizuje i te wszystkie rzeczy, żeby to było normalnie. W tym procesie możecie dać więcej wytycznych leksykalnych. No i mamy pierwszy fragment, nie? Ok, kawał tekstu widzicie z listowaniem i spoko. Kortyzol to hormon steroidowy. Czyli od razu mamy odpowiedź i wszystkie kluczowe informacje. Mamy boldowanie, czyli mamy co do zasady pierwszy fragment. Teraz, żeby sobie zrozumieć, co tutaj się wydarzy I zaraz przejdziemy do automatyzacji tego procesu. To jest nasz pierwszy fragment. Jak będziemy się automatyzować, mamy już jeden nagłówek. Teraz musimy wygenerować kolejny. Czyli robimy tak. Kolejnym nagłówkiem jest: jakie funkcje w organizmie pełni kortyzol? No i właśnie popełniłem błąd, bo musi być z H2, żeby się odnalazł w całym content planie, czy w content briefie. Nolich jest też dedykowany, bo nolich mamy do, yy, dedykowany nagłówkowi. Keywordy też mamy dedykowane nagłówkowi, żeby mieć pewność, że one tam się znajdą i się znajdują. Będą się znajdowały. Headingi, cały plan, czyli zostaje bez zmian. I zobaczcie. I tu mamy już wygenerowany pierwszy nagłówek, czyli w naszej automatyzacji, którą zaraz Wam pokażę, będzie informacja o pierwszym nagłówku. Makro kontekst zostaje taki sam, żeby kolejny nagłówek dotyczył tego samego, a nie odjechał nam od głównego tematu. No i po raz kolejny zobaczcie, idzie, idzie i teraz pójdzie nam w tą górną gałąź. Czyli jeżeli będzie-- o teraz się content generuje ten pierwszy generat. O i widzicie, jest druga gałąź. Czyli teraz następuje weryfikacja, czy nie powtarza nam informacji, nie powtarza nam słów kluczowych, kontekstów i tak dalej, i tak dalej, czyli nie ma tego samego i jakoś tam prostuje ten content, ten drugi. No i się dzieje teraz jakby już ten proces dostosowania contentu do tego, żeby dało się go przeczytać i był brany pod uwagę przez, przez wyszukiwarki. No i widzicie, i tu była, było zapytanie jakie funkcje spełnia kortyzol w organizmie? I od razu jest odpowiedź. Kortyzol odgrywa istotną rolę w naszym organizmie, zwłaszcza w kontekście realizacji i tak dalej. Czyli od razu krótkie pytanie, krótka odpowiedź i od razu jedziemy z tematem konkretnie. I to, i to, i to działa. Podsumowanie: weryfiku-- generujemy per nagłówek, przekazujemy poprzedni fragment treści już cały wygenerowany po to, żeby mieć pewność, że nie powtarzamy, prostujemy i humanizacja w mojej opinii nie jest to proces unikania detektorów na dzień dzisiejszy. Bo kurczę, to jest tak, że AI Overview AI Mode też jest generatem AI. My musimy mieć najwyższą jakość treści, więc pracujemy po stronie humanizacji, żeby ta jakość była jak najwyższa z perspektywy człowieka, żeby człowiek był zadowolony, ale głównie też maszyny, żeby umiała to czytać. No i tyle. I to działa. Teraz wejdę sobie na jedną stronę na jeden przykład Wam pokażę, że to działa. Jak wczoraj patrzyłem na frazę deep fake. Nie ma overview. Ale mamy też frazy na przykład peer to peer. Ostatnio pokazywałem inne frazy peer to peer, P2P właśnie w Vestigio się znalazło na samej górze nad Wikipedią, bo po prostu ten fragment, który został wycięty z Vestigio niestety nie mamy informacji, który był po prostu najlepiej wygenerowany właśnie pewnie to jest ten fragment pierwszy. No, pierwszy fragment jest Vestigio, czyli właśnie to krótkie pytanie, krótka odpowiedź, krótkie pytanie, krótka odpowiedź i od razu informacja. No to, to właśnie robi ten, ten, ten proces. No dobra, to tyle w tej lekcji. Starałem się omówić logikę. Widzicie ile to jest kroków, żeby wygenerować coś przyzwoitego. Na szczęście pojawiają się modele reasoningowe, które spowodują, że będziemy mogli przyspieszać te procesy. No ale jednak no trochę, trochę się musiało wydarzyć, nim tekst został wyprostowany i generowany faktycznie. Mmm, no to co? W następnej lekcji robimy automatyzację procesu i macie gotowy cały proces contentowy, który możecie używać i. No i super. Do zobaczenia.

---

**Powiązane materiały:**
- Notatka z lekcji: [T8L07_Tworzenie_tresci_z_AI_Generowanie_i_humanizacja.md](./T8L07_Tworzenie_tresci_z_AI_Generowanie_i_humanizacja.md)
- README Tygodnia 8: [Materiały tygodnia](../README.md) 