# T8L09: Tworzenie treści z AI - Audyt contentu - Transkrypcja

**Tydzień:** 8  
**Lekcja:** 09  
**Temat:** Tworzenie treści z AI - Audyt contentu  
**Link do lekcji:** [SensAI Academy](https://learn.sensai.academy/next/public/lesson/350)

---

## Transkrypcja lekcji

Cześć! Witam Was już w ostatniej części tego tygodnia. Będziemy teraz rozmawiać o audycie kontentu. Myślę, że jest to bardzo, bardzo, bardzo potrzebne. Biorąc pod uwagę AI Overview, AI Mode i to, że macie na swoich stronach jestem pewien bardzo dużo starego kontentu, który może performować po prostu lepiej. Tym bardziej, że cała semantyka, znaczy nie cała semantyka, Google poszukuje tych właśnie trójników semantycznych, ekstrahuje wiedzę do syntezy AI, bo po prostu potrzebują wiedzy faktycznej, co widzimy w tych snippetach, które są brane pod uwagę w syntezie czy stronach, które znajdują się w AI Overview. To samo będzie w AI Mode, czyli szukają danych faktycznych w takiej formie, która po prostu będzie dla nich najbardziej zjadliwa, ale jednocześnie wypełni temat. Dodatkowo Google znacznie lepiej ocenia strony, które lepiej pokrywają temat, wyczerpują jak już wiemy wszystkie konteksty, perspektywy i są pełniejsze. I pewnie zadając pytanie, czy Wasze treści istniejące na stronach internetowych takie są, to pewnie to jest różnie i z doświadczenia wiem, że żyjąc przez ostatnie lata w świecie SEO słów kluczowych i SEO narzędzi, które optymalizowały siebie nawzajem, pewnie jest dużo możliwości do poprawy i tym się właśnie zajmiemy. Audyt kontentu możemy podzielić na dwie perspektywy jedną perspektywę big picture, czyli po prostu strony, które nie istnieją, które nie performują. Thin content, content pruning, kategoryzacja treści, jakieś silosowanie, jakieś kanibalizacja i inne historie. To jest duży obrazek, nie będziemy się nim zajmować. My się zajmujemy AI, więc zajmiemy się jednostką treści i semantyką treści w tym tygodniu. Przygotowałem dla Was bardzo prosty proces, oczywiście do Waszej rozbudowy audytu contentowego, który pozwoli Wam optymalizować lepiej istniejące jednostki treści albo przynajmniej zidentyfikować co brakuje, jakich informacji brakuje, jakich słów kluczowych. Zrobimy sobie jakieś podsumowania, jakieś wytyczne i skończymy to bardzo fajną automatyzacją, którą również przygotowałem dla Was z Mateuszem, która pozwoli Wam masowo optymalizować kontent razem z briefem do optymalizacji, z wytycznymi, z podsumowaniem, sugestiami. Więc słuchajcie, zapraszam. I tu właśnie ma niesamowite zastosowanie Reasoning. W poprzednich lekcjach dotyczących contentu bardzo dużo stosowaliśmy zwykłych powiedzmy modeli regularnych, albo inaczej mówiąc nie reasoningowych. W miejscach, gdzie ja widzę, gdzie ja widziałem, gdzie widzę zastosowanie reasoningu to Wam mówiłem, że tu może być reasoning, to trzeba przetestować, tu możecie się tym bawić. Zostawiam Wam trochę przestrzeni do eksploracji własnej. Jeśli chodzi o audyt contentu, praktycznie wszystko robimy na reasoningu, bo to będzie kwestia porównywania jaki jest duży korpus wiedzy w internecie, jak wygląda nasza konkurencja. Jak już wiecie grafy wiedzy, grafy informacji, jakie są braki, jakich nagłówków brakuje, co tam jest big picture w internecie, a czego nie ma od Ciebie, więc jakby bezpośrednio tu mamy do czynienia z reasoningiem i na poprzednich szkoleniach, kursach, kohortach nie mieliśmy jeszcze tych modeli. Teraz mamy, więc macie naprawdę fajny proces przygotowany przeze mnie do swojego zastosowania. Już go po chwili omówię, już go za chwileczkę omówię, jaki jest cel procesu. Celem procesu jest ustalenie, jak można poprawić Wasz kontent. Tak in general, co tam zmienić, czego brakuje? Jakiej wiedzy brakuje w Waszym kontencie? Bo wiemy trudniki semantyczne, ekstrakcja, grafy wiedzy, grafy informacji. Jakich potencjalnie nagłówków brakuje w Waszym kontencie, czyli jak można go rozbudować i jak zedytować istniejące elementy, jakie są braki? No mniej więcej będziemy adresować taki proces. No i oczywiście mam dla Was automatyzację i cały flow. Już Wam opowiadam, co tutaj będzie potrzebne. Tutaj oczywiście założyłem w naszym pliku już cały proces, on jest gotowy na gotowo. Zaraz będziemy tutaj przechodzić. Natomiast chodźmy do flow. Pierwsza sprawa, musimy ustalić keyword, na który chcemy się optymalizować, czy ten seed keyword, znaczy to nie jest seed keyword, ale co do zasady powiedzmy ta najważniejsza fraza, powiedzmy o tak to nazwijmy. Język, gdzie się optymalizujemy, nasz adres istniejącej strony internetowej, graf informacji, frazy kluczowe, które nam wyszły z pierwszego kroku i knowledge graph. I teraz tak, w tym pierwszym kroku graf informacji, frazy kluczowe i knowledge graph, to jest ten pierwszy krok, który robiliśmy w poprzednim segmencie tego tygodnia, dotyczący budowania wiedzy. Po prostu wykorzystamy jeszcze raz to samo, bo tam jest pełna reprezentacja, przynajmniej bardzo duża reprezentacja wiedzy, więc nie będziemy do tego wracać. Powiedzmy w naszym arkuszu, na którym pracowaliśmy, no to są te frazy kluczowe, graf informacji i graf wiedzy, który mamy i tak będziemy sobie ten proces operować. Dla ułatwienia ja już ten proces przeprowadziłem, zaraz sobie tutaj będziemy patrzeć na wyniki. Ja Wam powiem, co się dzieje. Weszliśmy tymi parametrami, o których Wam opowiedziałem. No i co? Pierwsza sprawa, będziemy definiować brakujące informacje. Przyjrzyjmy się temu, co tu się znajduje. No to przede wszystkim tu jest reguła polegająca na tym, że będziemy szukać brakujących informacji z grafu wiedzy, z grafu informacji w porównaniu do istniejącego kontentu. Podsumowanie, listowanie i generalnie brakujące informacje w kontencie w stosunku do tego konsensusu panującego w internecie. Na tej frazie oczywiście. Przypominam, budując wiedzę używamy źródeł AI Overview i top 10 wyników, więc bardzo fajnie. Jeżeli chcemy to rozbudowywać, no to trzeba poszukać więcej, więcej, więcej źródeł. Zostawiam to Wam.Ymm, więc jakby to tak wygląda. Ten prompt jest udostępniony. Zachęcam do do weryfikacji. Ogólnie porówn-- porównywanie. Kolejny krok. Widzicie tutaj rozgałęziłem, żeby ten proces był w miarę szybki, bo jeśli mówimy o resoningu, a tym bardziej o cztery mini, no nie są to najszybsze modele na świecie, więc jakby staramy się urywać ten performance. Będziemy identyfikować brakujące frazy. Czyli wchodzimy do procesu głównym keywordem, żeby wiedział, w jakim tym makro kontekście się po raz kolejny obracamy. No tutaj akurat mamy temat main topic, ale to powiedzmy o-- my jesteśmy blisko makro kontekstu, istniejącym, istniejącym contentem na naszej stronie internetowej i listą słów kluczowych, które zostały zidentyfikowane w procesie ekstrakcji z SERP-ów, czyli wszystkie istotne słowa, które znalazły się wszędzie w wynikach wyszukiwania AI Overview. I takie coś właśnie tutaj będziemy sobie, przepraszam, że tak skaczę, porównywać i po raz kolejny missing keyword and entities should be directly connected to the keywords. No i szukamy, szukamy brakujących fraz kluczowych na stronie internetowej, które występują na AI Overview i w SERPie. Ok, teraz jeżeli mamy brakujące informacje i brakujące frazy kluczowe, dopiero możemy przejść do nagłówków. Czyli dostałem propozycję ok, że mamy istniejący content razem z nagłówkami, które się w nim znajdują. Ma brakujące informacje, brakujące frazy kluczowe, czyli ustawiliśmy już sobie to w procesie, czego nam brakuje. Więc prosimy proces, żeby, żeby nam przygotował nagłówki. One nie mają tak dużo reguł, jak na przykład generatyw w generowaniu contentu. Ja zachęcam, żeby każdy z Was przygotował sobie swoje, swoje reguły w tym procesie. No i teraz najfajniejsza rzecz, jeżeli mamy brakujące informacje, jeżeli mamy brakujące frazy, jeżeli mamy brakujące nagłówki, jesteśmy w stanie przeprowadzić reasoning w formie podsumowania i ten prompt jest absolutnie najważniejszy, ponieważ definiujemy tutaj ogrom wiedzy, którą dostanie model reasoningowy. Im lepszy model, tym lepsze podsumowanie. Zostawiam Wam wybór po Waszej stronie. No i co otrzymamy? Zobaczcie, tutaj jest ładnie zdefiniowany JSON, czyli podsumowanie, czyli krótkie podsumowanie różnic między information grab and existing content. Fajnie. Heading to ads- dostaniecie pełną listę nagłówków do dodania do Waszej strony, żeby ją urozmaicić. Ale nie tylko listę, no bo to jest fajnie, że macie listę, ale dostaniecie wytyczne, jak to dodać i co tam jest istotne oraz dostaniecie informację keyword to use, czyli jeszcze raz content briefu. To, co już sobie przerabialiśmy. Dostaniecie to samo i spis nagłówków do edycji na Waszej stronie z informacjami, jak je należy poprawić, żeby były lepiej zrozumiane przez wyszukiwarkę, jak je uzupełnić oraz listę słów kluczowych, które należy udostępnić, konkretnie uzupełnić konkretne nagłówki, bo po prostu ich brakuje. Tu jest masę reguł. Zapraszam do weryfikacji, do zapoznania się z nimi i zachęcam do zabawy. I coś, co zrobiłem dla Was w prezencie na podsumowanie tego tygodnia, żeby każdy mógł z tego sobie fajnie skorzystać. Tutaj wczoraj przygotowałem taki prompt do tego, żeby zamienić tę strukturę podsumowania na HTML, żeby uzyskać taki prosty raport, w którym będziecie mieli raport optymalizacji, główną frazę URL, jakieś proste podsumowanie, nagłówki do dodania oraz nagłówki do, do edycji. Jak widzimy dostaniemy-- a zaraz tutaj przejdziemy. To jest ten prompt. Chodźmy po kolei na składowe procesu. Także tak wygląda proces. Jest udostępniony. Wygenerowałem już dla Was wynik. Jeszcze go skopiuję stąd, żebyśmy byli on same page. W procesie otrzymamy . Zobaczcie wszystkie brakujące informacje, które istnieją na Waszej stronie internetowej. Ja akurat porównywałem się do strony sylwe pl dotyczący kortyzolu, czyli takiego artykułu, którego nie ma w top dziesięć wyników do powiedzmy, do tego konsensusu, który ustaliliśmy wiedzowego. Więc na tej stronie brakuje tyle kluczowych informacji, które znajdują się w grafach wiedzy i w internecie AI Overview. Tu jest propozycja nagłówków, żeby zaadresować tą wiedzę. Tutaj znajduje się spis słów kluczowych, które są istotne w kontekście kortyzolu i możemy dokonać weryfikacji. Powiedzmy, weźmy rytm dobowy, nie ma. Hormon steroidowy, nie ma. Adrenalina nie ma. Gospodarka cukrowa, nie ma. No i widzicie brakujące frazy kluczowe, które istnieją w temacie, a ich nie ma. Tutaj pewnie można by sprawdzić na przykład zespół Cushinga. Już kilkukrotnie się przemycił, nie ma. Czy na przykład HPA? Nie wiem co to jest HPA, ale pewnie HPA też nie ma. Więc widzicie, reasoning powoduje to, że ten model się nie domyślił, że tam czegoś nie ma, znaczy przewidział w tokenie, że tego nie ma, tylko ten reasoning naprawdę potrafi już całkiem fajnie przeanalizować content. No i tutaj te nasze Jasonowe podsumowanie, które również Wam odkładam do automatyzacji, jakby ktoś chciał coś z tym robić więcej. No i na dole HTML w formie, w formie, w formie raportu. Także tak to wygląda. I zobaczcie, nasz sheet troszkę się rozbudował. Dodałem Wam małe.Dodałem Wam zakładkę Audyt. No i mamy co to jest kortyzol. Język po raz kolejny na selekcie, żeby się przypadkiem nie pomylić. Adres strony do porównania. To już jest gotowe. Zaraz sobie przejdziemy. Te brakujące informacje macie poukładane. Podsumowanie Wam zostawiam w formie Jasona. Jeżeli ktoś chce coś z tym zrobić, ale jeżeli komuś wystarczą brakujące informacje to, to jak najbardziej. Jeżeli komuś potrzebna nagłówki proszę bardzo brakujące frazy, proszę bardzo i gotowa automatyzacja w Make, którą również przetestowaliśmy. Pokrótce ją sobie omówimy, czyli po raz kolejny, już pewnie po tylu tych make'ach, które wykonaliśmy, doskonale wiecie, co się dzieje. Pobieramy dane z tego arkusza ze statusem generuj. Zabezpieczamy, budujemy wiedzę, żeby mieć do czego porównywać w kontekście, w którym się poruszamy. Po raz kolejny zabezpieczamy odpowiedzi, bo w tym momencie jeden proces rozmawia z drugim procesem, więc dostaniemy jakieś odpowiedzi, które przetworzymy do Jasona, żeby cała komunikacja była bezpieczna. No i tutaj wysyłamy do tego procesu audytu contentowego wszelkie informacje zebrane z naszego arkusza oraz poprzedniego procesu wiedzowego. Tworzymy plik na Google Drive i odkładamy dane do sheets’ów. No i co? Brakujące informacje, brakujące nagłówki, brakujące frazy. Jason, jeżeli ktoś potrzebuje się pobawić. I chyba najfajniejszą rzecz, którą mam nadzieję, że będzie przydatna dla Was właśnie już raport w takim formacie. Co to jest kortyzol? Adres strony internetowej, którą optymalizowaliśmy jakieś tam. Widzicie krótkie podsumowanie, że obecny artykuł przedstawia podstawowe informacje o kortyzolu, jego normę, funkcje, bla bla bla bla bla bla bla bla bla bla bla bla bla. Ok, no, czyli jest jakaś charakterystyka tego artykułu. Pobawcie się tym promptem. Nagłówki do dodania, czyli wiązanie kortyzolu, białka z osocza, coś tam. No i informacje, wytyczne jak należy opisać. No i to są słowa kluczowe, które muszą się zawrzeć w tej treści, czy też nagłówki do edycji co to jest kortyzol? Informacje, jakie są brakujące w tym najważniejszym nagłówku pierwszym, czyli rozbudować definicję, podkreślić, że kortyzol to naturalny hormon steroidowy. Oczywiście tej frazy brakuje, więc w takim razie możecie iść dalej do AI i się optymalizować albo do copyrightera. Ja tych promptów trochę też celowo, jako ćwiczenie domowe możecie mieć zadanie, żeby sobie rozbudować prezencję tego albo, albo dane, które tutaj otrzymujecie, one się znajdują tutaj właśnie w Summary i w HTMLu. No i co? Ja myślę, że to jest bardzo przydatna, przydatna funkcjonalność, przydatny, przydatny proces i automatyzacja w obecnych czasach, jeśli chcecie, a raczej musicie zoptymalizować już istniejący content. Gdzie tu szukać ewentualnie ulepszeń? Na pewno w ilości danych, w rodzajach modeli reasoningowych. Tutaj jeszcze modele reasoningowe mają Open AI Reasoning Effort, czyli wysiłek reasoningowy. Być może tutaj możecie się pobawić i spróbować na przykład dać na najwyższym wysiłek, żebym się najbardziej zastanowił. No i tyle. Myślę, że jest to bardzo przydatny flow. Nie będę się już, nie będę wchodzić w szczegóły. Wszystkie prompty są udostępnione. Testujcie, bawcie się, rozwijajcie. Dzięki za ten wspólny tydzień. Mam nadzieję, że warto było dotrwać do końca i zobaczyć właśnie ten audyt, bo on naprawdę jest bardzo, bardzo, bardzo przydatny w 2025 roku w kontekście wszelakim. Dzięki za ten tydzień. Cześć.

---

**Powiązane materiały:**
- Notatka z lekcji: [T8L09_Tworzenie_tresci_z_AI_Audyt_contentu.md](./T8L09_Tworzenie_tresci_z_AI_Audyt_contentu.md)
- README Tygodnia 8: [Materiały tygodnia](../README.md) 