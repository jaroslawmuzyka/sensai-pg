# T8L02: Tworzenie treści z AI - Budowa wiedzy - Transkrypcja

**Tydzień:** 8  
**Lekcja:** 02  
**Temat:** Tworzenie treści z AI - Budowa wiedzy  
**Link do lekcji:** [SensAI Academy](https://learn.sensai.academy/next/public/lesson/346)

---

## Transkrypcja lekcji

Cześć! Witam Cię w kolejnej serii naszego tygodnia dotyczącego kontentu AI. Teraz przejdziemy do budowy procesu generacji treści z wykorzystaniem sztucznej inteligencji. Jest to praktycznie dokładnie taki sam proces, jaki ja wykorzystuję profesjonalnie, więc będziemy sobie, myślę, że robić całkiem ciekawe rzeczy. Proces zawsze będzie dzielił się na kilka takich samych etapów i zawsze proces będzie się rozpoczynać od budowy wiedzy. Jak dobrze wiemy sztuczna inteligencja czy modele językowe to są modele, raczej procesory językowe, a nie żadne źródła wiedzy, więc tą wiedzę należy pozyskać z różnych źródeł. W moim ćwiczeniu będziemy pozyskiwać ją ze stron internetowych, które, które są w wynikach organicznych w Top10. Ze stron, które są wybrane przez Google w snippecie AI Overview. Z snippetów AI Overview, wytniemy sobie je automatycznie oraz jest możliwość, żeby skopiować całe AI Overview do procesu i w ten sposób zbudować wiedzę, na której będziemy dalej budować ciąg dalszy tego, tego procesu. Ta opcja, zaraz będę to dokładnie tłumaczył, skopiowanie AI Overview jest opcjonalna, natomiast wszystkie narzędzia, to też pokażę, które potrafią ściągnąć AI Overview, nie potrafią go ściągnąć w całości, ponieważ tam jest ten przycisk rozwiń dalej. To z tego wynika. Kolejnym procesem będzie proces budowania nagłówków i on jest równie ważny. I okej, budowanie wiedzy jest bardzo, bardzo istotne, natomiast o jakości finalnej treści nie będzie świadczyła jakby generacja, która jest na samym końcu, a o jakości treści będzie świadczyła struktura nagłówków de facto i o tym, czy treść będzie skutecznie wypełniała dany kontekst. To będzie wychodzić z semantyki, również będzie świadczyła struktura nagłówków i dodatkowo czy finalna treść w ogóle będzie brana pod uwagę przez Google to tu będzie po raz kolejny struktura nagłówków i chyba... Okej, jeżeli wiedza będzie słabsza albo niekompletna, to się może wydarzyć w niektórych przypadkach to jest to do przeżycia, ale jeżeli struktura nagłówków będzie zła, to artykuł nie ma, nie ma sensu, więc jakby struktura nagłówków jest super ważna i skupimy się na tym w kolejnej części. Dalej, jak już będziemy mieli i wiedzę i strukturę nagłówków, przejdziemy bezpośrednio do procesu generacji treści i trzeba będzie przygotować nasz rak, o którym już mówiłem w poprzednich lekcjach i o których wspominał Damian w swojej części, czyli jakby zasilanie najbliżej dopasowaną wiedzą konkretnych nagłówków, zapytań czy pytań, które pojawiają się w treści po to, żeby treść była jak najlepsza i optymalizować to całe prawdopodobieństwo, o które walczymy ze sztuczną inteligencją. Także tym się zajmiemy w kolejnych częściach. Data chunking, czyli po prostu to samo, co robi chat GPT, to samo co robi Google w kontekście AI Overview, czyli podzielenia i ekstrakcji faktów dopasowanych do danych nagłówków. Zobaczycie w kolejnych lekcjach jak to będzie zrobione. Każdy proces-- pewnie możemy powiedzieć okej, mamy już wiedzę, ale wiedzę w jakimś tam formacie, mamy nagłówki: „Okej, no to już jesteśmy gotowi do generowania”. No właśnie nie, bo do wiedzy, do nagłówków musimy dopisać frazy kluczowe, które mają się tam znaleźć. To też wynika bezpośrednio z semantyki oraz bezpośrednio wiedzę, że w tym nagłówku masz odpowiedzieć na takie pytania i umieścić takie konkretne informacje, bo są najbardziej kluczowe i przygotować w ten sposób brief już do generowania. Myślimy o tym na takiej zasadzie jeżeli copywriter napisze-- jeżeli copywriterowi wyślemy zestaw nagłówków, czyli np. „Okej, szanowny copywriterze, tytuł naszego artykułu jest to x, y, z. Nasze nagłówki to jest raz, dwa, trzy, cztery, pięć. Pisz". No to dostaniemy coś. Jeżeli opiszemy mu, że w tym nagłówku chcemy mieć to, w tym nagłówku chcemy mieć to, w tym nagłówku chcemy mieć to, chcemy mieć pewność, że tutaj będzie opisane to, to, to, to, to i to. No to na końcu dostaniemy zupełnie inną jakość treści od copywritera. No i z AI pracujemy tak samo. Musimy bardzo dokładnie opisać, co w danym miejscu ma się znajdować. To jest właśnie taki brief generacyjny już na samym końcu. No i finalny proces generowania i humanizacji, de facto jeszcze formatowania treści, którą to zrobimy na samym końcu. No i po tych kilku lekcjach raz, dwa, trzy, cztery, pięć, skończymy z pełnym procesem i z generowaniem treści i z wygenerowaną treścią. Dobra, zaczynamy od pierwszego kroku, czyli idziemy zbudować wiedzę. Wszystkie przykłady, które będę pokazywać w dzisiejszych lekcjach są z wykorzystaniem narzędzia Deefai i w związku z tym jakbyśmy mieli generować taki workflow, jak właśnie tutaj widzimy od początku, zajęłoby mi to bardzo długo. Ja przygotowałem wszystkie workflowy, wszystkie workflowy są wyeksportowane pod lekcjami, więc będzie można zaimportować i bawić się z nimi w swoim środowisku Deefai. No dobrze, po kolei będziemy omawiać, co jesteśmy w stanie tutaj dzięki, dzięki, dzięki procesowi osiągnąć. Jaki jest proces logiczny przedsięwzięcia w celu zbudowania wiedzy naprawdę w niezły sposób. Na samym końcu będziemy wszystko automatyzować Make do Sheetsów. Widzę to w taki sposób, że każda kolejna lekcja będzie dołożenia klocuszka do Google Sheets, a po zakończeniu procesu będziemy mieli kompletnie wszystko zautomatyzowane. No dobra, zobaczcie. Tutaj mamy pole startu, mam nadzieję, że część z Was zapoznała się już z Deefaiem. Mieliśmy kilka lekcji takich wstępnych i będę po kolei również omawiać dosyć dokładnie, co tutaj się znajduje. Żeby przeprowadzić proces generowania treści do Deefaj...Potrzebujemy frazy kluczowej tematu, na który będziemy generować. Ja co do zasady preferuję myśleć tutaj o frazach kluczowych, żeby z nich wyciągać wiedzę i strony. I jeżeli tu wpiszemy temat, który nas interesuje, to też jest ok. Google sobie z tym doskonale poradzi. Kolejną rzeczą jest to language, czyli język, na którym będziemy, w którym będziemy operować, no bo tak też będziemy ekstrahować wiedzę, będziemy wyciągać frazy kluczowe i chcemy, żeby frazy kluczowe były ciągnięte w języku polskim. Czy będziemy w procesie? Zaraz zobaczycie- wyciągać nagłówki od konkurencji, to też chcemy mieć wszystko po polsku. Więc jakby tutaj definiujemy, żeby każdy bloczek AIowy sobie z tym doskonale poradził. No i właśnie tutaj powiedzmy o możliwość, możliwość skopiowania AI Overview z wyników. O co chodzi? Z Mateuszem wspólnie uznaliśmy, że robimy różne ciekawe frazy kluczowe w tym kursie. Mieliśmy już samochód, mieliśmy już kortyzol, mieliśmy kilka innych. Dzisiejszą frazą kluczową, frazą dnia będzie: jak schudnąć. Jak wiemy do wakacji już jest bardzo blisko. Albo jak oglądacie ten kurs, to już są wakacje, więc może jest metoda, żeby schudnąć jeszcze do sierpnia? Dobra, po kolei. Fraza jest jak, jak schudnąć? I ok, tutaj mamy overview. Uuups, wrong. I to jest właśnie urok rzeczy na żywo. Ostatnio Mateusz była fraza samochód i odpowiedzią w overview była: samochód to samochód. Samochód to po polsku samochód. Dzisiaj mam, ale już naprawili, ale Google jeszcze nie wie, jak schudnąć. O dobra . Spis treści jak schudnąć? O właśnie. No, to są uroki AI Overview, szanowni państwo i robienia rzeczy na żywo. Mateusz, musimy zmienić frazę. Może zrobimy frazę: jak schudnąć do lata? Może być? Dobra, co to jest? Co to jest kortyzol? Jak widać kortyzol, kortyzol, kortyzol jest, kortyzol, kortyzol działa. Być może Google nie wie, jak schudnąć. Dobra, nieważne. Zostawmy to. Ok, mamy naszą frazę kluczową: co to jest kortyzol? Powiedzmy, jest to AI Overview, który jesteśmy w stanie skopiować. Dobra, skopiujmy sobie tą treść i chodźmy do mojego procesu. W pliku uruchom wkleimy sobie nasze AI Overview. Naszą frazą będzie: co to jest kortyzol? Ja puszczę ten proces i zwyczajowo tak, jak zwykłem to robić, zawsze w osobnym oknie będę go omawiać, a w tle on sobie będzie sobie, będzie sobie lecieć. Czyli proces mamy puszczony. W tym momencie otwieramy nowe okno w celu omówienia procesu. On sobie tutaj postępuje krok po kroczku, krok po kroczku. Natomiast po kolei patrzymy, co się dzieje. Pierwszy bloczek mamy wklejony AI Overview, frazę kluczową i język, w którym operujemy. Pierwszy bloczek jest to zapytanie do Serp Data, czyli do tego narzędzia, z którym przyszło do nas od Senuto, żeby pobrać top 10 wyników wyszukiwania, ale okazuje się, że nie tylko. Zostanie tutaj pobrane również AI Overview już pokazuje. Co tutaj jest najistotniejsze? Najistotniejszy jest to właśnie ten adres, do którego będziemy się odsyłać oraz fraza kluczowa, którą będziemy przeszukiwać. W tym momencie tutaj, widzicie, nie definiowałem w tym procesie. Jeżeli chcielibyście pracować w Niemczech, w Anglii czy gdzie, gdzie byście chcieli, no to powiedzmy tutaj mamy hl, czyli język polski, geolokalizacja gl, czyli Polska. Jakbyśmy chcieli mieć Niemcy to by było de, de, na przykład jakbyśmy chcieli mieć Stany to pewnie było en i tutaj byłoby pewnie es-us, albo, albo, albo en to jest do weryfikacji, jakie są kodowe. Natomiast Polska jest zawsze Polską. Przypominam, że jeżeli definiujemy sobie język i wyszukiwarkę, w której będzie nasz- nie język, tylko wersję Google i lokalizację, to robimy to tutaj. Jak będziecie chcieli, będziecie musieli, tu jest klejony mój klucz, mój klucz API do Serp, Serpdata. Poproszę Mateusza, żeby, żeby, żeby to zamazał. Wersja w eksporcie, który dostajecie macie wpisane tu jest klucz API. Mniej więcej tak to będzie. Tak to będzie wyglądać. I teraz właśnie popsułem sobie automatyzację. Aha, no dobra. Ok. No i tu mamy mój klucz, klucz API. Wy tutaj wpisujecie sobie swój. Dobra, chodźmy do Serp, Serpdata. Sobie popatrzmy, co tutaj Synoto jest nam w stanie przynieść. Naszą frazą jest: co to jest kortyzol? Więc sobie wyślemy do, tą frazę do SerpData i przyjrzyjmy się, co nam tutaj przynosi SerpData. Zobaczcie. Pierwszą rzecz, którą, która nas interesuje to jest organic results, czyli wyniki organiczne Apteka Melisa, jakieś Sejnevo i Medicover. Ok, czyli to sobie bierzemy pod uwagę. Każdą z tych stron będziemy przeszukiwać i sprawdzać, jaką mają wiedzę. Natomiast SerpData jest o tyle miłe, że ono nam wyciąga overview, więc jakby to jest fantastyczne. Więc jakby doklejamy do naszej analizy wszystkie adresy URL, które znajdują się w AI Overview. I tutaj zobaczcie, tu są właśnie te snippety, ok? Czyli to są jakby te fragmenty, które zostały wycięte ze strony i ja jestem w stanie automatycznie wam wyciągnąć tylko te informacje, no nie? A nas interesuje to, żeby mieć całe AI overview. Dlatego skopiowaliśmy całość, bo te snippety niestety nie złożą się w całość, bo widzicie, to są trzy kropeczki, czyli nie mamy pełnej reprezentacji. Tutaj jest jakaś informacja jeszcze dotycząca AI Overview, jakiś kawałeczek tekstu. Zapewne jest to ten tekst, który się znajduje przed rozwinięciem na samej górze.Więc jakby widzicie różnicę, ile możemy wyciągnąć automatycznie, a ile warto dostarczyć ręcznie. Oczywiście,mm, możecie to robić, możecie tego nie robić w sensie dostarczać ręcznie, kopiować, wklejać całą zawartość. Ja automatycznie wam wyciągam wszystko, co da się wyciągnąć automatycznie w tym procesie. Ok, czyli tak wygląda SERP data. Nie wiem, czy tutaj jeszcze mają people also ask wyciągnięty people also ask. Widzicie, tutaj są pytania nam nie będą potrzebne w tym procesie. Natomiast nie macie odpowiedzi na te pytania, ponieważ no nie da się tego wyciągnąć automatycznie. Dobra, wracamy do procesu. Tutaj sobie proces postępuje, jak widzimy, on sobie tutaj coś robi,mm, pracuje, więc niech sobie pracuje, a my wracamy omawiać sobie krok po kroku co tu się dzieje. Dobra, tu jest pierwszy skrypt, nie będę go omawiał szczegółowo. Napisałem go oczywiście z AI. Jest to skrypt, który,mm, przetwarza wyniki z SERP data do formy zrozumiałej dla DeFi. On jest, on działa. No i zobaczcie, co jest tutaj najistotniejsze. On wyciągnie Wam wyniki wyszukiwania dla organic, organic, eee, organic search i to się znajduje tutaj. Pobiera wszystkie organiki. To zawsze się znajduje, no bo zawsze w organic są wyniki organiczne. Ale jeśli istnieje AI Overview, to jeszcze Wam wy-wytnie AI Overview i usunie te właśnie te końcóweczki, które znajdują się w AI Overview. No i finalnie dostaniecie unikalną listę adresów URL. W tym przypadku akurat zostanie ona u-u-u obcięta do dwudziestu pięciu adresów. D5 w standardowej konfiguracji pozwala przetworzyć jedynie trzydzieści elementów, eee, trzydzieści elementów w tablicy, więc więcej adresów niestety Wam się nie uda przetworzyć. W opcjach konfiguracyjnych i plikach konfiguracyjnych D5 można to zmienić, natomiast zakładam, że macie opcję trzydzieści, ja wpisałem dwadzieścia pięć, ale nie będzie więcej adresów niż w top dziesięć wyników, nie wiem, osiemnaście, czyli pewnie siedem, osiem w AI Overview i dziesięć w wynikach wyszukiwania, więcej nie będzie. Więc jakby w tym procesie to nas nie interesuje. Kolejny bloczek, który umieściłem Wam w procesie, jest to bloczek, który wyciąga te snippety. Zobaczcie, czyli jeśli znaleziono snippet, połącz w ciąg znaków i sprawdź, czy istnieje AI Overview, czyli zawsze sprawdzi, czy istnieje AI Overview w wynikach wyszukiwania i-i SERP data jest w stanie je zidentyfikować. Jeśli tak, to wyciągnie Wam te snippety, które Wam zaprezentowałem i stworzy z nich jakąś tam reprezentację AI Overview. No lepiej wiecie już, żeby to przekleić, no ale gramy kartami, jakie mamy. No i dobra, chodźmy dalej w procesie. Teraz robimy sobie iteracje, powiedzmy w tym miejscu, w którym się znajdujemy w procesie mamy już wyciągnięte co? Top dziesięć wyników. Wszystkie strony, które znajdują się w AI Overview i biorą udział w syntezie AI, czyli potencjalnie oczami AI są to najlepsze źródła, bo biorą udział w syntezie. Wiemy, że strony, które znajdują się w AI Overview mogą się nie znajdować w top dziesięć wyników, dlatego to dołożyłem do procesu i teraz będziemy chodzić po każdej stronie internetowej, mm, i ściągać jej kontent. Ja tutaj dla ułatwienia dla Was, żeby to Wam zawsze działało, dodałem właśnie GINE, czyli ten, tą metody ściągania stron internetowych i przetwarzania ich do Markdownu. Ona nie jest najwyżej, najbardziej optymalną, ale jest najbardziej dostępną metodą, no bo po prostu to jest bloczek w DeFi ju. Dla osób bardziej zaawansowanych pewnie polecałbym stworzenie własnych crawlerów, które będą wyciągać zawartość stron internetowych, bo w całej grze o budowę bazy wiedzy i przy dużej skali chodzi o usuwanie szumu i rzeczy, które nie są potrzebne z contentu, bo wiecie, Was interesuje wiedza, czyli Was interesuje content, czyli Was interesuje największy fragment treści bez zdjęć, bez adresów URL, bez całej nawigacji, bez całego tła, które znajduje się na stronach internetowych, które niestety Gina wyciąga, a crawleri powiedzmy trochę bardziej profesjonalne, nie wyciągają. Mogę Wam udostępnić taki kod, mm, w Pythonie do zabawy własnej, żeby taki crawler sobie zbudować. Co tutaj jest istotne? Jak pewnie się przekonacie albo już przekonaliście Make ma ograniczenia. DeFi daje nam teraz możliwość równoległości. Tego nie było wcześniej i to jest dosyć istotne, czyli cztery strony równolegle będą w tym samym czasie ściągane. Do tej pory była taka jedna, druga, trzecia, czwarta. Teraz cztery na raz. Można robić więcej. Natomiast niestety DeFi ma to do siebie, że jak zrobimy powiedzmy dziesięć wątków, no to lubi się, lubi się zgubić. Ja gdzieś tam identyfikuję, że cztery, pięć wątków równolegle to, to jeszcze nawet, nawet potrafi chodzić. To jest pierwsza rzecz, którą tutaj chciałem podkreślić. Ona jest dosyć istotna. Kolejną rzeczą, która jest dosyć istotna i to przewracało nam procesy w poprzednich kohortach, poprzednich kursach, które prowadziliśmy. To jest jakby ponawianie w przypadku niepowodzenia i to jest okej, bo wtedy Gina może zmienić sobie adres IP, na przykład w procesie albo coś. Więc jakby stosujcie i obsługa błędów, bo do tej pory było tak, że jeżeli używaliśmy Giny w szczególności, to jeśli napotkała błąd typu A nie wiem, strona jest zabezpieczona Cloud Flare'em, czy strona jest zabezpieczona jakimś firewallem, czy nie da się pobrać strony, czy wiele innych powodów, dlaczego nie da się pobrać strony, to proces nam przestawał działać, czyli jak był zautomatyzowany, to automatyzacja nam przestała działać. Wybierajcie sobie zawsze wartość domyślną i tutaj w tekst jak dacie nic to się nie zadziała, ale jak dacie sobie spację to znaczy, że spacja znaczy nic, heh, co do zasady i-i wtedy po prostu ominie Wam stronę internetową, która Wam nie działała, typu nie wiem, to może być Allegro, czy jakieś tam inne historie, mm, które Wam nie, no nie zadziałają Wam, czy X.com, czy wszelkie strony jakieś tego typu, no to wtedy dostaniecie pustą wartość. To jest okej, po prostu proces funkcjonuje dalej. Gdybyście chcieli sobie ograniczać zasób stron, które chcecie właśnie analizować, to w tym skrypcie musicie sobie dopisać, że na przykład, żeby nie wchodził na Allegro, na OLX-a, na jakieś takie serwisy, gdzie no potencjalnie na Allegro, na listingach Allegro nie ma kontentu, więc pewnie nie ma sensu tam wchodzić. To robimy w tym kroku.No i dobra, idziemy dalej już do AI, tutaj znajduje się całkiem jakby pokaźna, pokaźna, że tak powiem, pokaźny flow. Omawiamy sobie po kolei. No bo zobaczcie, weszliśmy adresem strony internetowej mamy top dziesięć wyników, wszystkie wyniki z AI Overview, wszystkie snippety z AI Overview. No i ja wkleiłem AI Overview, czyli mamy AI Overview, czyli całkiem dużo. Natomiast w tym wszystkim będzie się znajdowało sporo brandów, bo tam będzie właśnie, zaraz sobie zobaczymy co jest w top dziesięć. Będzie, no co my tu mamy? Apteka Melisa czy Medicover, czy jakieś, jakieś recepty.pl, Wikipedię, Medunety, Cfarm. Czyli trochę brandów jest i nie chcielibyśmy w naszym kontencie, czy budując wiedzę budować wiedzę, że Cfarm czy Medicover ma taką informację i na tej podstawie dalej sobie syntezować to do treści. Albo że te opony są według opisu strony internetowej xyz. Więc jakby usuniemy sobie brandy. No i co tutaj się dzieje? Zobaczcie, ja będę kopiował te prompty po chwili tak trochę omawiał ich logikę. Czyli Twoim zadaniem będzie właśnie ekstrakcja brandów z domen, z domen internetowych znalezionych w top dziesięć. No bo tak to działa. No domena, trochę brand, tam jeszcze będą tytuły przekazane chyba i tak dalej. No i co? I dostaniesz i masz, masz, masz output podać właśnie w formie brandów i domen internetowych w celu dalszej ekstrakcji. Zobaczycie w kolejnych procesach. No i zobaczcie do, do procesu wchodzimy listą zebranych, zebranych wyników wyszukiwania w top dziesięć, żeby je po prostu wykluczyć z dalszych, dalszych ekstrakcji. No i co? Do zasady tyle. Jest to bardzo, bardzo prosty, bardzo prosty proces. Usuwamy brandy i zobaczcie. Do tej pory w Deefy i w tego typu narzędziach nie dało się robić takich gałęzi. W tym momencie tutaj będziemy operować na czterech gałęziach. Do tej pory był krok po kroku, krok po kroku, też do tej pory, jakiś czas temu krok po kroku i w poprzednich jakby scenariuszach, które Wam udostępniałem, krok po kroku, krok po kroku. Przez co na koniec dnia ten proces wykonywał się bardzo długo. W tym momencie w Deefy możemy robić gałęzie i powiedzmy już sobie je nazwiemy. Pierwszą gałęzią to będzie właśnie gałąź ekstrakcji słów kluczowych, czyli ze wszystkich stron internetowych z AI Overview i z top dziesięć wyników wyciągniemy sobie wszystkie słowa kluczowe istotne w kontekście kortyzolu. Kolejną gałęzią jest to gałąź ekstrakcji bazy wiedzy. Jak wiemy baza wiedzy reprezentowana jest przez trójniki semantyczne, z czego później jest składany graf informacji. I to właśnie się wydarzy w tym miejscu, gdzie przejdziemy przez każdą stronę internetową plus AI Overview i wyciągniemy sobie informacje. I tutaj będziemy właśnie chodzić po stronach internetowych, zaraz Wam to pokażę. A tutaj zobaczcie, mamy taki warunek logiczny, że jeśli AI Overview, tak Wam to zrobiłem, żeby to było w miarę uniwersalne, że jeśli istnieje AI Overview lub wkleiliście AI Overview, to jeszcze wyciągniemy wiedzę z AI Overview. Jeśli nie istnieje, to od razu wyciągamy z top dziesięć wyników tą, tą, tą, tą, tą wiedzę. Także tak wygląda logika tego systemu. Dalej dla porównania i przyszłego budowy, przyszłej budowy nagłówków i content briefingu na poziomie nagłówkowym wyciągniemy nagłówki z konkurencji. Bo w sumie czemu nie? Skoro już mamy wyciągnięty content konkurencji, to wyciągnijmy najbardziej kluczowe nagłówki i na samym dole, żeby mieć pełne pokrycie wiedzy, zrobimy sobie knowledge graph. To nie będzie ten knowledge graph, który robiliśmy w poprzednim tygodniu dotyczącym semantyki big picture, tylko to będzie graf na podstawie promptów Senuto. Bo my już nie potrzebujemy takich dużych połączeń. To z czym jest zależne? Co z czego wynika, jakie są te krawędzie i tego typu historie. Na poziomie contentu potrzebujemy po prostu wiedzieć na temat tego contentu jak najwięcej, więc jeszcze uzupełniam cały proces o graf wiedzy Senuto, żeby dosłownie wiedzieć wszystko. No i kończymy proces. No to po kolei. Usunęliśmy sobie tutaj brandy, czyli powiedzmy mamy już czystą wiedzę i robimy sobie ekstrakcję słów kluczowych. Czyli mamy dziesięć stron internetowych i plus AI Overview. I co tutaj się, co tutaj się dzieje? Wszystkie prompty praktycznie w stosunku do, do poprzednich wersji tego skryptu są zaktualizowane. Czyli jesteś jakimś tam asystentem, którego zadaniem jest ściągnięcie kluczowych encji z podanego contentu. Musisz zidentyfikować encje i najbardziej powiązane highly connected to main topic, to już, to już znacie z poprzednich tygodni, że to ma być na temat kortyzolu i wszystko, co jest bardzo połączone do kortyzolu. No i oczywiście zestaw wszystkich kluczowych, kluczowych, kluczowych instrukcji. Co tutaj jest najistotniejsze? Format. Zawsze taki sam format, żeby dało się z tym rozmawiać w kolejnym bloczku i bloczki ze sobą rozmawiały. I chyba, i chyba tyle kluczowych informacji, w tym, w tym prompcie. Chyba najważniejsze będzie to highly connected to main topic, który określamy jako czy to jest kortyzol, czyli kortyzol. No i tyle. Tu nie ma, nie ma, nie ma więcej kluczowych informacji, o których warto byłoby mówić. To jest bardzo prosta sytuacja. W kolejnym bloczku tutaj będziemy mieli dziesięć, siedemnaście, dwadzieścia w zależności ile stron przejdziemy w kolejnym bloczku. Po prostu powtarzamy to jeszcze raz, wyciągając część wspólną, unikalną i jeszcze dodatkowo filtrując, bo z AI jest tak, że-On lubi być, znaczy AI jest po prostu swego rodzaju prawdopodobieństwem. Natomiast jeżeli jeden bloczek nam coś stworzy, wyciągnie, w tym przypadku powiedzmy dziesięć, siedemnaście razy, to w drugim to po prostu doskonale sobie wyfiltrujemy. I tak też tu zrobiliśmy. Co widzimy, że tutaj mamy: Extracting unique keywords and entities from provided text and directly, directly related to main topic. Czyli ponawiamy filtrację i usuwamy duplikaty dla, dla najlepszego jakości odpowiedzi. Eee, okej, wszystko wiadomo w języku. To jest dosyć istotne, bo ten proces jest zbudowany w ten sposób, że jak będziecie chcieli robić po angielsku czy po niemiecku, to dacie radę to zrobić po niemiecku. Pamiętajcie w Serpdata wpiszcie sobie kraj, w którym chcecie operować, albo dorzućcie to gdzieś na zewnątrz. Trochę tutaj jakby wytycznych semantycznych. Mmm, okej, nic, nic super, super, super istotnego. To wszystko macie udostępnione. Zachęcam. Dobra kolejna sytuacja. Po tych samych stronach będziemy budować graf wiedzy i tutaj się przyjrzymy temu trochę, troszkę, troszkę mocniej, bo tutaj będziemy mieli przede wszystkim to, co już doskonale znamy, czyli semantic predicate from, from provided content. Wyciągamy trójniki semantyczne, jabłko jest czerwone, jabłko jest zielone, jabłko jest takie, bo jak wiemy głębokość właśnie w ten sposób podejścia jest dużo większa niż knowledge grafowy, gdzie jest dookoła, czyli tam jest szerokość, a my chcemy mieć głębokość, czyli maksymalnie pokryty temat. Więc dokładnie to tutaj się dzieje. Robimy to do formatu subject predicate object. Dzięki temu budując nagłówki, będziemy doskonale wiedzieć, że okej w temacie kortyzolu jest to, to, to, to, to, to, to, to, to, to, to, to i to. Więc ja muszę zbudować takie konkretne nagłówki, które odpowiedzą na to, to, to, to, to i to. To jest właśnie proces i to, to, to robi, to robi nam różnicę. Jak widzicie tutaj wszystko mamy opisane, ale to już pewnie do moich promptów się i Damiana przyzwyczailiście. No i example jak to ma wyglądać i to co do zasady jest bardzo stabilne, bardzo stabilne. Ekstrakcja nagłówków, czyli kolejna logika, którą tutaj sobie omówimy. No ale pewnie możecie się domyślić, że to jest po prostu extracting. Pewnie możecie się domyślić, że jest to po prostu ekstrakcja nagłówków H2, H3 ze stron internetowych. No bo one są najistotniejsze i po prostu zareprezentowane w formie, w formie, w formie nagłówków. Okej, to co się znajduje na dole, to jest ten knowledge graph, czyli u góry sobie budujemy bardzo precyzyjną wiedzę w tym konkretnym jednym kontekście. Na dole sobie budujemy big picture, ale nie taki big. Nie będę tego promptu omawiać. Damian Sałkowski go chyba omawia w swojej części. Jest to prompt stosowany przez Senuto do budowania wiedzy i on da zobaczyć zupełnie inne rezultaty i inne rezultaty niż, niż, niż graf informacji. Jest to trochę zbudowane na kanwie promptów Microsoftu i ich podejścia do, do encji, do relacji. Zobaczycie, że to jest potrzebne w kolejnych procesach, więc będziemy sobie tego używać. No i teraz tak. Co my tutaj mamy dalej? Za każdym razem zostanie wyciągnięta część wspólna z fraz z grafu informacji. Zaraz pójdziemy do tego i z headingów usunięcie nagłówku to jest jasne. Natomiast tutaj macie dołożoną ekstrakcję AI overview. No i to już widzieliśmy. To jest identyczny prompt jak, jak w przypadku grafu informacji, czyli tych trójników semantycznych, bo to są trójniki semantyczne, więc czy to jest AI overview, czy strona internetowa, to jest ciąg znaków ze spacją, więc jakby to jest to samo. No i okej, jednym z najistotniejszych promptów będzie ten tutaj. Można mu się troszeczkę lepiej przyjrzeć. Możecie sobie go poczytać, jakie są tutaj założenia, bo w tym momencie przede wszystkim, a będziemy nad nim mocno pracować, więc przekazujemy go dalej w JSON-ie, to już wiecie dlaczego. Te modele są naprawdę bardzo dobre w przekazywaniu danych w JSON-ie i to będzie zawsze stabilne. I widzicie, tutaj będzie budowanie przede wszystkim zawsze, jeżeli budujemy graf informacji, mapy informacji, graf informacji, semantic predicates. Jest tutaj troszeczkę filtracji. Wszystko oczywiście po raz kolejny musi być related do dodanych keywordów. Encje, relacje i atrybuty w formie JSONa. I tak to zawsze ma wyglądać. No i troszeczkę jakby wytycznych dotyczących fokusu tematycznego i innych informacji. Wszystko jest udostępnione dla Was. Sprawdzajcie sobie logikę. I teraz tak, ten prompt tu jest najważniejszy. Ten jest dosyć ważny tu knowledge graph, ale on jest bardzo stabilny, więc jakby zaraz wam powiem o co chodzi. On jest dosyć ważny. Powiedzmy frazy kluczowe my wyciągamy dlatego, że możemy, a nie dlatego, że są nam potrzebne. A w związku z tym, że możemy, no to po prostu je wyciągamy. No i headingi też możemy, ale nie są najistotniejsze. Jeżeli chcecie zwiększać jakość odpowiedzi, to zobaczcie. Ja każdy z tych promptów tutaj oparłem na GPT for All Mini, czyli na dosyć już na dzień dzisiejszy starym modelu, ale na potrzeby edukacyjne szybkim, więc jakby dlatego go wybrałem. Jeżeli chcecie uzyskiwać lepsze wyniki i lepiej dopasowane odpowiednio lepszej jakości odpowiedzi, te modele trzeba zmieniać na coraz lepsze. Natomiast tylko i wyłącznie ten prompt ostatni, czyli budowa grafu informacji zasługuje, żeby można tutaj było użyć modelu reasoningowego. Żaden z innych nie ma takiej potrzeby i stosowanie modeli rezoningowych jest bezzasadne, więc jeżeli chcecie uzyskać dużo lepszą odpowiedź na samym końcu, ewentualnie reasoning możecie wprowadzić tylko i wyłącznie tutaj. Tutaj pozwoliłem sobie w tym knowledge grafie wrzucić GPT4e mini. Dlaczego? Dlatego, że tutaj wrzucam od razu wszystkie strony internetowe.Eee, i z bardzo dużej ilości kontentu robię ten graf w jednym uderzeniu. Wynika to z tego, że nie chcę sobie dzielić już na małe części, tylko z całości robię i ten cztery jeden mini ma po prostu największe okno kontekstowe, więc tylko on był mi w stanie obsłużyć ten proces należycie. W żadnym innym przypadku nie ma takiej potrzeby, bo w tym momencie tu sobie dzielimy na malutkie części, iterując po każdej stronie internetowej, wyciągając z małej strony internetowej jeszcze mniejszą część. A w tym momencie wrzucamy tutaj wszystkie strony internetowe, to z tego wynika mój dobór. I co do zasady nie ma potrzeby tutaj wrzucać nic, nic lepszego. No dobra. To tyle omówienia logicznej struktury budowania wiedzy, gdzie możemy szukać jakby rozwinieć, jeżeli byście chcieli rozwijać ten proces, ja go nie będę rozwijać już dla Was, ale możecie sobie to potraktować jako zabawę dla chętnych. Pierwsza sprawa, jeżeli chcecie mieć lepiej docięte wyniki, to na pewno można wymienić tą Ginę tutaj na, na coś, co lepiej będzie filtrowało strony internetowe. I wiecie, im czystsze dane, tym wynik będzie lepszy. Trochę nie jest to do końca shit in & out, ale im czyściej, tym na końcu czyściej. Przy okazji taniej, przy okazji szybciej. I wiele innych argumentów za tym, żeby tutaj sobie wymieniać, wymienić ten proces. Ja zostawiam jak jest. Kolejną rzeczą możecie w tym prompcie tutaj zmieniać, wycinać znaczy w tym skrypcie wycinać domeny, po których nie chcecie chodzić. To są możliwości rozbudowania. Co do Serpdata nie macie żadnych możliwości rozbudowania, bo jest jaki jest. Natomiast tu jakbyście chcieli w tym miejscu o mniej więcej tu, jakbyście sobie zrobili query expansion, które pokazywałem w poprzednim tygodniu i na przykład zrobili tak jak chat GPT obecnie, że przepisali frazę na przykład tą frazę tutaj na pięć pytań i na przykład poszukali pięć pytań i przetworzyli wszystko dalej, no to, to są metody, metody na to, żeby ten proces był lepszy. Gdybyście chcieli to zrobić jeszcze super lepiej, to potencjalnie tutaj moglibyście te pytania przetłumaczyć na język angielski i zobaczyć co jest w Anglii czy tam w Stanach Zjednoczonych i, i to wszystko dołączyć do procesu. Natomiast ja tego nie, nie, nie robię na tym, na tym, na tym poziomie. Po prostu informuję, gdzie ja widzę jakieś możliwości rozbudowy. No dobra, to tyle logicznie. Chodźmy do wyników, które sobie do procesu, do wyników w drugim oknie. No dobra, jesteśmy, jesteśmy już przy wynikach, czyli to, co nam się wykonało w tle i będziemy sobie po kolei omawiać to, co uzyskaliśmy. Przy takich procesach zachęcam Was, żebyście sobie klikali tutaj. O, zobaczcie wyśledzenie. No i tutaj będziemy widzieć co się dzieje. Zobaczcie, czyli tutaj mamy właśnie zapytanie do Serpdata i okej, to jest bardzo miłe, że nam odpowiadają. Natomiast zebrane URL-i zobaczcie, przetworzyliśmy właśnie w tym skrypcie to, co Wam mówiłem, czyli odpowiedź jaką jest Serpdata do formatu, gdzie mamy adresy po prostu wszystkich stron internetowych z overview i search, więc jakby to jest okej. Natomiast bardziej ciekawa jest ilość danych, którą my jesteśmy w stanie wyciągnąć automatycznie. Zobaczcie z Serpdata automatycznie z overview jesteśmy w stanie wyciągnąć tylko tyle, więc warto czasami skopiować. Nie ma na razie lepszego rozwiązania z Googla wynik, no bo po prostu jest większa objętość. Jak nam bardzo zależy, to z tego to wynika. I tutaj możemy sobie debugować jakieś tam poszczególne, poszczególne okienka i patrzeć, co się w danej chwili wydarzyło. I zobaczcie, cały proces za pierwszym razem trwał czterysta dziewiętnaście sekund. No i potencjalnie będziemy mieli z nim problem, jeśli chodzi o Make'a, bo Make ma limit trzystu sekund. Będziemy zaraz próbować go przyspieszyć albo ponawiać, zobaczymy, co nam z tego, z tego wyjdzie, pewnie. Dlaczego jeszcze powiedziałem, że warto tutaj wymienić Ginę? Gina nie jest najszybsza i pewnie z tego wynika ten czas, ponieważ całe zbieranie danych ze strony internetowej trwało, tutaj widzimy pięćdziesiąt sekund, pewnie można by to ograniczyć, natomiast najdłużej trwało równolegle osiemnaście, osiemnaście, osiemnaście, osiemnaście. No i tu jakaś bzdura, bo, o nie, o nie, o nie, czterdzieści sekund równolegle. Dobra, nie będziemy się tym zajmować. Przyjrzyjmy się po kolei wynikom. Znajdują one się tutaj, a będę, będę konkretne części, skopiujemy wszystko sobie do notatnika, konkretne części będę Wam omawiać, co my tutaj mamy. Czyli pierwsza sprawa otrzymaliśmy zestaw słów kluczowych, na podstawie których zbudujemy nasz artykuł, bo wiemy, że one się znajdują zarówno w overview, są najbliżej powiązane do kortyzolu i znajdują się na stronach internetowych. Potencjalnie dobrze je mieć na swojej stronie, czy to do linkowania wewnętrznego, czy, czy to do planowania nagłówków. Po prostu one muszą się znaleźć, no to wynika później dalej z tych embeddingów, dystansów, semantyki i innych historii, więc jakby je wyciągamy. Kolejną rzeczą jest to właśnie ten information graph, czyli wszystkie informacje dotyczące korty-kortyzolu, na podstawie których będziemy sobie budować ten kontent. Widzimy, że ten graf nie jest największy na świecie. To nie wynika z tego, że my tej wiedzy nie mamy. My mamy tą wiedzę, tylko zastosowany jest model 4o mini. Stosując lepsze modele, tak jak mówiłem, te grafy będą dużo lepsze. Kolejną rzeczą, która nam się przyda, są to nagłówki H2 i H3. One są najważniejsze, które znalazły się w wynikach wyszukiwania i będziemy sobie tak dalej planować oraz w celu uzupełnienia i posiadania dużego obrazka. Tutaj mamy prompt od Senuto. Znamy podobny prompt z poprzedniej lekcji semantyki, bo mamy wprowadzenie opisu i mamy wprowadzenie siły powiązań. Ona nie jest nam potrzebna do budowania kontentu, tak jak do linkowania wewnętrznego czy struktury. No ale mamy, jest to w miarę uniwersalne, więc, więc to mamy. Jest to w zupełności inny prompt niż ja stosowałem do budowania grafów w Neo4j, dlatego, że tam mieliśmy konieczność tych relacji. Jedno wynika z drugiego. W przypadku budowy kontentu my takich konieczności nie mamy, takiej potrzeby nie mamy. Więc jakby to jest nasza odpowiedź i potencjalnie jest to pełna reprezentacja, którą potrzebujemy do uzyskania kontentu. No dobra, proces działa. W kolejnej części tej lekcji pokażę Wam jak to zautomatyzować i zbudować ten nasz docelowy automat do budowania treści.

---

**Powiązane materiały:**
- Notatka z lekcji: [T8L02_Tworzenie_tresci_z_AI_Budowa_wiedzy.md](./T8L02_Tworzenie_tresci_z_AI_Budowa_wiedzy.md)
- README Tygodnia 8: [Materiały tygodnia](../README.md) 