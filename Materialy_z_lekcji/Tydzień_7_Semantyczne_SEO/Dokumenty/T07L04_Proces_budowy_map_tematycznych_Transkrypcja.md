No dobra, w poprzedniej lekcji pokazałem Ci podstawowe zastosowanie jak można wykorzystać deep research i zbudować graf wiedzy. Teraz przejdźmy do trochę bardziej zaawansowanych pomysłów, konceptów i rozwiązań. Omówię w kolejnych lekcjach, to jest lekcja wstępna, proces, mój proces, tak jak ja o tym myślę, budowania topical mapy. Oczywiście zachęcam Was na końcu, żebyście mój proces rozwijali, rozmawiali z AI jak to rozwinąć, dodawali rzeczy, których nie przewidziałem na potrzeby edukacyjne, a będzie ich trochę, będę mówić, jakie rzeczy można w których miejscach pododawać. OK, wszystko zaczniemy od query expansion, czyli od rozbudowywania słów kluczowych, bo Google też tak robi, bo samochód coś tam znaczy, jest encją samochód. Natomiast Google w wynikach wyszukiwania zazwyczaj przepisze słowo samochód na, na przykład użytkownik pyta czym jest samochód albo czym jest samochód albo samochód nowy samochód używany, nie wiem, to ta fraza zostanie na pewno przepisana. Deep Research żeby zostać, żeby został przeprowadzony również miał zadany temat samochód i Gemini tam przepisało frazę samochód np. do dziesięciu fraz typu czym jest samochód, czym charakteryzuje się samochód, jakie są samochody i tak dalej. Więc my odtwarzamy proces, który istnieje w Google. Będzie osobna lekcja, będziemy o tym szerzej rozmawiać. W każdym razie to jest pierwszy krok i to będzie następna lekcja. Dalej Deep SERP Analysis, tak to ładnie nazwałem. Jak już jesteśmy w kontekście tych pięknych buzzwordów czy słówa anglojęzycznych, powiedzmy bardzo głęboka analityka SERPów, ponieważ dla zrozumienia keyword research jest to jakby wyplucie jakichś słów kluczowych z jakąś tam zależnością z jakiegoś tam narzędzia po jakimś uważaniu narzędzia i po aktualizacji datasetu czy bazy danych narzędzia. W Google tu i teraz istnieje ustalony konsensus tu i teraz przez wyszukiwarkę, który się zmienia w trybie nie wiem codziennie. Codziennie dochodzi nowa informacja, codziennie jest nowa rzecz, codziennie jest coś nowego w wyszukiwarce Google. Jeżeli nie codziennie, to w tygodniowym interwale czy w miesięcznym interwale. Jeżeli nie w miesięcznym interwale, to na pewno w interwale przy kolejnym update'cie. Daje nam to ogromną przewagę, bo jeżeli nasza konkurencja bazuje tylko na zestawach słów kluczowych z narzędzi, to pewnie następna aktualizacja może być za pół roku albo i dalej. Natomiast w Google aktualizacja wiedzy jest dzisiaj i jutro i w przyszłym tygodniu, więc głęboko chodząc po Google'u, to też będę o tym pokazywał, jak to działa i analizując anturaż tematyczny i wszystko, co się znajduje głębiej np. zaczynamy od samochodu, a przechodzimy przez query expansion na zestaw słów i wchodzimy magię głęboko w Google, no nagle będziemy mieli pokrycie, zaraz to zobaczycie w wiedzy tak jak Google myśli o danych tematach. Więc jakby będziemy sobie robić coś takiego. Dalej będziemy chodzić po setkach stron internetowych i dokonywać czegoś na zasadzie dekompozycji, czyli ha! No to jest to. Nawet miałem na to jakiś ciekawy przykład, że na siłowni można robić trzy rzeczy: masę, ale masa nie jest korzystna w perspektywie strategii SEO, bo po prostu rośniemy w objętość, a nie wiemy, a ciągle mamy tłuszcz, czyli np. ten content sprzed pięciu lat, o którym mówiłem w pierwszej lekcji. Możemy robić redukcję, czyli chudniemy np. Content Proning. No i to też jest. Możemy na tym zyskać? Możemy, ale co do zasady nie będziemy grubym wiórem. Albo możemy robić rekompozycję, czyli po prostu spalać tłuszcz i budować masę mięśniową. To jest najlepszy, najlepszy kierunek. I ta właśnie ta dekompozycja, rekompozycja na samym końcu, bo na początku mamy dekompozycja, później rekompozycja SERPów do grafów wiedzy i do Topical Map nam odpowie na pytanie, jak Google rozumie grafy dla danych słów kluczowych, jak rozumie dla danej, dla danej niszy i na podstawie tego zbudujemy sobie topical mapę. W kolejnym kroku tych map, tych grafów będą setki. Zobaczymy jak nam pójdzie z analizą słów kluczowych, w tych ćwiczeniach kolejnych. No do czata już to nie da, to się nie da już do czatu to, to nie da rady. Więc jakby wprowadzę tutaj jedną bazę grafową, bardzo przyjemna. Neo4j jest za darmo. Pokaże o co chodzi, bo są różne bazy, są bazy wektorowe. O tym mówił Damian i Roman, gdzie przechowujemy wektory. Są bazy relacyjne typu Supabase, czyli PostgreSQL czy MySQL, gdzie przechowujemy relacje, ale są też bazy grafowe, gdzie przechowujemy grafy i pewnie Microsoft Knowledge Graph przechowuje w podobnej bazie. Gdzie tego będzie bardzo dużo, zobaczycie, więc wprowadzimy sobie taką bazę w kolejnych lekcjach. Pobawimy się tam wizualizacjami i zobaczycie jakie to jest logiczne, jak tam rzeczy ze sobą mają faktyczne znaczenia w świecie, w tych grafach i jak jedno się łączy z drugim i wiecie co, można cyklicznie odświeżać i to wszystko będzie się rozbudowywać na podstawie wyników wyszukiwania. Pokażę Wam, jak pobierać z tej bazy, bazy informacje. No i na końcu zrobimy te ćwiczenia akurat w czacie. Waszą pracą domową będzie napisanie swoich procesów z wykorzystaniem API. Jak można po tym wnioskować? Zbudować frazę kluczową. De facto powtórzymy ten sam proces, żebyście zobaczyli różnicę między, między tym malutkim grafem, a takim dużym, który wytworzymy. Jak te tematy rosną? No i w sumie tyle. Zaczynamy z Query Expansion. 