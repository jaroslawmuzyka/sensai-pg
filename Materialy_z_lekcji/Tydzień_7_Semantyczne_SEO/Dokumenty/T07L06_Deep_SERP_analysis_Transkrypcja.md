Dobra, jesteśmy w następnym kroku. Mamy wykonany ten query expansion, czyli duży, widzimy duży obrazek tematu, w którym się znajdujemy i do tego zainspirował mnie taki graf, który znalazłem dawno temu jak widzimy po prawej stronie na dole 2023 rok i tutaj chłop chyba albo ktoś, jakaś pani, nie wiem kto, zrobili analizę polegającą na tym, że przez przeskakiwanie autosuggest dla frazy chat GPT versus wchodzili pięć kroków głębiej, głębiej, głębiej, czyli jakiś tam pewnie na jednej stronie masz, jak masz autosuggest czy jakiś related queries, pewnie masz nie wiem, osiem, dziesięć słów kluczowych, czyli pierwszy krok dziesięć, później z dziesięciu kolejne dziesięć, dziesięć, dziesięć, więc wyszło tego dosyć dużo. No i po sklastrowaniu tych wyników okazało się, że jak zaczęli od core'u, czyli od tego, co my zaczynamy, czyli samochód na samym środku, to nagle wybudował im się big picture. Obrazek dookoła tematyczny, czyli z chata GPT przeszli do Berta, to już omawialiśmy, czy tam Image Generation, czyli generowanie obrazu przez sztuczną inteligencję AI Agent. Widzimy, że w 2023 roku ten segment AI Agent był mały. Pewnie jakby ponowić ćwiczenie w 2025 roku, to ten segment AI Agent byłby ogromny. No ale właśnie to jest to, że ta wiedza w internecie rośnie, żyje, niekoniecznie tak szybko jak zestaw słów kluczowych w narzędziach czy tag generation, SEO tools czy tam inne historie, które są powiązane do czata GPT. I robimy to samo. Robimy to samo, czyli wychodzimy od frazy kluczowej samochód, raczej od tego query expansion, który wykonaliśmy, czyli już dużego obrazka i będziemy się przeklikiwać w Google'u przez related queries. Dla zobrazowania procesu mamy samochód i w trakcie przygotowania do tej lekcji, tutaj Mateusz Wyszogrodzki docenił właśnie odpowiedź AI Overview, że samochód to po polsku samochód. Mateusz się śmieje. Ja też, dziękujemy za informację. Tu kiedyś było takie chyba, że koń jak koń, każdy widzi czy coś takiego. No właśnie. I zobaczcie. Wracając do tematu tu mamy people also search for, czyli mamy snippet polegający na tym, że ludzie również szukają tych rzeczy, czyli samochodu. Przeskoczymy sobie na samochód osobowy. No jesteśmy w kolejnym kontekście. Kolejne strony internetowe mamy. No i z samochodu osobowego możemy przeskoczyć na samochód używany osobowy i tak dalej, i tak dalej, i tak dalej, i tak dalej na całkiem sporej głębokości. Tam akurat ćwiczenie samochody używane z salonu i samochody używane z salonu, Volkswagen używane i nam się buduje już big picture. Na tym czat GPT, BERT, image generation. Jakby proces był taki sam, tylko że tutaj akurat szło po autosuggest. Z zasady możecie sobie, to jest porada, jeżeli chcecie rozszerzyć tą analizę, zrobić po autosuggest i tam pewnie po kilku innych rzeczach, które wymyślicie i my robimy tylko po na potrzeby edukacyjne po tym snippecie rozszerzanie. Dobra, mamy na to przygotowanego collaba Deep Search Analysis. Przeszukiwanie głębokie. Co tu potrzebujemy zrobić? Głębokość. My tych fraz kluczowych, które wygenerowaliśmy w kontekście query expansion jest dokładnie tych fraz czterdzieści cztery, więc jest całkiem sporo. Tutaj mamy raz, dwa, trzy, cztery, pięć, sześć. Ok, czyli mamy po cztery, czyli mamy czterdzieści cztery razy cztery. Mateusz policz, ile to jest? Czterdzieści cztery razy cztery, Mateusz, a ja będę mówić dalej. Sto siedemdziesiąt sześć możliwości będziemy mieli tak na dzień dobry, przy głębokości jeden. Jeżeli będziemy zwiększać tą głębokość to odpowiednio będzie mnożone, mnożone, mnożone, mnożone, mnożone, mnożone, mnożone, mnożone i zwiększona ilość. Więc jeżeli chcecie zrobić bardzo głęboką analizę, no to możecie sobie suwaczkiem przesunąć bardzo głęboko. My na potrzeby treningowe robimy z głębokością jeden, bo po prostu każdy proces trwałby za długo. Korzystamy z rozwiązania Damiana Sałkowskiego Serpdata w tym ćwiczeniu więc jakieś tutaj zrobimy sobie opóźnienie w requestach, żeby nie zrobić problemu. Jeżeli chcecie pracować za granicą to tutaj w polu HL wpisujecie język, w którym operujecie GL czyli geolocalization to też wpisujecie. No my jesteśmy w Polsce, więc jakby spoko. Tu możecie sobie zrobić liczbę wątków, my sobie damy na powiedzmy na pięć wątków nasz proces, żeby, żeby dynamicznie to się działo. No i jakby dodatkowe konfiguracje. Teraz ten skrypt zacznie chodzić po SerpData, czyli ściągać dane z Google i przeklikiwać je głębiej, głębiej, głębiej, głębiej, głębiej, głębiej. Wyciągając nam wszystkie adresy URL znajdujące się w top dziesięć dla danej frazy i rozszerzając dalej, głębiej, głębiej. Odpalamy proces i wracamy do podsumowania, jak proces się wykona. No dobra, mamy to. Przyjrzymy się co tutaj się wydarzyło. Ja Wam omówię jak działa skrypt. Dla łatwości skopiujemy to do notatnika. Ok, a więc skrypt jest stworzony w ten sposób. Jak widzicie jest, jest wielowątkowy. Zainicjowaliśmy go czterdzieści trzy słowa kluczowe, czyli z tego query expansion i po kolei sobie wchodziliśmy po Google'u. Co jest ciekawe, zobaczcie dodane nowe zapytania do słów kluczowych. Czyli cały czas rozszerzaliśmy, rozszerzaliśmy tą bazę. Pominięto zapytanie. Tak stworzyłem ten skrypt, że jeżeli już coś zostało skraulowane albo znajduje się w kolejce, to nie dodajemy tego jeszcze raz, co jest możliwe, bo kręcimy się dookoła jednego komina, więc jakby, żeby to było optymalne, ale ciągle to jest dosyć długo. No i trochę to trwało. Trochę to trwało. Natomiast mamy następujące wyniki. To są, to są listy słów kluczowych, które zostały przekraulowane, czyli zobaczcie z tych stu czterdziestu, z tych czterdziestu czterech seedowych słów zrobiliśmy sto dziewięćdziesiąt dwa słowa, więc jakby całkiem, całkiem fajnie nam to, nam to poszło. Miało być sto siedemdziesiąt, sto siedemdziesiąt sześć zdaje się.Więc na niektórych słowach było tego, tego, tego znacznie więcej. No i okej, jakbyśmy chcieli rozszerzać to dalej, no to głębokość. Głębokość, no nie? No i dobra, zobaczcie. I na tych stu dziewięćdziesięciu sześciu słowach dookoła kontekstu samochodów, tutaj mamy już jakby tablice, akcesoria do samochodu i mamy, widzicie strony internetowe, które tam się znajdują. Statystyka mówi tyle, że nam tutaj się zrobiło URL dla-- czyli dla stu osiemdziesięciu trzech zapytań było więcej niż dwa URL-e, bo usuwałem po raz kolejny duplikaty. No bo wiecie, że jedna strona może pozycjonować się na duży zestaw słów i jakby na tym kierunku idziemy, czyli topical authority. Więc jakby dla osiemdziesięciu sześciu fraz były URL, ja zacząłem też usuwać jakby grupy, które mają mniej niż dwa adresy, dlatego że to były powiedzmy takie outlinery. To do niczego nie pasowało, to był szum, to był na przykład w jakimś kontekście pojawiała się strona rządowa albo zostawała w jednej grupie jedna strona, która tam jest przypadkowo. Więc taki szum po prostu Wam odciąłem w tym skrypcie. Wyniki zapisane i ok. Zobaczmy, ile mamy słów kluczowych, ile mamy w takiej finalnej analizie stron. Mamy stron pięćset osiemdziesiąt sześć stron internetowych Szanowni Państwo, które będą nam właśnie budować korpus wiedzowy. Sporo, nie? Jeżeli chcecie zbudować topical mapę z Deep Researchu czy z zestawu słów kluczowych, życzę Wam powodzenia. W tym przypadku pod analizę, która ma tylko głębokość jeden, wykonamy analizę pięciuset osiemdziesięciu sześciu stron internetowych, budując z nich knowledge graph. To będą grafy dwustronne. Zaraz Wam to wytłumaczę. Jeśli można mówić o dwustronnych grafach, zaraz wytłumaczę, o co chodzi. Także chodźmy do następnej lekcji. Będziemy analizować pięćset osiemdziesiąt sześć stron internetowych i budować wiedzę w temacie samochodów. Dzięki. Napisy przez: Matija Kuzma 