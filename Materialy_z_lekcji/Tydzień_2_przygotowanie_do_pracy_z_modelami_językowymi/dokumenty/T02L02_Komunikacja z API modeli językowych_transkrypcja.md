Cześć, w tej lekcji zajmiemy się komunikacją z modelami językowymi, ale przez API. Generalnie w większości naszych automatyzacji, czy w kolejnych tygodniach, lekcjach tego kursu, będziesz raczej komunikować się z modelami językowymi przez API w różnej formie, czy to przez kod Pythona, czy to przez narzędzia do automatyzacji, takie jak MAKE czy N8N. Natomiast warto zrozumieć koncepty, które rządzą API i jak to działa, żeby po prostu łatwiej się nam z tym pracowało. Przygotowałem taki specjalny, interaktywny Google Collab, w którym krok po kroku przeprowadzam się przez meandry korzystania z API różnych modeli językowych. Zachęcam do tego, żeby przejść ten Collab po obejrzeniu lekcji samodzielnie, bo on naprawdę w bardzo prosty sposób wyjaśnia jak działają te wszystkie zagadnienia. Natomiast zanim przejdziemy do Colaba, chciałem powiedzieć o tym, że każdy dostawca modeli językowych posiada tak zwany Playground, czyli taką funkcję, która pozwala skorzystać z API, nie wysyłając żadnych żądań HTTP, czyli nie generując żadnego kodu, nie pisząc żadnego kodu. I takie rozwiązanie posiada chociażby OpenAI, to się nazywa OpenAI Playground i zazwyczaj w takim serwisie generujemy swoje klucze API, czyli takie powiedzmy hasło, dzięki któremu się skomunikujemy z tym API. Więc tutaj generalnie możemy sobie wyklikać wszystko i on nam wygeneruje kod, którym powinniśmy wywołać to żądanie. Przykład, tutaj mamy konsolę właśnie od OpenAI, która nazywa się Playground i tu możemy wybrać wszystkie modele, które są dostępne przez API, wybrać narzędzia, zaraz też będziemy to omawiać, skonfigurować system message, na przykład jesteś pisarzem wierszyków, skonfigurować wszystkie parametry zapytania i wysłać ten prompt. To co tutaj jeszcze możemy zrobić to właśnie podświetlić kod, wybrać język programowania, w którym programujemy i na przykład OpenAI pokaże nam tu konkretny kod, który powinniśmy wysłać, żeby właśnie odtworzyć ten prompt, który tutaj napisaliśmy. Więc takim powiedzmy przedsionkiem do API i skryptów Python czy innych takich skryptów, które komunikują się z API są właśnie tego typu interfejsy. W każdym takim interfejsie właśnie można wygenerować też swój klucz API, akurat w OpenAI jest to w zakładce dashboard i API keys. Oczywiście to może się zmienić, bo OpenAI i inne modele językowe się rozwijają, więc klucze API możesz szukać w innym miejscu, natomiast w Google Colabie podlinkowałem te wszystkie miejsca, gdzie te klucze należy szukać. Takie samo rozwiązanie ma Google i tutaj akurat się to nazywa AI Studio. Tutaj możemy wygenerować klucze API, przy czym tutaj akurat potrzebujemy najpierw założyć projekt w Google Cloudzie, natomiast możemy tutaj wygenerować klucz, możemy też przejść do studia i tutaj właśnie też wszystko wyklikać i tak jak w przypadku OpenAI dostać gotowy kod, który powinniśmy wysłać, żeby to rządanie API przetworzyć. to też możemy otworzyć w Google Colabie. Więc każdy dostawca modeli zazwyczaj zapewnia taką infrastrukturę. To nie jest to samo co chat, to nie jest to samo co aplikacja Gemini. To jest powiedzmy taki interfejs, gdzie programiści mogą skonfigurować zapytanie zanim po prostu napiszą kod, tu je przetestować, zobaczyć czy działa i dopiero wtedy wdrożyć na produkcję. Przejdę teraz do właśnie Google Colaba, który przygotowałem na potrzeby wyjaśnienia tej lekcji. Okej, czyli jesteśmy w Google Colabie, który jest takim interaktywnym samouczkiem dla Ciebie, do nauczania się komunikacji z modelami językowymi. Pamiętaj, że z tymi API modeli językowych można się komunikować właśnie za pomocą takich skryptów, za pomocą różnych języków programowania, no ale też za pomocą narzędzi no-code, takich jak Make czy NSMN i tego będziesz się uczyć w kolejnych lekcjach i kolejnych tygodniach. Natomiast taki sposób komunikacji niekiedy jest bardziej wygodny, czasem nawet bardziej przystępny. Możesz sam samodzielnie dokładnie sterować tym zachowaniem modelu. Te narzędzia no-code nie zawsze udostępniają, powiedzmy taką dowolność, natomiast to też bardzo pomoże zrozumieć wszystkie te koncepty, które rządzą interfejsami API. Więc w pierwszym kroku musimy zainstalować potrzebne biblioteki, które będziemy używali w ramach tego notebooka, ponieważ będziemy korzystali z API takich rozwiązań jak OpenAI, Google Generative AI, Antrofit, Request i tak dalej, więc instalujemy te biblioteki. Tak jak już wiesz z lekcji o Google Collabie, to za każdym razem, kiedy uruchomimy ten notatnik, musimy te biblioteki zainstalować ponownie. To nie trwa długo, więc nie jest to też jakoś szczególnie uporczywe. Niech się to instaluje, ja przejdę do kolejnej komórki. Teraz musimy skonfigurować nasze klucze API. Tutaj znajdziesz wszystkie linki, które prowadzą do miejsc, gdzie ten klucz API należy wygenerować. Więc będziemy korzystać w naszym notatniku z OpenAI, Gemini, Clode, Gina i OpenRouter. Więc teraz jeżeli chcesz, możesz przerwać ten filmik, przejść do tych rozwiązań i generować te klucze, ale możesz też obejrzeć po prostu całą lekcję i zrobić to samodzielnie później. Te klucze zapisujemy w ramach Secret Key i tutaj jest nazwa każdego z tych kluczy, więc ona musi się zgadzać z tym, a wartość to jest to, co skopiujemy z tych rozwiązań. I to jest bardzo ważne, bo to tylko raz konfigurujemy i możemy używać we wszystkich notatnikach. więc jeżeli raz to skonfigurujesz, to później już na Twoim koncie to istnieje i jeżeli podasz tą nazwę zmiennej, w zmiennej środowiskowej, to będzie zawsze można z tego korzystać, więc to jest bardzo wygodne. Nie musimy konfigurować każdym notowniku tych kluczy oddzielnie, tylko przechowujemy właśnie w jednym miejscu, więc uruchamiam tę komórkę z kluczami, mam już te klucze skonfigurowane, no i teraz przechodzimy do już konkretów. Przede wszystkim, co to jest w ogóle API? No API to jest taki interfejs programistyczny, to jest powiedzmy taki sposób wymiany wiedzy pomiędzy tobą a serwerem aplikacji, z której korzystasz. Więc to jest nic innego niż taki standard komunikacji w ramach powiedzmy internetu. I teraz większość aplikacji tak naprawdę posiada API, bo większość aplikacji, pewnie słyszałeś to słowo, jest zbudowana jako aplikacje headlessowe, to znaczy, że backend nie jest połączony z frontendem i backend z frontendem komunikuje się przez API, więc my do tego API też możemy się po prostu podłączyć za pomocą właśnie takiego klucza, który udostępnia jakaś usługa i pobierać te dane, które w tej usłudze moglibyśmy normalnie wyklikać, prawda? I teraz jak działa HTTP API? Przygotowałem tutaj też specjalną grafikę i opis, który pozwoli to zrozumieć, więc tak naprawdę działa to w taki sposób, że my wysyłamy, my tworzymy żądanie, tak zwanym HTTP protokole, klient wysyła to żądanie, serwer odbiera żądanie, serwer też sprawdza czy to żądanie jest prawidłowe, to znaczy czy zawiera klucz API, czy inną metodę powiedzmy autentykacji. Serwer tworzy odpowiedź, serwer wysyła odpowiedź, no i klient odbiera odpowiedź. Czyli w sześciu krokach przechodzimy od tego, że wysyłamy request do API do momentu, kiedy odbieramy po prostu dane z tego API. Podstawowe koncepcje, które należy zrozumieć i definicje. Więc pierwsza definicja, którą należy zrozumieć to jest endpoint. To jest taki specyficzny adres URL, do którego wysyłasz żądanie. i zazwyczaj w dokumentacji właśnie te endpointy są opisane, więc w przypadku chociażby na przykład OpenAI to będzie api.openai.com.v1.chart.completition. Oczywiście tych endpointów w OpenAI jest więcej, natomiast to jest ten endpoint, do którego wysyłamy żądanie, jeżeli chcemy, żeby OpenAI nam odpowiedział na jakiś konkretny prompt. Druga istotna, powiedzmy, definicja w ramach API to są metody HTTP i mamy dwie takie główne metody HTTP. Pierwsza metoda to jest POST i najczęściej używana metoda i służy do wysyłania danych do serwera w celu ich przetworzenia lub utworzenia nowego zasobu. To znaczy, że my jakieś dane posiadamy i wysyłamy je do serwera, serwer je przetwarza w jakiś sposób i odsyła nam w jakiejś uzupełnionej formie. Na przykład załóżmy, mamy jakieś słowa kluczowe, wysyłamy je do API, na przykład Senuto, no i Senuto przetwarza te słowa kluczowe i wysyła nam jakieś informacje. Druga metoda GET służy do pobierania danych z serwera. Może być na przykład używana do pobrania listy dostępnych modeli, czyli my nie wysyłamy żadnych informacji, tylko żądanie do serwera o to, żeby nam zwrócił jakieś po prostu informacje. Tu są pozostałe parametry, one już dotyczą głównie modeli językowych, więc nie będę tego odczytywał, natomiast to co jest jeszcze bardzo ważne to są statusy HTTP serwera, to znaczy status HTTP zwraca nam najczęściej informacje o rodzaju błędu lub o sukcesie danego żądania. Na pewno jest to Ci znane, ponieważ w branży SEO stosujemy te oznaczenia często. Jeżeli serwer odpowiada nam statusem HTTP 200, to znaczy, że to żądanie zostało poprawnie wykonane i pobraliśmy dane. Jeżeli na przykład wysyła nam 404, to znaczy, że jakaś informacja na tym serwerze nie została znaleziona, czyli na przykład możemy wysyłać żądanie do błędnego adresu endpoint. To co bardzo ważne jeszcze w kontekście komunikacji z API to jest format danych jakim jest JSON, czyli JavaScript Object Notation. Większość generalnie aplikacji i większość API komunikuje się właśnie w tym standardzie, więc zrozumienie tego standardu jest kluczowe do tego, żebyśmy mogli się poprawnie komunikować z API modeli językowych. Tu jest objaśnione w jaki sposób to działa, więc zachęcam tutaj też do przeczytania. I tutaj jest pierwsza komórka, która właśnie objaśnia w jaki sposób my możemy używać JSON-a. Więc to co tutaj widzisz, my wysłaliśmy jakieś żądanie w języku Python do API OpenAI, a API OpenAI odpowiedziało nam właśnie w formie JSON-a, którego musimy w jakiś sposób przetworzyć. I to jest właśnie taki standard komunikacji. Wysyłamy w JSON jakieś zapytanie, dostajemy w JSON odpowiedź i najczęściej właśnie naszym zadaniem jest to, żeby odpowiednio te dane z JSONa wyciągnąć. W narzędziach no-code, takich jak Make czy N8N jest to też kluczowe, ponieważ musimy zazwyczaj odpowiednio zdefiniować z jakiego pola, z jakiego obiektu chcemy wyciągnąć dane i w jaki sposób je przetworzyć, więc zrozumienie tego zagadnienia jest kluczowe dla dalszej pracy z modelami językowymi. I tutaj mamy też opisane w jaki sposób to najczęściej w modelach językowych wygląda. Przejdziemy zatem do praktyki i to co jest jeszcze ważne w komunikacji z modelami językowymi to jest to, że możemy albo korzystać z API bezpośrednio wysyłając request HTTP albo z dostępnych bibliotek SDK, które udostępniają dostawcy modeli językowych. No i jakie są przewagi pomiędzy jednym a drugim rozwiązaniem? Żądanie HTTP to najczęściej będziemy musieli wszystko obsłużyć sami, to znaczy wysylemy żądanie HTTP, natomiast jeżeli dostaniemy jakiś konkretny błąd, to musimy też sobie sami go opisać. Natomiast w przypadku biblioteki SDK ona już zazwyczaj obsługuje wszystkie błędy, wszystkie metody i tak dalej, więc najczęściej po prostu musimy mniej kodu napisać i jest to znacznie prostsze, więc zazwyczaj w większości przypadków zalecam, aby korzystać z gotowych bibliotek, dlatego też wyżej w komórce pierwszej instalowaliśmy te biblioteki OpenAI, udostępnia swoją bibliotekę. Niektóre nawet modele językowe dostosowują się do biblioteki OpenAI, na przykład modele Google Gemini też można wyłać przez bibliotekę OpenAI, więc jest to dość użyteczne. I jeżeli byśmy się chcieli właśnie bezpośrednio skomunikować, to tutaj jest skrypt, który właśnie do tego służy. Używamy tutaj biblioteki Request i właśnie wysyłamy żądanie do tego endpointu OpenAI razem z tymi promptami. No i tak jak tutaj widać, wysyłamy żądanie post, no i tutaj jest odpowiedź API. Użyliśmy 123 tokenu. Ok, teraz wyślemy właśnie żądanie do OpenAI, wykorzystając OpenAI SDK, czyli tą bibliotekę, którą udostępnia nam OpenAI. I tak samo tutaj właśnie importujemy bibliotekę OpenAI. Tutaj mamy, jak widać, już też moduły do obsługi limitów, moduły do obsługi zapytań, autentykacji i tak dalej, więc to jest bardzo uproszczone. i też dostajemy właśnie odpowiedź z OpenAI w taki sposób. Tutaj mamy komórkę, która komunikuje się z Google Gemini, tak jak w przypadku OpenAI. Gemini też posiada taką bibliotekę i możemy się z nią skomunikować, natomiast w przypadku bibliotek Google'a czasem sytuacja jest nieco bardziej skomplikowana, ponieważ Google zarówno ma usługę Vertex AI, jak Google Generative AI i tam czasem są pewne rozbieżności, więc tutaj warto na to też zwrócić uwagę. Mamy tą odpowiedź. Tutaj jest komórka, w której się komunikujemy z API Antrofic do modelu Clouder i też dostajemy odpowiedź w takiej formie. Kolejnym takim ciekawym rozwiązaniem jest Open Router. To jest taka usługa, która udostępnia możliwość korzystania z ponad 300 modeli językowych w jednym miejscu, w jednym standardzie API, więc jeżeli piszemy jakieś aplikacje albo piszemy jakieś automatyzacje i na przykład skorzystamy w jednym miejscu z OpenAI i na przykład za tydzień Google wypuści jakiś model językowy, który jest nieco lepszy niż te do tej pory OpenAI, to żeby to zmienić musielibyśmy przepisać te nasze skrypty. Jeżeli wykorzystamy OpenRouter, to wystarczy, że zmienimy zazwyczaj tylko nazwę modelu, tak jak tutaj widać. Wysyłam tylko nazwę modelu, z której chcę skorzystać, a to żądanie do tego modelu zazwyczaj będzie w takiej samej formie. Przejdziemy sobie teraz na stronę OpenRouter, żeby Ci to zaprezentować. Czyli możemy sobie tutaj wyszukać jakiś konkretny model. Na przykład teraz wyszedł taki model jak QAN w wersji takiej, która przewyższa powiedzmy modele na przykład OpenAI. Więc potencjalnie mogę sobie z niego skorzystać. Tutaj OpenRouter nam daje tutaj ścieżkę do tego modelu, więc jakbym chciał z niego skorzystać, to po prostu przeklejam tą nazwę modelu tutaj. I wcześniej korzystałem z DeepSeek, teraz mogę skorzystać z Quena i on będzie za każdym razem działał w taki sam sposób, więc nie musimy się martwić na przykład o nazwy zmiennych, o to czy w przypadku OpenAI coś się nazywa system prompt, a w przypadku Antropica system message i nie musimy tych po prostu kluczy definiować. Open Router właśnie daje dostęp do tych modeli i to API przez Open Router jest po prostu zazwyczaj w taki sam sposób skonstruowane, czyli wystarczy, że wyślemy raw content i raw system i zawsze będzie działać w taki sam sposób. Oczywiście są pewne specyfiki, jak na przykład modele multimodalne i sposoby usyłania chociażby obrazków do nich, natomiast Open Router jest takim chyba najbardziej dostępnym i najbardziej przystępnym rozwiązaniem, jeżeli chodzi o komunikację z modelami językowymi przez API. ma modele wszystkich najważniejszych dostawców, czy to Gemini, czy to GPT, więc możemy po prostu się z nimi komunikować. Te ceny tych modeli są takie same jak w przypadku dostawców, czyli ceny Gemini będą dokładnie takie same, jakbyśmy bezpośrednio się komunikowali z Gemini. Okej, więc przechodząc sobie dalej, skomunikujemy się teraz z Open Router. Też dostaliśmy, przygotowuję żądanie, no i wysyłamy to żądanie do modelu DIPSEE, dokładnie korzystamy z damowego modelu i mamy sparsowaną odpowiedź też w tej samej formie. I teraz możemy sobie porównać jakie odpowiedzi każdy z modeli wygenerował. Możesz zobaczyć w tej komórce jak te modele generują te odpowiedzi i jak one wyglądają. Każdy model dostał ten sam prompt dokładnie, więc tu można porównać w jaki sposób te różnice wyglądają. I tutaj zrobiłem komórkę, która podsumowuje wszystkie różnice w komunikacji, ponieważ są pewne różnice, które należy zwrócić uwagę. Na przykład jeżeli chodzi o przekazywanie promptu, to w przypadku OpenAI przekazujemy to jako element listy w takim znaczniku message, a w przypadku na przykład Google Gemini w systemie instruction, w przypadku Antrofit trochę jeszcze inaczej to wygląda i w przypadku OpenRootera też trochę inaczej, więc tu są wszystkie te różnice, które wynikają ze specyfiki danego modelu. Dobra, co jest bardzo ważne, to w przypadku API w odróżnieniu od takich rozwiązań chatowych jak ChatGPT korzystamy z trochę innej waluty, to znaczy za ChatGPT płacisz 20 dolarów miesięcznie lub 200, jeżeli masz tą wersję najwyższą i masz jakiś określony limit tego, ile możesz na przykład tam wiadomości wysłać. W przypadku komunikacji z API sytuacja ma się inaczej, bo taką walutą, za którą się rozliczamy, to jest właśnie liczba tokenów, czyli model językowy bierze nasz tekst, ten prompt, który wysyłamy, tokenizuje go, oblicza ile tokenów użyjemy i za tyle właśnie płacimy. Zazwyczaj modele językowe posługują się walutą wyrażoną w tysiącu tokenów, czyli zazwyczaj na cenniku modeli językowych będziemy mieli informację ile płacimy za tysiąc tokenów. Inna cena jest zazwyczaj za tokeny na wejściu, inna cena za tokeny na wyjściu i tokeny na wejściu są zazwyczaj tańsze od tokenów na wejściu. Zazwyczaj ta różnica w cenie może być dwu do pięciokrotności. i my tutaj sobie teraz porównamy, jak liczba tokenów wpłynęłaby na cenę w przypadku różnych modeli językowych. Ja tutaj wykorzystuję bibliotekę TikToken i właśnie tokenizer CL100K Base. Każdy model językowy i każdy dostawca może stosować trochę inny tokenizer, więc te dane nie muszą być w 100% takie same jak na przykład w Google Gemini. Natomiast powiedzmy, że z dużym prawdopodobieństwem ta liczba tokenów, jeżeli wyślisz ją bezpośrednio przez API, nie powinna się jakoś szczególnie różnić, natomiast nie mamy dostępu do tych tokenizerów wszystkich tych rozwiązań. Więc tutaj wysyłamy jakiś system prompt i jakiś user prompt i policzymy sobie ile tych tokenów jest. Więc najpierw zainstaluję skrypt, tą bibliotekę, Tiktoken i tak jak widać jesteś pomocnym asystentem AI, odpowiadaj z więźle, to oszacowana liczba tokenów wynosi 21. Natomiast prompt użytkownika wyjaśni, czym jest tokenizacja w kontekście LLM, Jest tych tokenów 19, więc zużyliśmy 40 tokenów. I teraz przygotowałem tutaj taki specjalny kalkulator kosztów użycia API, ponieważ jakby 40 tokenów nie brzmi jak dużo, natomiast jeżeli skonstruujesz jakiś system, który będzie wysyłał bardzo dużo tokenów, to ten koszt staje się dość istotny. Chociażby podam tu przykład, że w Senuto wydajemy na API OpenAI chyba około 20 tysięcy złotych miesięcznie. Więc dla nas jest już bardzo kluczowe, jaki model wybierzemy. Pewnie dla Ciebie w pewnych skrajnych przypadkach będzie to też bardzo istotne. pobrałem aktualne ceny wszystkich modeli z tej strony llmprice.com i zdefiniowałem tutaj koszt na wejściu i wyjściu dla każdego z modeli językowych więc możemy sobie to tutaj zdefiniować i teraz mamy tutaj kalkulator kosztów dla wcześniej zdefiniowanych promptów czyli dla tych promptów które mają tylko 40 tokenów zazwyczaj nasze prompty będą dłuższe oczywiście więc porównamy sobie ile to by kosztowało w przypadku modeli Google Gemini i Antrofic i OpenAI. I teraz jeszcze tutaj musimy zdefiniować, ile oczekujemy, że będzie tokenów na wyjściu, no bo nie wiemy, jaki prompt wysyłamy, nie wiemy, jak model odpowie, więc zakładam, że będzie to na przykład 1200 tokenów. I ten tutaj skrypt prosi nas jeszcze o zdefiniowanie, ile będziemy promptów wysyłać takich właśnie, o takiej treści. Więc załóżmy na przykład, że wyślemy takich promptów miesięcznie 5000 i to nam pomoże obliczyć, jaki byśmy koszt miesięcznie ponieśli właśnie za korzystanie tutaj z tych modeli językowych. No nie musimy konkretnie 5 tysięcy. Możesz też wybrać konkretny model, żeby sprawdzić, jakby to wyglądało kosztowo. Więc wywołajmy tą komórkę i sprawdźmy, ile wynosłyby nasze koszty. Więc średnio musielibyśmy zapłacić, średni koszt za te 5 tysięcy tokenów, tych krótkich oczy, 5 tysięcy zapytań z tymi krótkimi tokenami to by wyniosło 6 dolarów. Najtańszy byłby Google Gemini 1,5 Flash. On jest darmowy oczywiście, natomiast ma swoje ograniczenia. Najdroższy byłby Gemini 2,5 Pro, bo ten koszt wynosiłby 60 dolarów. I tu jest koszt podany szacunkowy dla wszystkich tokenów. Jeżeli masz jakieś zadanie, na przykład pisanie artykułów, generowanie nagłówków, to możesz wprowadzić w tych komórkach wyżej swoje prompty, określić ile takich generacji będziesz przerabiać miesięcznie i ten kalkulator policzy Ci ile mniej więcej to by kosztowało i do tego możesz dopasować swój wybór, jeżeli model, który wybieramy dla konkretnego zadania. Okej, bardzo ważne w kontekście komunikacji z API modeli językowych jest zrozumienie tego, że one posiadają jakieś limity, do których się musimy stosować. I pierwszym takim limitem, do którego musimy się stosować jest liczba tokenów. Zazwyczaj właśnie każdy model ma zdefiniowaną liczbę tokenów na wejściu i wyjściu. Niektóre modele można dostarczyć im więcej kontekstu, niektóre modele mogą generować więcej tokenów na wyjściu, ale zobaczmy na przykład jak to wygląda w przypadku modeli OpenAI. Mamy tutaj model OpenAI O4 Mini i tak jak widać on może przyjąć 200 tysięcy tokenów na wejściu. Ile to jest 200 tysięcy tokenów? No to zależy od tego, co tam wyślemy, prawda? Bo na przykład jest taka sytuacja, że jakiś znaczek emoji może zajmować 60 tokenów nawet. Ale zazwyczaj można przyjąć dla uproszczenia wyliczeń, że jeden token to średnio dwa, trzy znaki. W języku polskim będzie to trochę więcej. W języku angielskim będzie to trochę mniej. I on jest w stanie dać nam 100 tysięcy tokenów na wyjściu. inny model jakiś, na przykład GPT-4O, on ma 128 tysięcy tokenów na wejściu, 16 tysięcy tokenów na wyjściu. I to jest bardzo ważne, żeby to zrozumieć, bo w przypadku, kiedy rozmawiasz z czatem GPT, to on ci po prostu powie, że przekroczyłeś limit tokenów, natomiast w przypadku API, jeżeli mamy jakąś automatyzację i nie do końca kontrolujemy na przykład to, ile tych tokenów będzie na wyjściu, bo na przykład nie wiemy, czy że nie wiem, jeżeli na przykład mamy jakiś system, który generuje nagłówki i wysyłamy do wygenerowania nagłówków na przykład treści konkurentów, to nie do końca wiemy jak one długie będą, więc musimy to po prostu projektując naszą automatyzację, czy projektując nasz skrypt, który będzie wysyłał żądanie do API o wygenerowanie nagłówków, musimy już po prostu to przewidzieć i zaprojektować to tak, żeby nie przekraczać tego limitu, bo po prostu będziemy mieli różnego rodzaju błędy. I demonstracja jak działają limity wejścia do GPT-4.0. Więc ten skrypt, który tutaj wywołaliśmy, on symuluje bardzo długie zapytania. I teraz widzimy w pierwszej iteracji on wysłał prompt, który ma 27 tokenów, więc tutaj bez problemu model GPT-4O sobie z tym poradził. Natomiast druga symulacja polega na tym, że on powtórzył to samo zdanie 5000 razy i wyszło tutaj 235 tysięcy tokenów. Jak pokazywałem wcześniej, ten model GPT-4O ma 128 tysięcy tokenów na wyjściu. Jak widać model odpowiada nam właśnie bed requestem. 400, bo przekroczyliśmy context length. Tak samo właśnie limit tokenów na wyjściu. Jeżeli wyślemy 15 tokenów, no to on bez problemu odpowiada. Jeżeli wyślemy tych tokenów więcej, no to mamy max token, limit wyjściowy przekroczony i nie jesteśmy w stanie nic z tym zrobić. Po prostu model w ogóle nie odpowie na to pytanie. Okej. I to są takie podstawy komunikacji z API modeli językowych. Oczywiście nie musisz wiedzieć, jak te wszystkie skrypty pisać, będziemy później potem je też generowali, będzie można też to wszystko potem skonfigurować za pomocą narzędzi nauków, natomiast po prostu jest to w celach takich, żeby lepiej to zrozumieć. Okej, kolejne, już przechodzę tutaj do zaawansowanych funkcji API modeli językowych, ponieważ nowe modele językowe udostępniają różne dodatkowe funkcje, które pozwalają nam rozszerzyć ich możliwości. Jedną z takich funkcji jest właśnie function calling. I to jest taka funkcja, która dostępna jest zarówno w modelach Gemini, jak i w modelach OpenAI. I wyobraź sobie tak, że czasem, żeby wygenerować jakąś odpowiedź promptu, czasem, żeby model wygenerował jakąś odpowiedź, musi skorzystać z jakiegoś narzędzia, żeby dostarczyć sobie kontekstu. I tutaj podaję przykład. Jako, że modele mają odcięcie wiedzy, na przykład wiedzą tylko, co się działo do października 2023, to na przykład nie wiedzą, jaka jest dzisiaj pogoda, prawda? Więc musimy na przykład skonfigurować im takie narzędzie, które odpowiada im pogodą. I co tutaj mamy w tej komórce? Przykład. Chcę zapytać, która jest teraz godzina w Nowym Jorku? Jeżeli wejdę sobie do OpenAI i zadam to pytanie, myślemy tak o prompt no to on pogada jakimś takim ogólnym powiedzmy tekstem, który mówi jaka jest strefa czasowa w Nowym Jorku my chcemy wiedzieć jaka jest aktualnie godzina w Nowym Jorku. Do tego służy właśnie function calling i tak jak widzisz mamy tutaj zdefiniowaną funkcję z narzędziami OpenAI i taka funkcja tutaj w tym skrypcie napisałem skrypt, który sprawdza po prostu godzinę w Nowym Jorku biorąc pod uwagę jaka jest strefa czasowa i jest tutaj zdefiniowana funkcja którą wysyłamy właśnie do promptu do OpenAI. Tutaj podam Ci przykład jak to działa wykorzystując playground tak jak tutaj widzisz mamy coś takiego jak narzędzia i możemy stworzyć jakieś narzędzie. Jednym z tych narzędzi będzie jest właśnie funkcja. I tutaj możemy właśnie zdefiniować funkcję, która służy do wywołania pogody, do sprawdzania czasu. Na przykład mogę tutaj zdefiniować jakąś funkcję, która checking time in different cities. I on tak jakby wygeneruje jakąś funkcję i ona zostanie opisana i ten opis jest bardzo ważny. I kiedy napiszemy prompt do modelu językowego, to przy wysyłanym żądaniu on będzie wiedział, że ma dostępne narzędzia. Tutaj jak wyklikniemy ten kod, to widzimy, że jest wysyłane w zapytaniu API coś takiego jak tools i tutaj one są opisane. Więc jeżeli wyślemy prompt, który model skojarzy z tą funkcją, z tym narzędziem, to wywoła to narzędzie i to narzędzie my musimy skonstruować. To znaczy, że my musimy sprawdzić tą godzinę i przekazać ją do modelu, on potem ją uwzględni w swojej odpowiedzi. Podam Ci inny przykład. Załóżmy, że mam API Senuto i chcę skonstruować tylko chatbota, który się z nim komunikuje. I w ramach toolsu, w ramach function calling konfiguruję wszystkie endpointy API. Na przykład to jest endpoint do pobierania usług kluczowych, więc go opisuję, że ten endpoint pobiera słowa kluczowe. I kiedy użytkownik zrobi taki prompt, na przykład pobierz słowa kluczowe, to model będzie mógł skojarzyć, że ma dostępne takie narzędzia, dostał tą informację w API i za pomocą tej funkcji wywoła funkcję w API naszym senuto, które pobierze te słowa kluczowe. API senuto zwróci mu te słowa kluczowe, a on następnie doda sobie to jako kontekst do promptu i w taki sposób odpowie i w ten sposób to działa. I tutaj zapytamy go o to, która jest godzina w Nowym Jorku i wyślamy taki prompt do właśnie OpenAI i teraz jest gotowy, zainstalowaliśmy i teraz tak jak widzimy mamy odpowiedź. Aktualnie w Namiorku jest godzina 8.32 dnia 2 maja 2025. I tak jak widać właśnie on skorzystał z tego narzędzia i dzięki temu jesteśmy w stanie rozwinąć trochę funkcję modulu językowego. To jest trochę powiedzmy zaczątek agentów AI, natomiast trochę bardziej uproszczona funkcja. Więc jeżeli na przykład załóżmy do czego możemy korzystać tą jeszcze funkcję. Możemy na przykład skonfigurować jakiegoś chatbota firmowego, który może na przykład pobrać dane ze Slacka na przykład. Czyli nie wiem, mamy jakieś API Slacka i mamy jakąś funkcję, która pobiera na przykład najważniejsze wątki ze Slacka z ostatniego tygodnia, więc jeżeli użytkownik w czacie zadał pytanie na przykład, co działo się na Slacku w ostatnim tygodniu, to model by wiedział, że może skorzystać z takiej funkcji, wywołałby tą funkcję, nasz skrypt, który tą funkcję przetwarza, dałby modelowi kontekst do zapytania i w ten sposób model by wiedział w jaki sposób odpowiedzieć. Inna możliwość modeli językowych to jest ustrukturyzowana odpowiedź. To jest na przykład bardzo ważne z punktu widzenia agentów AI, bo jeżeli jeden agent coś przetworzy, to musi do innego agenta przetwarzać w jakiejś konkretnej formie. My też jak piszemy jakieś skrypty, to model językowy jest niedeterministyczny, więc może za każdym razem w trochę innej formie odpowiedzieć. Będziemy się uczyć kontrolować tą odpowiedź, natomiast możemy też skonfigurować taką funkcję, która nazywa się na przykład JSON Mode. Ona akurat w JSON Mode się nazywa tak, bo OpenAI w innych modelach językowych może się nazywać trochę inaczej. I tutaj możemy zmusić model, żeby odpowiadał nam w formie JSON-a jakiegoś. Możemy go zmusić albo żeby odpowiadał w formie JSON-object i wtedy on sobie sam zdefiniuje, jak ten JSON-object względem odpowiedzi musi wyglądać, albo możemy zdefiniować mu JSON-schema, czyli w jakiej formacie JSON-a ma nam odpowiadać. I też w ramach OpenAI możemy po prostu wydać tekst format i taki standardowy tekst, kiedy nic nie zdefiniujemy, to jest właśnie model odpowiada w formie tekstu. ale mamy dwie inne możliwości. JSON Object i tutaj niczego nie definiujemy, ale model już wie, że chcemy dane w JSON-ie i on wtedy sam zdefiniuje, jak ten JSON powinien być skonstruowany. A tu mamy JSON Schema i tą Schema musimy wygenerować, na przykład akt... I on tutaj sobie tą generuje definicję. Oczywiście my możemy to napisać sami i to jest właśnie definicja tej JSON Schema, w której on odpowiada. I teraz jak zobaczymy w tym kodzie przekazujemy informację, że chcemy dane w formacie JSON i zdefiniowanym w jakim formacie JSON. I już OpenAI ma jakiś mechanizm, który po prostu parsuje odpowiedź z modelu językowego tekstową i właśnie umieszcza ją w formie JSON-a. Być może jest tam jakiś po prostu system, który dodatkowo wysyła dodatkowy prompt, że weź tą odpowiedź tekstową i wrzuć ją do tego JSON-a. My możemy po prostu z tego skorzystać, więc będziemy oczekiwali, że zawsze model nam odpowie w takiej samej formie. I tutaj przykład jak to działa w praktyce. Wyślemy jakiś prompt do modelu językowego i sprawdzimy jak on odpowie w ramach JSON schema i JSON object. Ok, więc w przykładzie pierwszym wymusiliśmy ogólny JSON za pomocą funkcji JSON Object i jak widać mamy jakiś JSON Object, który właśnie w taki sposób wygląda, natomiast w przykładzie drugim wymusiliśmy konkretną JSON Scheme i tak jak widać model też się do niej zastosował i w taki sposób nam odpowiedział. to akurat jest lista keywordów, które powinniśmy użyć i inne dane, które są dla nas istotne z tego punktu widzenia. Kolejna taka zamysłana funkcja, z której pozwalają nam korzystać modele językowe, to są jakieś narzędzia hostowane, czyli takie narzędzia, które są dostępne w ramach modelu. To wyjaśnimy na przykładzie WebSearch od OpenAI. Tak jak wiesz, ChatGPT ma możliwość korzystania z wyszukiwarki internetowej i my możemy go zmusić, żeby z tej wyszukiwarki internetowej właśnie skorzystał. I teraz przechodząc do Playground, to możemy tutaj właśnie skonfigurować websearch i możemy skonfigurować kraj, dla jakiego będziemy dokonywać wyszukiwania w tym promptzie, czyli na przykład websearch dla Polski. No i teraz model wie, że jak wyślę jakieś zapytanie do tego konkretnego endpointu, tego API, jeżeli wyślę jakieś zapytanie, które będzie wymagało wyszukiwania i model sam zdecyduje, czy należy wyszukać jakieś informacje, to on będzie właśnie szukał tych informacji w Bingu, w języku polskim. I to podam przykład. Włączamy tą funkcję przez API. Korzystamy z modelu GPT-4.1 i teraz akurat mam dzisiaj ochotę na kebaba, więc pytam, jakie restauracje w Krakowie polecisz, jeśli mam ochotę na kebab. Akurat nie znajdujemy się w Krakowie, ale w Warszawie to możemy zmienić. Dzisiaj będę z Mateuszem powiem, zawał kebaba, więc skonfigurujemy, jaki kebab możesz polecić. i jak mam włączoną tą funkcję websearch to OpenAI sobie przetworzy to moje zapytanie na jakieś zapytania do Binga na przykład wyślę keyword do Binga kebab w Warszawie Mokotów sparsuję te wyniki poszukiwania zastosuję jakiś dodatkowy prompt na przykład jak bym tu napisał, nie wiem, że lubię na przykład nie wiem, mięso wołowe albo wołowo baranie, no to bym pewnie pod tym kątem przeszukał te kebaby i jak wiesz, rynek gastronomiczny jest bardzo dynamiczny, więc nie miałby sensu pytać takiego modelu o kebaba na Mokotowie, kiedy wiem, że on ma wiedzę do 2024 roku. Więc po to jest ten właśnie website, żeby on mógł z tego skorzystać i nie musimy wtedy my dostarczać mu tych danych, tylko on sobie po prostu z niej skorzysta. Więc teraz zapytamy go o tego kebaba, on dokona wyszukiwań w ramach swojej bazy wiedzy. Przepraszam. On dokona wyszukiwania bingu i zwróci nam odpowiedź. Więc jest to jakaś odpowiedź od OpenAI i tu mamy sparsowane. Kebab u Karima, otwórz teraz, to znaczy, że model sprawdził, dostanę na samą informację w bingu, że jest teraz otwarte. Średnio to kosztuje 20-40 zł, ma takie oceny. I tutaj jeszcze bardzo ważne na przykład, że klienci chwalą domowy smak, ręcznie robione pity i świeże składniki. Szczególnie polecana jest wersja z wołowiną oraz wyjątkowy sos czosnkowy. Więc jak nas chciał stworzyć jakiegoś asystenta, który polecałby kebaby, na przykład, nie wiem, dla alergików, dla kogoś, to mógłbym właśnie skonfigurować taki prompt, skonfigurować narzędzie WebSearch i wtedy model korzystałby z wyszkiwarki internetowej, żeby na to pytanie odpowiedzieć użytkownikowi. I tak właśnie działa ten WebSearch. Mógłbym jeszcze to właśnie dodatkowo użyć tutaj JSON-Skima, więc miałbym te kebaby w formie właśnie JSON-a i mógłbym na przykład na jakiejś stronie internetowej to po prostu umieścić. Jeżeli chodzi o narzędzia, to tutaj postawimy kropkę, natomiast muszę tutaj jeszcze zaznaczyć, że są inne narzędzia w ramach OpenAI, chociażby coś takiego jak File Search i w ramach tygodnia zdaje się czwartego będziemy omawiać powiedzmy RAGI, modele embeddingowe, ale tutaj możemy po prostu aplaudować nasze pliki, czyli na przykład, nie wiem, jakąś firmową bazę wiedzy mógłbym zaplaudować albo jakieś sprawozdanie finansowe czy cokolwiek innego i dołączyć do tego promptu. Więc teraz, jak będę wysyłał zapytanie, to model będzie miał jeszcze dodatkową informację, że w ramach narzędzi jest coś takiego jak Vector Search i tu jest nazwa pliku, ID pliku, który dołączyłem do tego zapytania. Więc dzięki temu za każdym razem na przykład jakbym wysłał prompt, na przykład o załóżmy miałbym swój research kebabów i wrzuciłbym go w PDF-ie dla modelu językowego, to on wrzucając ten PDF tutaj to jeżeli wysłałby prompt, żeby mi znalazł jakiś dobry kebab to on mógł przeszukać ten plik i właśnie zwrócić odpowiedź właśnie na podstawie analizy tego pliku PDF. I dodatkowe jeszcze funkcje w ramach API OpenAI to jest batch API. To znaczy, że OpenAI, jak wiadomo, użycie modeli językowych takich dostawców modeli jest różne. Na przykład w niedzielę, w nocy ono będzie trochę mniejsze, prawda? Czy na przykład w sobotę rano trochę mniejsze. Więc OpenAI udostępniło taką funkcję jak batch API, więc jeżeli nie potrzebujemy na przykład odpowiedzi od razu, tylko możemy na nią poczekać do 24 godzin, to OpenAI otworujemy taką funkcję. ok, wyślijcie nam to, my wam damy 50% taniej tokeny, ale poczekacie 24 godziny odpowiedź. Więc na przykład jeżeli generujemy na przykład 1000 artykułów, nie potrzebujemy tego od razu. I jak chcemy zrobić to 50% taniej, to możemy to w API batchowym wysłać. To, co jest ważne, to to, że tutaj wysyłamy wszystkie prompty w ramach jednego pliku i możemy wysłać w jednym pliku do 50 tysięcy promptów i plik może mieć maksymalnie 200 megawajtów. Więc tutaj sobie konfiguruję te prompty. Oczywiście ja teraz tego nie zrobię tak w całości, po prostu wyślę to batch API i zasymulujemy jak to by wyglądała komunikacja z OpenAI. Więc wysłaliśmy tutaj, on zborzy coś takiego jak job ID i możemy zawsze wysyłać request do API OpenAI, żeby sprawdzić czy to ID zostało już obsłużone i czy są wyniki dostępne dla tego konkretnego zapytania. Więc trzeba by było jeszcze napisać jakiś po prostu mechanizm, który zapisywałby to do jakiejś bazy danych i na przykład raz na kilka godzin zrobić odpytanie endpointu OpenAI, czy on skończył pracę, a później dostaniemy właśnie 500 odpowiedzi w jednym pliku. No i to jest chyba w zasadzie tyle, jeżeli chodzi o komunikację z API modeli językowych. Ja Cię bardzo zachęcam, żebyś przeszła, przeszedł jeszcze raz ten otatnik samodzielnie, przeczytając wszystkie komentarze, bo tam w tych komórkach tekstowych bardzo dokładnie jest opisana jaka funkcja z jakiej korzystamy i jak ona ma zależności. I żeby cię nie wystraszyć, nie musisz rozumieć jak te kody działają na tym etapie, wystarczy, że będziesz rozumieć te koncepty, które wokół API są stworzone. Przygotowałem jeszcze w tym notatniku taki specjalny quiz sprawdzający wiedzę, więc wystarczy, że tutaj wybierzesz konkretne odpowiedzi i wywołasz tą komórkę i model powie czy udało ci się poprawnie odpowiedzieć. to w ramach takiej sprawdzenia, czy udało się wszystkie koncepty tego notatniku zrozumieć. I to tyle w tej lekcji. Dziękuję i zapraszam do kolejnej.