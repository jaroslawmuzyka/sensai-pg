
Gemini
Hello, Mateusz
How can I help you today?
What can Gemini do
in Google Drive
Summarize a topic
based on files in my Drive
Summarize a folder
in my Drive
Gemini for Workspace can make mistakes, including about people, so double-check it. Learn more
W tej lekcji poznasz trzy narzędzia, które pozwalają dostosować dane dla modeli językowych w taki sposób, aby były one przyjazne i były w możliwy, łatwy sposób przetwarzane przez modele językowe. Te narzędzia będą przewijać się w różnych lekcjach, czy to moich, czy Roberta, więc warto, żebyś dokładnie je poznał/poznała. Zaczniemy od pierwszego, które ja bardzo osobiście lubię i które wykorzystuję w wielu swoich automatyzacjach. Jest to narzędzie Gina Reader. Gina to jest taka firma, która pewnie też tutaj zagości nie raz w niektórych naszych odcinkach, bo produkuje różne rozwiązania AI-owe, czy to modele rankingowe, czy to modele embeddingowe, czy to właśnie narzędzia wspomagające przygotowanie danych dla modeli językowych. Jednym z takich narzędzi jest właśnie Gina Reader. To jest narzędzie płatne, natomiast jego koszt jest bardzo niski, więc tutaj akurat się tym nie musimy szczególnie przejmować. Kosztuje to za 200 dolarów mamy 11 miliardów tokenów do przetwarzania, czyli treści pobranych ze stron internetowych. Myślę, że to naprawdę wystarczy na nieskończenie wiele, bo to tak jakbyśmy chcieli kilkukrotnie całą Wikipedię przekraulować, to mniej więcej taka skala działania. Jak to działa? Generalnie działanie tego narzędzia jest bardzo proste. Wpisujemy tutaj adres URL do tego narzędzia, ono zwraca tekst danej strony internetowej w formacie markdown. To narzędzie ma też dostępne API, natomiast pokażę Wam, jak to konkretnie działa. Tu wpisuję sobie domenę r.gina.ai/lojalemon.pl. I teraz dostaję te dane w formie takiego markdowna razem z tytułem, adresem URL i całą treścią tej strony, która jest zawarta na tym adresie konkretnym URL. I działa to w taki bardzo prosty sposób. Wysyłam adres URL do Gina Readera. Gina Reader zwraca mi treść w formacie markdown, taką przyjazną dla modelu językowego. Teraz jak ja to wysyłam do modelu językowego, to będzie to zdecydowanie lepszy format niż gdybym wysłał taki czysty HTML, z którym może model językowy mieć problem. Poza tym taki nieoczyszczony HTML zazwyczaj zawiera dużo dodatkowych tagów, w związku z czym gdybyśmy taki HTML wrzucali do modeli językowych, to przetwarzanie takich danych byłoby droższe. Gina Reader ma też kilka dodatkowych funkcji. Ma też wyszukiwarkę wbudowaną, więc jeżeli wpiszemy zamiast r.gina.ai, s.gina.ai i wpiszemy jakieś słowo kluczowe, to zwróci nam na przykład adres URL, które na dany temat znalazłam. Ale też umie czytać obrazki, to znaczy, że jak na stronie internetowej znajdują się jakieś obrazki, to zamiast obrazka dostaniemy jego opis, czyli to, co się na nim de facto znajduje. Jest to przydatne też w kontekście rozumienia całego tekstu. Czyta też PDF, więc jeżeli mamy jakieś dane, jakiś URL w formacie PDF zapisany, to możemy też go przetworzyć i wysłać do modelu językowego. Więc tutaj możemy skorzystać z API. I też pokażę na końcu tej lekcji, w jaki sposób to API podłączyć na przykład do automatyzacji w Make'u. Więc wystarczy, że po prostu wpiszę adres URL tutaj, wyślę i to w zasadzie tyle. Mogę jeszcze ustawić kilka dodatkowych parametrów, w jakim formacie chcę te dane. Najczęściej właśnie wykorzystujemy format Markdown w przypadku modeli językowych, ale jest też w stanie, że Gina zwróci nam te treści w formacie HTML, jest też w stanie zrobić screenshot'a strony, czy page shot'a, czyli cały adres URL. Możemy ustawić timeout, możemy crawlować konkretny CSS selektor, możemy poczekać na jakieś dane i tak dalej, i tak dalej. Jest tu kilka takich. Możemy też oczywiście wykorzystać server proxy. Więc to jest jedno narzędzie, które ja wykorzystuję bardzo często i działa bardzo sprawnie, szybko i nigdy nie ma żadnych problemów. Drugie narzędzie, które jest dość popularne i zdobywa dużą popularność, to jest firecrawl. Firecrawl w odróżnieniu od Giny, gdzie podajemy adres URL, to w firecrawl możemy podać domenę i firecrawl całą tą domenę będzie starał się scrawlować. Natomiast on działa bardzo podobnie do Giny, natomiast ma tu taką funkcję, która jest bardzo ciekawa, LLM Extract się ona nazywa. I de facto modele językowe pozwalają wnieść scrawlowanie stron na trochę wyższy level, bo do tej pory, jeżeli chciałeś/chciałaś scrawlować jakąś stronę internetową i zawierały się na niej jakieś informacje, które chciałeś/chciałaś przetworzyć, to trzeba było wskazać np. CSS selektor lub XPath, gdzie znajdują się te informacje. Za pomocą modelu językowego możemy pobrać dane do formatu markdown, a później za pomocą promptu spytać o wyekstraktowanie poszczególnych informacji. I możemy to robić z wielu stron, np. jeżeli byśmy chcieli zapytać, czy ta firma, do której należy ta strona pochodzi z Polski, to mogę np. wziąć domenę senuto i tutaj skonfigurować, w jakim formacie będą zwracane dane i jakie zmienne powinien mi zwrócić ten crawl. Czyli mogę np. zmienną ispolishcompany i w formie stringa chciałbym, żeby mi zwrócił. I teraz jak scrawluję tą stronę, to Firecrawl pobierze treść z tej strony i zapyta model językowy, który ma wewnątrz, nie wiemy jaki tam jest model językowy, natomiast to narzędzie posiada model językowy i zapyta, czy na podstawie tych informacji, które udało się zebrać, może ocenić model językowy, że ta domena należy do firmy, która zlokalizowana jest w Polsce. Czyli jeżeli będą tam np. jakieś dane adresowe lub np. NIP, który by wskazywał na to, że firma jest polska, to model językowy najprawdopodobniej będzie w stanie odpowiedzieć na to pytanie. I gdybyśmy mieli np. tysiąc stron internetowych i chcielibyśmy sprawdzić, które należą do firm Polski, to jest to właśnie super narzędzie do tego, bo nie musimy konfigurować XPathów ani CSS selektorów, ponieważ jest to zrobione za nas. I teraz Firecrawl będzie crawlował tą stronę i odpytywał model językowy o to, czy ta firma jest z Polski. I tutaj zwróciło mi jeden, to znaczy, że wykrył, że firma synu.com jest z Polski, no bo mamy tam dane adresowe z Polski, mamy pewnie polski NIP i tak dalej. Weźmy jakąś inną np. firmę, która teoretycznie nie jest z Polski, np. Disney.com. No i tutaj zwrócił false. Tutaj trzeba było jeszcze dopracować format danych, ale jako, że Disney.com nie jest polską firmą, pewnie na stronie Disney.com nie znajdują się żadne elementy, które by na to wskazywały, no to z tej ekstrakcji wiemy, że to firma polska nie jest. I to jest super narzędzie, bo możemy w ustrukturyzowany sposób dostać dane, które są nieuporządkowane i w bardzo prosty sposób je uporządkować. Czyli w przypadku SEO, jak np. crawlujemy konkurencję, to np. moglibyśmy, nie wiem, wyciągać autorów z bloga, wyciągać tagi z bloga, pomimo, że nie wiemy, gdzie one są umieszczone, to jeżeli w całym kontencie się one znajdują, to model językowy będzie w stanie to wyekstraktować. Oczywiście moglibyśmy tu wykorzystać dżinę i wziąć treści z dżiny i wysłać dziennie zapytanie do API OpenAI, żeby te dane scrawlował, natomiast FireCrawl ma to natywnie wbudowane. FireCrawl też jest narzędziem open source. W materiałach do tej lekcji będzie link do repozytorium na GitHubie. Można tego FireCrawla u siebie zainstalować, ale można też korzystać z takiej wersji przeglądarkowej, jeżeli ktoś nie chce w ogóle korzystać z takiej wersji, gdzie trzeba programować, to tutaj jest po prostu cennik, można korzystać z tego narzędzia w taki sposób, jak z takich popularnych crawlerów, które obecnie są dostępne na rynku. Myślę, że każdy taki model dokładnie zna. Kolejnym narzędziem, które pojawiło się dość niedawno, a bardzo jest ciekawe i zacząłem z niego też korzystać, to jest Crawl4AI. I to jest taki konkurent dla FireCrawla, bo posiada tak w zasadzie te same funkcje, natomiast chwali się tym, że jest znacznie szybszy i to też jest narzędzie tylko open source. Crawl4AI nie ma jeszcze takiej wersji komercyjnej. Być może jak to oglądasz, ona już się pojawiła, bo jak wiadomo, w świecie AI wszystko się zmienia. Natomiast tutaj możemy uruchomić to na swoim serwerze przez dockera, albo korzystać na przykład w Google Collabie jako skrypt w Pythonie. Możemy załadować bibliotekę Crawl4AI i korzystać z niej. Pokażę Ci kilka funkcji, które są tu ciekawe. Udostępnię oczywiście ten Google Collab, żebyś mógł lub mogła sobie też z tego skorzystać. Instalujemy wszystkie potrzebne biblioteki i tutaj pierwsza funkcja to jest właśnie pobieranie danych do markdownu. Mogę pobrać treść strony senutokom.pl, tak jak pobierałem przez Gina Reader. On tutaj pobiera i mam całą treść, która się znajduje na tej stronie w takiej formie właśnie przetworzonej, podobnej do Gina Reader i za darmo. Oczywiście muszę tutaj mieć dostęp do Google Collaba, który też jest darmowy, natomiast to już nic nie kosztuje, więc to jest bardzo ciekawa funkcja. Oczywiście ten Crawl4AI jest też bardzo szybki z tych benchmarków, które oni podają jest dwa razy szybszy niż Firecrawl. Oczywiście tutaj podałem jeden adres URL, mógłbym ten skrypt skonstruować w taki sposób, że te adresy URL przechowuję na przykład w bazie danych i crawluję je każdy oddzielnie. To tylko przykład. Można też używać proxy. Tutaj akurat nie wykorzystujemy proxy, ale jeżeli byśmy daną witrynę chcieli bardzo mocno crawlować albo chcielibyśmy crawlować na przykład wyniki wyszukiwania i zbierać pozycje za pomocą modelu językowego, to pewnie serwery proxy by się przydały, ale jest też właśnie opcja ekstraktowania danych w formie ustrukturyzowanej, czyli w formie JSON-a i tutaj możemy właśnie skonfigurować, co chcielibyśmy z danych stron wyekstraktować. Na przykład załóżmy, że mamy, nie wiem, 100 firm SaaS, które są naszymi konkurentami i chcielibyśmy poznać ich cenniki. No to każda strona cennika w każdej firmie jest trochę inaczej skonstruowana, więc byłoby to trudne zadanie, więc do tego właśnie wykorzystać możemy takie narzędzie. Tutaj akurat podałem stronę cennika Senuto na stronie wersji angielskiej. Konfigurujemy tutaj model językowy, który będzie służył do ekstrakcji tych informacji. Akurat ja wybrałem model GPT-4.0 i konfigurujemy prompt, który wyślemy do modelu językowego, czyli tutaj Crawl for AI pobierze treść tych stron w formie markdownu, wyśle je do GPT-4.0 przez nasz klucz API i naszym skonfigurowanym promptem wyekstraktuje te dane, następnie nam wyświetli, będziemy mogli je zapisać do bazy danych. I tutaj skonfigurowałem właśnie prompt, który wskazałem modelowi, że chce wyekstraktować z tej strony pakiety, ich ceny, płatność, czy jest roczna, czy miesięczna, czy to jest pakiet, czy add-on, czy płatność jest roczna, czy miesięczna właśnie. I kiedy właśnie klikniemy tutaj ten skrypt, to Crawl for AI pobierze treść tej strony do markdowna, wyśle do modelu GPT-4.0 zawartość tej strony i model przeanalizuje ten treści i powinien w takim formacie JSON je zwrócić. Więc jeżeli bym miał na przykład 100 takich stron, których nie wiem dokładnie, jak są skonstruowane te cenniki, to właśnie model językowy może pomóc mi je scrawlować. To jest właśnie nowa era crawlowania, gdzie nie musimy mieć takich danych ustrukturyzowanych, a bardzo łatwo je ustrukturyzować. Wcześniej bardzo dużo czasu trzeba byłoby poświęcić, żeby te dane w takiej formie, w jakiej chcemy przetworzyć. I tutaj widzimy, zwrócił nam dane właśnie w takiej formie, jak poprosiliśmy. I mam na przykład pakiet Lite, który kosztuje 39 euro miesięcznie za miesiąc. I to jest pakiet i kiedy płacimy za miesiąc tyle on kosztuje. Mamy pakiet Lite, który kosztuje 32 euro, kiedy płacimy za rok, czyli płatność jest annual. No i mamy też pewnie jakieś dodatki, na przykład SERP Analyzes, który kosztuje 12 euro miesięcznie, jeżeli płacimy za miesiąc. I to jest add-on, a nie pakiet. No i tak bym mógł na przykład pobrać te wszystkie inne strony internetowe i też zapisać sobie je w bazie i w prosty sposób je przecrawlować. Teraz pokażę Ci, jak podobny efekt osiągnąć za pomocą narzędzia no-code, które służy do automatyzacji, nazywa się Make. Zapewne już w trakcie kursu albo jeszcze w jego trakcie będziemy z tego narzędzia wielokrotnie korzystać. To jest rozwiązanie dla osób, które nie chcą korzystać z takich rozwiązań, gdzie trzeba pisać skrypty Python albo ich używać. Więc spróbujemy zrobić taki szybki crawler, który wejdzie na stronę, pobierze jej treść i wyśle zapytanie do modelu językowego, żeby przeanalizował, czy firma, na której stronę weszliśmy, znajduje się w Warszawie oraz przygotował krótki opis tej firmy. Więc przygotowałem wcześniej plik Google Sheets, który też będziesz miał dostępny, będziesz miał o dostępnych materiałach poniżej. Link Google Sheets ze wszystkimi adresami stron, które chciałbym scrawlować. I tu jest automatyzacja w Make. Pamiętaj, że w Make można zaimportować plik z automatyzacją. Ten plik znajdziesz w materiałach, więc możesz go zaimportować i od razu z niego korzystać. Więc tutaj w pierwszym kroku łączymy się z tym naszym plikiem Google Sheets, który przygotowaliśmy. W drugim kroku łączymy się z API Gina. I tutaj API Gina, tak jak powiedziałem, wpisujemy to po prostu adres rgina.ai/adressurl, który jest pobierany z tej komórki. Wybieramy, że połączenie robimy metodą get. Wszystko, co tu musimy zrobić, to tylko podać klucz autoryzacyjny, który znajduje się na stronie rgina.ai/reader i w zakładce API, mamy swój klucz API, jeżeli już zapłaciliśmy za to. I pobieramy teraz w pętli wszystkie treści ze wszystkich adresów URL, które znajdują się w tu pliku Google Sheets. Następnie korzystamy z modelu językowego. To okładz wybieramy w Make model, do którego chcemy wysłać zapytanie. Tutaj wysyłamy zapytanie do modelu GPT-4O mini. To jest ten najtańszy model, który ma też 128 tysięcy tokenów na inpucie, więc możemy tam całkiem sporo informacji wrzucić. Wybieramy role system, czyli konfigurujemy tą charakterystykę tego, powiedzmy, agenta. I prosimy go, żeby przeanalizował treść strony i powiedział, czy firma, z jakiego miasta jest firma, jaki typ firmy to jest i żeby opisał nam tą firmę, czym ona się zajmuje. Tu jest cały prompt wygenerowany przy pomocy klode. Też go będziesz mógł wykorzystać, mogła wykorzystać w swoich automatyzacjach. Natomiast w roli user podajemy tutaj informację, że tutaj znajduje się treść tej strony, czyli ta pobrana z wcześniejszego kroku. I klikamy tu OK. No i zapisujemy te dane. Oczywiście jeszcze Make ma taką opcję, że jest w stanie parsować te dane. Tu wybieramy taką opcję response format map i JSON object i parse on JSON response true. Więc jeżeli w polu system skonfigurujemy, w jakiej formacie w JSON-ie chcielibyśmy te dane otrzymać, to Make będzie starał się parsować te dane do takiego formatu i dzięki temu będziemy w stanie od razu te dane umieścić w poszczególnych kolumnach w Google Sheetsie, co jest bardzo wygodne. Więc tutaj, jak mamy już tą sparsowaną informację, to przekazujemy informacje, gdzie te dane mają zostać zapisane. Więc row number to jest row number z tego kroku, który tu pobraliśmy. URL to jest też to. Description to jest to, co wygenerował model językowy, czyli tutaj jego wybór. Oraz city name, czyli nazwa firmy, gdzie ta firma operuje, czyli location city, city name. No i teraz, jak uruchomimy taką automatyzację, to powinien się mniej więcej odbyć taki proces, jak się odbywa w crawl for AI, w tym skrypcie, który pokazywałem, albo w file crawlu, czyli w tym LLM extract, czyli gdzie wyciągamy te konkretne informacje, które po prostu potrzebujemy. Natomiast to jest taka wersja no-code. W file crawlu oczywiście możemy zrobić to bardzo podobnie, natomiast tutaj będziemy w stanie tego narzędzia do wielu innych rzeczy też użyć i będziemy go wielokrotnie używać, więc to jest opcja dla osób takich, powiedzmy, które wolą no-code niż code. Automatyzacja zapisuje poszczególne elementy, czyli pobrał wszystkie tutaj adresy URL, pobrał z nich treść poprzez gina.api, przeanalizował model językowy tę treść i zapisał odpowiednie informacje make w naszym pliku Google Sheets. Teraz możemy przejść, zobaczyć. Tutaj mamy firma Senuto. Senuto to platforma oferująca zaawansowane narzędzia SEO i analizy treści. To nie jest meta description, to jest opis, który stworzy model językowy na podstawie analizy treści. I city name, czyli nazwa miasta, z której pochodzi dana firma, czyli jakby gina pobrała treść ze strony, a LLM przeanalizował, że w tej treści znajduje się mniej więcej lokalizacja, czyli to jest Warszawa lub tutaj, jak na przykład w przypadku white.com, bielsko-biała, co by się zgadzało. Więc jeżeli mamy na przykład taki duży zestaw danych i chcemy nacechować go różnymi typami danych, to się nazywa data outreachment, czyli na przykład mamy domeny, chcielibyśmy się dowiedzieć na przykład, o czym one są, skategoryzować je, przypisać kategorie tej firmie i tak dalej, i tak dalej, to możemy właśnie taką automatyzację w make'u stworzyć, tą, którą ja tutaj pokazałem, będzie ona dostępna w materiałach. No i to są narzędzia, które służą do tego, żeby poprawnie dostarczyć dane do modeli językowych. Wymieniliśmy FireCrow, Gina i Crow4AI i też pokazałem Wam, jak to zrobić w make'u, więc jest to gotowy taki zestaw narzędzi do tych działań. Będziemy w trakcie kolejnych lekcji jeszcze wielokrotnie do tego się odnosić i wracać.