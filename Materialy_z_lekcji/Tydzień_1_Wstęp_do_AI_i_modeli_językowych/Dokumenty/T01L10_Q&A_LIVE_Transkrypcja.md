Także witam oficjalnie Państwa na pierwszym naszym spotkaniu live.

Na chwilę obecną mamy 53 osoby, ciekawe ile będzie w szczycie.

Jesteśmy po pierwszym tygodniu z wstępu, bardzo podstawowy tydzień, ale niezbędny, żeby przejść dalej.

Pojawiło się trochę tutaj truść z Waszej strony, na przykład od Pawła Cengla pojawił się kontakt DeepSeek'a, więc sobie dzisiaj porozmawiamy również o DeepSeek'u.

Nie był planowany w kursie, no ale skoro pojawiła się taka potrzeba, to będziemy sobie o tym rozmawiać.

Ja pokażę również Manusa.

może coś sobie wygenerujemy manusem, to jest taki agent do stworzenia różnych śmiesznych rzeczy, który został udostępniony publicznie 2-3 dni temu.

Wcześniej był udostępny tylko na zaproszenie, prawie dostępny publicznie.

Szerzej tego typu obszary będą omawiane w tygodniu dotyczącym Vive Codingu i kodowania z AI, ale jakieś ćwiczenie możemy sobie dzisiaj wykonać.

W kalendarzu mamy przeznaczone godzinę na ten live, natomiast jak będzie więcej więc oczywiście zostaniemy z Wami dłużej.

Damian już zapowiedział temat Slido, więc tam sobie po prostu róbmy ewentualne pytania w formie tekstowej.

Będzie porządek.

No i co, jeśli chodzi o zadawanie pytań głosowych, to bym miał taką prośbę, jakby każdy się wyciszył, żeby nie wprowadzać kakofonii.

Jeżeli ktoś chce zadać pytanie, to zróbmy taką kulturę, że łapka w górę i po kolei sobie będziemy szli po łapkach w górę.

Damian, czy coś jeszcze w kontekście organizacyjnego na teraz, na live?

Tak, jeszcze jeden z kontekst chciałem poruszyć, bo jest spora część osób, które są nowe i to jest 200 osób i część osób, które już były na poprzednim kursie i to jest 160 osób, czyli łącznie jest nas 360 osób. No i dla tych osób, które już były, to chciałem po prostu tak skomunikować, że my staramy się robić ten kurs w taki sposób, żebyście mogli pomijać te rzeczy, które są dla was oczywiste, więc jeżeli na przykład jest jakaś wiedza, która już była, koncepcja tego, że jest 160, 200 nowych osób wymaga, żebyśmy przeszli przez wszystko ponownie, to po prostu sobie to pomijajcie, ale jest w każdym tygodniu sporo nowych rzeczy, więc możecie do tego podejść selektywnie.

Też Robert, to miałem zacząć od tego, żeby pokazać, jak z tym hit-hubem sobie pracować, tak?

No dokładnie, jeżeli możesz zaprezentować to od przyjemnością, to pokaż mię w spotk.

Dobra, tak jak wspomnieliśmy, to chcieliśmy w tym kursie dołożyć kilka narzędzi dydaktycznych, które mają wesprzeć taki proces nauki, który dopasuje się do każdego z Was.

No i w ramach tych narzędzi dydaktycznych to są repozytorium GitHub, co zaraz pokażę, podcast z podsumowaniem, który Mateusz wygeneruje i wróci do tygodnia pierwszego, agent głosowy, którego też za moment udostępnimy, który będzie miał dane z lekcji i będzie można zawsze do niego zadzwonić, o coś do niego dopytać.

i teraz chciałem pokazać jak z tym repozytorium na GitHub współpracować.

Jeżeli ktoś nie ma dostępu albo ktoś nie może uzyskać dostępu z jakiegoś powodu, to Mateusz chętnie tutaj pomoże.

Mateusz Wyszagrodzki to jest osoba, która nagrywa konkurs i dba o to, żeby wszystko było technicznie ładnie spięte.

No i teraz o co chodzi z tym repozytorium?

Generalnie wybraliśmy miejsce GitHub jako miejsce, do którego będziemy zrzucać wszystkie swoje materiały, dlatego że tutaj bardzo łatwo umieścić kod, bardzo łatwo zarządzać po prostu współpracą, no i też da się połączyć do repozytorium GitHub z różnych innych miejsc.

No i takim polecanym narzędziem, które można wykorzystać do tego, to jest cursor, czyli taki IDE do programowania.

On będzie omawiany w kursie, jak go użyć do programowania, ale teraz pokażę Wam, jak go użyć do tego, żeby z tym repozytorium rozmawiać.

Oczywiście udostępnimy pewnie też jakąś inną metodę komunikacji z tym repozytorium, natomiast tak będzie pewnie najprościej.

No więc, żeby to repozytorium sobie do tego cursora załadować, to wystarczy zainstalować tego cursora i kliknąć tutaj New Window, klon repo i kopiujemy po prostu adres URL tego repo z naszego konta.

I to, co jest bardzo ważne, to jest coś takiego jak cursor rules.

I zarówno mamy cursor rules dla repo rule i to jest taki cursor rules, który używamy do tego, żeby mentorzy mogli publikować materiał w tym repo.

Dzięki temu każdy mentor, a jako że ich jest pięciu, każdy mógłby trochę inny styl komunikacji przyjmować, moglibyście się tym pogubić.

To te custom rules po prostu sterują tym, jak mają być katalogi sformułowane i tak dalej, i tak dalej.

Więc to po prostu pilnuje mentorów.

A tutaj mamy coś takiego jak zasady użytkownika.

To znaczy, że jeżeli dodam, tu mam ten czat, z którym komunikuję się z tym repo, to jeżeli dodam jako kontekst te custom rules, które tutaj są zaimportowane, czyli to wybieram, to ten czat dostanie jako kontekst te informacje, a w tych informacjach jest zawarte, co się znajduje w poszczególnych folderach i jakie informacje w tych folderach się znajdują.

No i teraz składując ten cursor rules do kontekstu, mogę sobie rozmawiać z tym repozytorium.

Na przykład mogę zapytać jakie zagadnienia...

Ktoś dzwoni teraz.

I on opowiada, tak, że...

A przepraszam.

Jan odpowiada także w tygodniu drugim, który będziecie mieli dostęp do tego już w tym tygodniu, poruszane były takie tematy jak Supabase, komunikacja Zapi i tak dalej.

w kontekście mogę dalej dopytać na przykład w kontekście komunikacji zapi jakie tematy były omawiane no i on na podstawie tego będzie w stanie też to zrobić, bo w każdej lekcji w każdym tygodniu jest transkrypcja tego co na wideo było mówione jak będą to się znajdowały jakieś na przykład kody na przykład Python jakiś, to będzie można po prostu załadować ten skrypt Pythona, który jest z repozytorium i poprosić cursora, żeby go przerobił pod nasze potrzeby Więc to jest jakby to kluczowe narzędzie dydaktyczne, które zawsze Wam pozwoli do tych materiałów w prosty sposób wrócić i w prosty sposób z nich korzystać.

Jeszcze tak najszybko mówię, co w tym repozytorium się znajduje.

Więc mamy kilka takich katalogów głównych, na przykład data sety, czyli jak będziemy udostępniać jakiś data set, który służy do wykonania jakoś ćwiczenia, to on zawsze w tym miejscu będzie do znalezienia.

Czyli na przykład jeżeli udostępnimy data set, załóżmy słów kluczowych, które służą do sklasyfikowania intencji, to on zawsze tutaj będzie.

a tu są materiały z lekcji, czyli wszystkie tygodnie i w każdym tygodniu mamy tutaj dokumenty, transkrypcję tych dokumentów.

W pliku readme zawsze są zapisane wszystkie notatki do każdej lekcji, czyli tu widzimy na przykład lekcja historia modeli językowych i tu są wszystkie linki, które były używane na przykład w lekcji wideo, wszystkie materiały dodatkowe, notatniki, kolaby i tak dalej.

Tu mamy katalog z promptami, czyli mamy tutaj wszystkie prompty, jakie będziemy używać w kursie, będą się tutaj pojawiały w takiej formie tabeli, więc będzie zawsze można do nich wrócić w takiej formie.

Plik z narzędziami, wszystkie narzędzia, które mamy w kursie, one są tutaj.

Oczywiście jak otworzycie to w GitHubie, to ładniej wygląda niż tutaj, to jest zapisane.

No właśnie Damian miałem o to prosić, bo teraz ten plik jest w Markdownie, tak jak to będę na przykładzie GitHub, a tam jest dużo ładniej to organizowane.

Tak, tylko czekaj, jak się to repona...

jak się to repona nazywa Mati?

Możesz dać na swoim klon.

Logika jest taka sama.

Tam w cursorze można kliknąć, żeby ładnie wyświetlał.

Dobra.

Więc jak jesteśmy w katalogu prompt, to widać mamy wszystkie prompty w takiej formie tabeli. Jak kliknę ten konkretny prompt, to się on otworzy i będę mógł zobaczyć jak on wygląda.

Tak samo jak mam na przykład narzędzia.

To wszystkie narzędzia, których używamy w kursie, one są tutaj opisane.

Wyszczególnione jest, w którym tygodniu dane narzędzie było poruszane, do czego ono służy i tak dalej. Dalej mamy notatniki Colab.

Jeżeli używamy jakiegoś notatnika w Google Colab, to w drugim tygodniu zaczniemy z tego korzystać. One są tutaj też opisane i ze wszystkich tygodni będą tutaj one trafiały.

I tutaj są wszystkie linki do wszystkich notatek ze wszystkich lekcji.

Czyli mamy tutaj tydzień, temat lekcji, link do notatki i link do transkrypcji, czyli jak wejdę sobie tutaj, to widzę notatkę z danego konkretnego wideo, a dalej widzę transkrypcję, jeżeli byście chcieli z nią jeszcze coś zrobić.

Czyli mam tutaj całą transkrypcję. I tak należy korzystać z tego repozytorium.

Czyli tutaj po prostu jest takie centralne źródło, jedyne źródło prawdy dotyczące materiałów z kursu.

Myślę, że to się udało nam dość mocno poprawić względem poprzedniej edycji.

Teraz wydaje mi się, że będzie porządek.

Ja mam pytanie od Pawła.

Paweł, jesteś wyciszony?

Ja tylko na szyku chciałbym wtrącić, dla osób, które pierwszy raz instalują cursora, dostaniecie odpowiedź od agenta w języku angielskim, więc jeśli będziecie chcieli po prostu przestawić go na wersję polską, to wyślijcie mu komendę odpisz po polsku.

Dzięki.

Też chciałem powiedzieć, że to niekoniecznie musicie tutaj korzystać z cursora, może to być Windsurf, Cline i cokolwiek, jakikolwiek fork, VS Code, który jest dostępny na rynku tych narzędzi i powstaje codziennie dużo, więc te custom rulesy powinny też w innych miejscach działać.

Akurat cursor do dużego wykorzystania trzeba za niego zapłacić, a Cline na przykład jest za darmo, więc można też to tam załadować.

Ja myślę, że też dołożymy trochę jakby innych interfejsów, żeby ułatwić tą komunikację.

Dobra, no to dzięki Damian za to.

Prośba taka, jeżeli ktoś nie ma dostępu, a chciałby mieć tam dostęp, to prośba o zgłoszenie do Mateusza Wyszogrodzkiego, dlatego wszyscy dostaną wszelkie konieczne dostępy.

Tak, jakby co też jak wiesz, jak wiesz, jak wiesz, jak wiesz, jak wiesz, jak wiesz, jak wiesz, Bardzo Cię Mateusz porłało, jakbyś powiedział jeszcze raz.

Na platformie dwie pierwsze lekcje, to są lekcje właśnie wprowadzające, gdzie macie ten formularz do uzupełnienia Waszego nicku na GitHubie, jeżeli jeszcze tego nie zrobiliście i wtedy dostajecie automatycznie zaproszenie do dołączenia do tego repozytorium.

No właśnie, teraz widzę, że jest 18 osób, więc jeszcze dużo przed nami.

Dobrze, o Krzysztof?

Ja jeszcze taka mała techniczna sprawa, bo jakby w materiałach z lekcji zdublowały się tydzień, piąty i szósty te materiały, bo wiemy, że Mateusz tam poprawiał N na N i w tej chwili jest jakby to zdublowane. Warto to usunąć, bo po prostu to niepotrzebnie. Gdzieś tam będzie dwa razy się tokenizowało i wysyłało gdzieś tam w tych materiałach, czy po prostu w cursorze, czy w tymkolwiek, więc posprzątajcie po prostu tam materiały z lekcji i jest po prostu, no, dwa foldery są do usunięcia z po prostu, no, z zawartością i zostawcie tylko te już poprawne.

No dobrze, oczywiście to zrobimy.

Słuchajcie, będziemy startować jeszcze zapytam Mateusza, czy tam jakieś formalności Twojej strony organizacyjnej na tym poziomie, czy możemy przejść do live'a?

Możemy lecieć. Dobrze, no to szanowni Państwo, zróbmy w ten sposób, że skupimy się zaraz na pytaniach dotyczących pierwszego tygodnia i na koniec jak wyczerpiemy zasób pytań dotyczących pierwszego tygodnia przejdziemy sobie do pytań wszelakich, ok?

Więc jeśli jest jakieś pytanie to proszę łapkę w górę lub na Slido.

Jeśli nie, to przejdę do kontekstu Dipsyka, który prosił Paweł Cengiel.

Na Slido pusto.

Łapek w górę nie widzę, na czacie też pusto.

Więc przechodzimy do Dipsyka.

Dipsyka nie umieściłem w agendzie kursu, dlatego, że aktywnie z niego nie korzystam, natomiast ma trochę ciekawych funkcjonalności, może nie funkcjonalności, ale przynajmniej modeli, o których chciałbym troszeczkę zagaić. Jak wiemy, jest to chińska produkcja i przyglądając się arenom, powiedzmy, ojezus, te strony internetowe są po prostu straszne, gdzieś te deepseek'i tutaj się znajdują, siódme miejsce, ósme miejsce, więc może to jest argument za tym, żeby go wykorzystywać.

Są to modele bardzo duże, więc nie jesteśmy pewnie wprost w stanie ich hostować u siebie, mimo że są to modele darmowe.

Co tutaj się ciekawego pojawiło?

Chińczycy, tworząc Lipsyka, wyszli troszeczkę z innego pomysłu treningu tego modelu i stworzyli coś, co się nazywa mixture of experts.

O co tutaj w tym wszystkim chodzi?

Jeżeli mamy duży model, dobraćmy sobie dowolny model z OpenAI, no to powiedzmy to jest ekspert we wszystkim.

Oni zrobili zestaw, już nie pamiętam ilu, chyba 16 ekspertów, który każdy odpowiada za inne zadanie, czyli powiedzmy jeden ekspert, który odpowiada za matematykę, inny za fizykę, trzeci za biologia, czwarty na przykład za historię, przez co to było bardzo innowatorskie podejście na tamte czasy, na tamte czasy, to chyba był styczeń, więc dużo się zmieniło i uzyskali niesamowity performance, co zapewniło im takie pozycje.

Dzisiaj wrócałem taką krótką notatkę dotyczącą również DeepSeaka.

Planowana jest wersja druga tego modelu reasoningowego od DeepSeek, czyli powiedzmy tego myślącego, który podobno zakłada odejście całkowite od architektury transformerów, to będzie niesamowitym nowością, jeśli to się potwierdzi, do architektury recursive cognition latency. Nie wiem, co to znaczy i też nie będę zabierał tego żadnego stanowiska w tym kontekście, ponieważ jest to informacja nieoficjalna, natomiast Damian, nie wiem, czy kojarzę, albo Krzysztof, na kiedy jest zapowiedziany DeepSync, bo tak od kilku tygodni jest zapowiadana nowa wersja, ale jeszcze jej nie dowieźli.

Macie jakąś wiedzę, co oni obiecują, kiedy to się pojawi?

Coś tam, Twitter generalnie mówił o przełomie maja i kwiecień, znaczy maja i czerwca, no nie?

Ale wiadomo, że to może być różnie, jakby tam z Chin te informacje średnio przychodzą, a jakby to, co gdzieś tam przeglądam z tych, wiesz, nowości gdzieś tam, to właśnie mówią, że jakby maj, koniec maja.

No właśnie, też tak od jakiegoś czasu widzę, że już za tydzień, już za dwa, już za kilka tygodni.

Więc potencjalnie, jeżeli się sprawdzi to, co nieoficjalnie napisał ten człowiek dzisiaj, jest notatka na Discordzie, no to możemy się spodziewać jakiejś rewolucji. Czy tak będzie? Nie wiem.

Natomiast schodźmy po kolei. Co my tutaj mamy w tym DeepSeeku? Jest to produkcja chińska.

Mają oczywiście czata, do którego zaraz sobie przejdziemy. Nie ma tam żadnych istotnych rzeczy w tym czacie, bo jest bardzo mało funkcji. Natomiast mają kilka właśnie tych modeli rezonningowych.

One są całkiem tanie i pewnie jeśli miałbym szukać jakichś plusów wykorzystania o DeepSicka, to z plusów na pewno będzie cennik, ponieważ te modele są zdecydowanie tańsze niż modele konkurencyjne.

Zobacz Paweł, zadałeś pytanie, więc odnoszę się do Ciebie.

Tu mamy te modele rezonningowe i powiedzmy najlepszy model rezonningowy pewnie w standard price'ie będzie kosztował na wejściu powiedzmy 0,14, na wejściu 2, a jeżeli byśmy przyrównali to samo do, może zmienię sobie tryb udostępniania, bo udostępniam kartę, udostępnie okno, będzie mi szybciej przeskakiwać dla Was.

Jeżeli byśmy przyrównali do modeli google'owych, to te, mimo że Google jest teraz nańsze, te różnice w cenie są zdecydowane na rzecz DeepSea, więc jeżeli mam szukać jakichkolwiek argumentów, żeby niej okorzystać, to na pewno argumentem będzie cena.

Dosyć poważne, że tak powiem, zniżki.

Są to głównie zniżki nocne, w konkretnych godzinach.

po prostu to trochę jak z prądem w sieci, mają go za dużo, więc dają możliwość realizowania jakiejś operacji dużo taniej.

To jest też kuszące w sumie z perspektywy SEO, bo zawsze można sobie zaplanować jakieś zadania powiedzmy w tych godzinach, zrealizować i oszczędzić.

To jedyny argument, który doszukuje się w celu wykorzystania DeepSeaka.

Co tutaj jeszcze jest ciekawego w tym DeepSeaku?

Czad.

Czad jest goły.

jest arcygoły ale tych okienek mam, próbuję je wyłączyć czad jest arcygoły, mamy możliwość tego deepthinkingu, czyli to co mamy wszędzie Wam pokazywałem na przykładzie czy to jakiegoś reasoningu czy jakiegoś głębiej w groku, no to jest to samo i co do zasady mamy raptem search oczywiście mamy multimodalność czy zgrywanie jakichś plików, ale to mamy wszędzie nie mamy tu Paweł żadnych funkcjonalności takich jakie nam dają pozostałe modele czatowe typu jakieś czy inne historie, więc no golas.

Golas.

No?

A czy mógłbyś Robert wymienić, nie wiem, top 3 zadanie, jeśli chodzi o deep secret, z którym radzi sobie dobrze i z którymi mógłby sobie radzić lepiej?

Słuchaj, ja mogę Ci pokazać jeden case, który zrobiłem na deep secret, to był jedyny case, jaki zrobiłem, to jest taka strona Armia Art, mogę Wam wysłać, na DeepSeeku w związku z cennikiem cennikiem wygenerowałem na tej domenie bardzo dużo kontentu, ja później w tygodniu moim dotyczącym kontentu będę wymieniać co tu się wydarzyło i dlaczego to zadziałało, natomiast mamy powiedzmy efekt tego kontentu, więc pewnie można założyć, że generowanie treści przez DeepSeek po dobrej cenie w dobrym procesie, który omówimy sobie w kolejnych tygodniach, działa, więc odpowiadając na twoje pytanie, pewnie generowanie kontentu jest okej.

Gdzieś była informacja w kontekście deep-seeku, że on zawiera w swoim korpusie treningowym elementy języka polskiego.

Jest to informacja dosyć stara, tak jak były publikowane te deep-seeki na początku roku, więc pewnie tutaj też mogę upatrywać jakiś przewag, jeśli to jest informacja prawdziwa, że jakiś tam jest element korpusu polskiego. No mają rezonning, więc jakby wnioskowanie dużo tańsze niż u konkurencji.

Chyba tyle.

Ja generalnie w tym momencie chyba nie ma sensu używać tego R1.

Rezoning masz Gemini Flash 2,5 wersji wersji Rezoning za 15 centów za milion tokenów.

Te topowe modele też odjechały, więc jeżeli Lipstick nie płyści czegoś w wersji 2, to pewnie bez sensu w tym momencie.

Aczkolwiek to głównie model, w ogóle to jest drugi model na świecie używany.

mają 8% rynku, ale głównie China i Rosji tam mają po 30 parę procent, więc to była taka chwilowa, chwilowy szał, przy czym konkurencja już dojechała i odjechała. W ogóle DeepSig został wytrenowany na danych z OpenAI odpytując ChatGPT w dużej mierze.

Tak, to faktycznie był ten skandal, w którym była awantura o to, że DeepSig desał.

Chyba, że wersja druga ciekawego opuści, no ale to też musicie się przyzwyczaić z tym modelami jest tak, że co tydzień może ktoś innego coś wypłucić nowego, więc trzeba sobie po prostu wypracować taki mindset, że się po prostu przełączamy w pięć minut na inny model, dlatego też jak jakieś narzędzia tutaj wykorzystujemy to takie, które pozwalają to robić. Na przykład jeżeli mamy tutaj cursor, to narzędzie do programowania, to jakiś model językowy nowy wyjdzie, to oni za pół godziny już go mają w panelu i można się po prostu przełączyć.

Więc tutaj trzeba być elastycznym, natomiast Dipsy w tym momencie to jest trochę trudno w kontekście modelu, który w styczniu nie jeszcze był to powy powiedzieć prehistoria, ale ale na razie bym tak to oceniał, czekałbym na wersję drugą, jeżeli ona coś przyniesie, to bym sobie do tego wrócił, no nie? No ja też, Adam, ja próbowałem, znalazłem w głowie jakiś argument, na pewno jest to po stronie ceny, ten jeden case, który się udał, no to po prostu odpaliło, jeżeli chcesz generować treści, to może to działać, ale więcej, więcej plusów tutaj nie dostrzegam ze względu na to, że konkurencja po prostu w tych prostych interfejsach ma po prostu zwykle nie dużo więcej funkcjonalności przydatnych, ma lipcy, trochę golas, no i faktycznie tak jak Damian mówisz, jeżeli ten R2, który jest zapowiedziany, prowadzi coś rewolucyjnego, a takie są jakieś sygnały, no to może wtedy będzie jego kolejny moment.

Robert pojawiały się dwa pytania na slajdę w międzyczasie.

Wiem, że Karol zgłosił rękę, ale tam te pytania były w pierwszej kolejności.

No dobra, to jedziemy z nimi.

Jest pierwsze pytanie, które jestem totalnie nowa w narzędziach AI i proszę o wytłumaczenie, czy ten program Kursor i druga alternatywa jest konieczna.

Co one zmieniają?

To dawiam, dawaj.

No więc odpowiadając, nie jest konieczne.

Natomiast w tygodniu dziewiątym będziemy uczyć programowania z wykorzystaniem modeli językowych i tam daniem, które będzie ten tydzień prowadził i jest mentorem tego tygodnia, on wykorzystuje cursor.

I cursora też możemy użyć do tego, żeby przeszukiwać to repozytorium na GitHubie, w którym znajdują się wszystkie materiały ze wszystkich lekcji do wszystkich tygodni. Natomiast też do tego repozytorium możesz wejść przez przeglądarkę i ręcznie się tam poruszać. Nie ma z tym problemu. Natomiast Wyślę też linki do tych narzędzi, bo może powiedziałem nazwy, a to nie jest takie oczywiste dla wszystkich.

Więc tu jest cursor, a tu Klein.

Więc nie jest to konieczne, natomiast ułatwi ci przyswajanie materiałów z kursu, a w tygodniu dziewiątym, jeżeli będziesz chciała go przejść, to pewnie ten cursor będzie istotny.

Natomiast to jest świetne narzędzie, nie tylko można go wykorzystać do pisania kodu, można też tam pisać artykuły, cokolwiek sobie tam wymyślisz.

to jest taki edytor tekstowy z możliwością przeszukiwania dokumentów i z różnymi modelami językowymi, więc zastosowań ma wiele.

I jeszcze tam było jedno pytanie, zaraz będziemy odpowiadać na pytania z czatu.

Ja tutaj chciałem tylko dopowiedzieć, że cursor stanowi właśnie ten interfejs, gdzie możemy komunikować się z tym GitHubem, możemy też czytać na stronie internetowej materiały.

Z zamianem wypracujemy jeszcze jakiś inny interfejs czatowy, który pozwoli Wam w bardzo łatwy sposób rozmawiać z materiałami, z którymi pracujemy.

Też na przykład jak poczytasz sobie o serwerach MCP, czyli powiedzmy takim standardzie komunikacji pomiędzy narzędziami a modelami językowymi, to cursor na przykład daje dużo możliwości, żeby tak doklodę, to będzie można korzystać z wielu dodatkowych funkcji, na przykład poprosić cursora, żeby połączył się na przykład z jakimś narzędziem, pobrał jakieś dane i ich użył w jakimś procesie.

Przyczyłem tu cursor, Klein i Winster, to są takie trzy narzędzia, które obsłużą te gepozytorium.

A jeżeli chodzi o takie narzędzie, które udostępnimy pewnie do komunikacji z tym GitHubem, to być może na DeFi-u zrobimy jakoś chatbota, który będzie w przeglądarce po prostu działał bez logowania.

Albo to, co napisał Roman, czyli Telegram.

To i wymyślimy, ogarniamy to.

Zobacz, ze slajdu to jest fajnie, że kohorta pojawiła się w niedzielę, czy planujecie każdy tydzień puszczać troszkę szybciej niż poniedziałki, czy to zależy.

Akurat pytanie bardziej do Mateusza, ale widzę, że on zniknął gdzieś.

Mateusz?

Plan jest taki, żeby puszczać wcześniej.

Pracujemy nad tym, żeby materiały nagrywać stosownie wcześniej. Część już jest w nagranych, część jeszcze jest w produkcji.

Będziemy się starać dostarczać wam jak najszybciej, żebyście mieli jak najwięcej czasu na...

To teraz Pan...

...w tą niedzielę, czy...?

Tak, no jakby no...

Dobra, super. To Robert, możemy teraz...

Karol miał pytanie, to Karol zapraszam.

Ja chciałem właściwie dopowiedzieć tylko do DeepSea, jeśli chodzi o jego zastosowanie.

to przed wdrożeniem bezpłatnej wersji 4.0 to Dipsick naprawdę dobre teksty w języku polskim generował, szczególnie dla osób, które nie chciały bądź nie potrafiły bawić się w promty, tylko proste zapytanie, napisz mi pracę na temat silnika samochodowego, to naprawdę jeśli chodzi o zastosowanie języka polskiego, Dipsick dużo lepszy tekst językowo wypluwał, natomiast już przy stosowaniu odpowiednich promptów ta jakość zdecydowanie skłaniała się w kierunku czatu GPT.

Teraz, jeśli porównujemy to w ogóle z wersjami takimi jak Gemini 2.5, czy mówimy o 4.5 czatu GPT, to R1 DeepSeaka już jest daleko w tyle.

Natomiast w momencie, w którym się pojawił dla takiego zastosowania dla osób zupełnie zielonych w tym temacie, naprawdę był ciekawym rozwiązaniem.

Być może dlatego mi ta armia odkłaliła również wtedy.

Podsumowując, jakby nie widzę zastosowania na dzień dzisiejszy anturażu całej konkurencji chyba żadnego.

Dzięki Karol.

To co, Grzegorz?

Halo, halo Grzegorz.

Grzegorz Opiatowski.

Teraz ok?

Tak, tak, słyszymy się.

Ok, cześć wszystkim.

Jeśli chodzi o czysto techniczne pytanie, jeśli chodzi o narzędzia płatne, to wystarczający jest OpenAI, ten chat GPT?

I to będzie okej?

Czy jeszcze jakieś płatne niezbędne są w czasie kursu?

Na tym etapie chat GPT na pewno jest wystarczający, jeśli chodzi o funkcje chatowe.

W kolejnych elementach kursu przygotujemy takie zestawienie.

Raczej nie będzie wielkich zastosowań płatnych, natomiast trzeba będzie wykupić na przykład trochę tokenów w OpenAI, jeżeli będziemy uczyć się po API.

Natomiast to będą minimalne środki i chyba trochę będzie po stronie serwerowych. Przygotujemy takie zestawienie, nie masz rady Grzegorz, mieliśmy to zaplanowane z Mateuszem, jak należy się uzbroić w sprzęt, natomiast zrobimy to w minimalnym możliwym stopniu, żeby nie stosować tutaj niepotrzebnej rozrzutności software'owej.

W porządku, dziękuję.

Też dopowiem, prawdopodobnie tam będziesz potrzebować kilku dolarów na open router, jakiś VPS, to pewnie za 20 złotych miesięcznie się uda.

Będą jakieś nowe koszty, to też w zależności od tego, jak będziesz później to wykorzystywać, prawda, ale każdy z tych narzędzi, które pokażemy, ma albo wersję darmową, każdy model, który pokażemy, albo ma wersję open source, albo jakąś tanią, tanie zastosowanie.

Ja w branży nie jestem zbyt długo, dopiero zaczynam praktycznie, tak, że tych Nie zrozumiem.

Inaczej.

Mam po prostu dostęp firmowy do czata GPT.

Tam też jest API, jak patrzyłem.

Więc tak może prościej.

Przepraszam, czy to wystarczy po prostu?

Dostęp do czata GPT to nie jest dostęp do API.

Okej, to wiem, że API też mam tam dostęp.

W OpenAI.

W czat GPT masz po prostu konto takie konsumenckie na przykład plus albo plus na przykład 20 dolarów.

a jak chcesz przez API się łączyć, to się łączysz już z modelami w Open... Tak, ale Grzegorz powiedział, że ma.

Grzegorz powiedział, że ma. A, ok.

Ok, dobra. Ok, dziękuję.

Przygotujemy też taką całą listę z oprogramowania.

No dobra, Paweł, czy wyczerpaliśmy temat Bipsika, czy jeszcze tutaj mamy coś, do czego wracamy, czy idziemy dalej? Lecimy.

Dalej. Lecimy.

Marek?

Ja bym w sumie jeszcze do Bipsika dodał kwestie na przykład promptowania, jeżeli chcemy tworzyć artykuły gdzieś tam w oparciu o tym EAT, no i tutaj jeżeli będziemy go promptować w ten sposób, żeby on dostarczał nam jakieś źródła, jak to robi Perflexity, to będzie wychodził z totalnymi bzdurami. Większość tych linków albo nie są tematyczne, a 90% są kompletnie zerwane, co są 404, więc to też warto chyba mieć gdzieś tam na uwadze, że on po prostu bazuje, mimo podpięcia do searcha, na czymś, co już nie jest aktualne.

tak, tak, ja jestem przekonany, że to nawet nie jest search, tylko to jest jakby jakieś historyczna wiedza, którą on posiada i wymyśla po prostu te adresy, no więc jakby ma kolejny argument czemu warto na niego uważać przynajmniej na dzień dzisiejszy natomiast jeśli chodzi o stajnie chińską, chciałem wam pokazać rozwiązaniem. Nie wiem z jakiej on po prostu wyszukiwarki korzysta, może z jakiegoś bajdu i po prostu nie ma polskich linków za bardzo w indeksie, nie?

no to też na pewno tak się dzieje natomiast chciałem Wam pokazać fajne narzędzie chińskie, jak już jesteśmy w tym regionie, nazywa się Manus i to jest agent który może do nas zrobić rzeczy, zdaje się też na jakąś tam przeglądarkę możecie też wejść na stronę Manus i M jest już dostępny dla wszystkich, ja wklejam teraz to na czar i jeśli chodzi o poszukanie rozwiązań w Chinach to na pewno Manus do tego może się sprawdzić. Możemy tutaj robić jakieś strony internetowe w tym manusie, aplikacje, inne historie. Proponuję zrobić taki tandem ćwiczeniowy na tym live'ie. Użyjmy sobie Dipsicka, napiszemy mu powiedzmy przygotuj mi specyfikację techniczną aplikacji do zadawania pytań live na czacie.

Dobra, aplikacje do łową aplikacji do dołowania pytań.

Powiedzmy coś takiego, Czyli stworzymy sobie jakąś tam prostą dokumentację w tygodniu dotyczącym programowania.

Będziemy o tym mówić więcej.

I teraz to wklejmy do Manusa.

Na końcu tego live'a zobaczymy sobie, co Manus nam wyprodukuje.

Zaraz będziemy musieli na niego chwileczkę poczekać.

Co on tutaj zrobił?

jakiś agent potrafi robić takie strony internetowe czy też aplikacje no ale poczekajmy na deepseek'a dobra uznajmy że tyle nam dobra tyle nam starczy to ja metodę deepseek'a deepseek'a tego manusa No i zobaczymy na końcu tego live'a, co nam wyprodukuje ten agent.

Jak widać, tutaj zaczął jakieś myślenie, tworzenie planu, no i już nam robi tą aplikację, więc na koniec sobie otworzymy i zobaczymy, co nam wyprodukował.

To jest udostępnione dwa dni temu dla wszystkich, więc możecie się tym pobawić.

to słuchajcie, czekamy na kolejne pytania a niech nam niech nam Manus pracuje w tle prejemy na czat jeszcze dwa projekty które są open source openmanus i camel one działają podobnie generalnie Manus to jest takie sprytne rozwiązanie on używa clode 3.7 jak naprawdę, i po prostu ma instancje przeglądarki i podobne narzędzia i dzięki temu jest po prostu w stanie wykonywać tę operację, ale tak naprawdę nie jest to nic super skomplikowanego, więc można to gdzieś też u siebie hostować.

Nim przejdziemy do pytania, bo widzę, że jest podniesiona łapka, zobaczcie, co tu się dzieje. Zaczyna jakby rozpisywać sobie zadania, pisze jakąś listę to do, następnie zaczyna wykonywać jakieś serwerze Ubuntu, widzimy tutaj jakieś tam operacje dyskowe, tworzy katalogi i jak widzimy zaczyna nam budować tą aplikację tutaj, zaczyna budować jakiś kod źródłowy, to jest dosyć ciekawe no i po krok po kroku będzie się tutaj coś dziać, ja już przestaję udostępnić, zobaczymy na samym końcu live, co nam wyprodukował.

Była łapka w górę, Grzegorz?

Cześć, to nie tyle może w kontekście pytania, co w kontekście Manusa, testowaliśmy go w takich luźniejszych powiedzmy zastosobaniach, czyli planowanie podróży dwa zupełnie różne dwie zupełnie różne opcje no i rzeczywiście po 20 minutach można dostać taki kompleksowy na przykład plan podróży zagranicznej z linkami do konkretnych ofert na przykład na bookingu w zależności od oczywiście prompna z powiedzmy wstępnie przygotowanymi planami lotów i jakimiś propozycjami no ja dostałem w swoim planie na przykład informacje, tabelę dopuszczalną zawartość alkoholu na przykład w poszczególnych krajach przez które przejeżdżam i tak dalej więc generalnie no fajnie to wygląda on tam ruchamia sobie te wirtualne przeglądarki oczywiście klika w linki jak to się w słysłowie, ale efekt jest bardzo, bardzo ciekawy no fajnie, fajnie ten proces wygląda, nawet warto sobie wypatrzeć co on po kolei robi to działa fajnie w pętli, ale robi wrażenie No i na koniec myślę, że po 80% dostarczył jakościowe informacje, tam w 80-90 latach.

Jeśli chcecie, to wrzucam też tutaj link na GitHub'a, gdzie są wszystkie narzędzia i prompty Manusa zlikowane.

Więc można podejrzeć, jak oni to robią.

Właśnie to jest to, Grzegorz to powiedziała, fajnie wygląda proces, bo tutaj nam rozrysował task progress i nam odklikuje, co on robi.

I teraz waliduje funkcje i teraz jakiś raport dostaniemy i po kolei idzie, więc pewnie fajne rzeczy można tutaj spożyć nawet w kontekście SEO, dać mu jakieś zadania, optymalizację strony, czy czegoś, ja tego jeszcze nie robiłem, ponieważ zarejestrowałem się tu dzisiaj, no i mamy pierwsze ćwiczenie, jest trochę kredytów za darmo, za resztę trzeba zapłacić, więc bawmy się tam.

Okej, czy mamy jakieś pytania, Damian?

O, pojawiło się nas Lido, pojawiło się nas Lido, Damian.

Dobrze, to odczytasz, Robert?

Jak dodać na LinkedIn informację, że bierze się udział w projekcie, jest jakiś szablon?

To nie do końca rozumiem. Chodzi o Sensei Akademii?

Jeśli tak, to...

Tak, chodzi o narzędzie, które po prostu pobiera zdjęcie profilowe z konta i ten banner stawia.

To Mateusz, poprosimy Mateusza, żeby zaraz podał sobie o link.

Muszę poszukać, bo to Justyna robiła.

Ale chyba Justyna też mamy na czacie, więc Justyna jak ma.

Tak, za chwilę podać tę instrukcję, także dajcie mi sekundę.

Pojawiło się pytanie tutaj od Pawła Cęga również na Slido.

Czy i jak często trzeba aktualizować biblioteki wykorzystywane do utworzonych aplikacji webowych w MAN-u?

Pytam o zabezpieczenie przed wirusami.

Wiesz, to jest bardzo dobre pytanie i pewnie to jest ten sposób, Paweł, że to akurat będzie w tym kolejnym tygodniu dotyczące programowania Najlepszą praktyką jest to, żeby w dowolnym narzędziu, czy to będzie Manus, czy Lowell, czy Bolt, czy każde inne narzędzie do tworzenia aplikacji, z którymi będziemy się bać w tym kursie, wykonać ćwiczeń za pierwsze zadanie w tym narzędziu, później zsynchronizować z GitHubem albo pobrać do siebie na komputer i często gęsto Daniel będzie wam pokazywał dokończenie tego programu na przykład w cursorze i tam się dopiero odbywa aktualizacja w ślad za na przykład, że dochodzi nowa wersja jakiejś biblioteki.

I nie umiem ci odpowiedzieć na pytanie, jak często w kontekście do manusa, natomiast tak bardziej logicznie, po prostu niech coś manus zrobi, później sobie to pobierz gdzieś i już aktualizuj konkretne aktualizacje czy wymagania gdzie indziej.

Bo ja nie wiem, w jakich wersjach on operuje.

Ale z reguły to raczej nie robi się tego często.

No raczej nie.

Okej.

O, mamy Grzegorza.

Jeszcze pytanie, jeśli chodzi o kursora GitHub, a jeżeli cursor działa miś pełni poprawnie, to GitHub mimo wszystko jest konieczny?

Czy to już jest wystarczające?

Jeszcze raz.

Jeśli cursor działa miś poprawnie, to GitHub także już jest jeszcze konieczne, żeby mieć oba te programy, czy cursor w zupełności wystarczy na potrzeby kursu?

No, GitHub to nie jest program, to jest repozytorium do przechowywania kodu, ale jeżeli programujesz z AI-em, to koniecznie potrzebujesz GitHub-a, bo AI co 5 minut średnio ci coś zepsuje, więc musisz deployować ten kod gdzieś, a jeżeli będziesz miał go na GitHub-ie, to możesz łatwo się do niego po prostu cofnąć zawsze.

Tak, tak, także Grzegorz, to jest tak w kontekście kursu, że GitHub jest za darmo i to jest tylko platforma, taka powiedzmy social media dla programistów, gdzie trzymamy nasze dane, no a cursor jest u ciebie na komputerze, podpinasz się do jednego źródła i tyle, nie?

Okej.

Jeżeli coś próbujesz programować z AI-em, to bez GitHub-a będą chłopoty, nie?

Więc bardzo polecam, żeby mieć jakieś repozytorym kodu.

Okej, dziękuję.

Ale to wiesz, to na razie ci tak odpowiedziałem bardzo ogólnie, no bo to jest szeroki temat, więc będzie do tego dedykowany tydzień, więc tam po prostu to ci się wszystko rozjaśni.

Teraz pokazałem po prostu ten GitHub, bo tam umieszczamy materiały z kursu i do tego na razie się ograniczyłem, ale to spokojnie jakby przez ten czas, który tutaj ten kurs mamy, to sobie to wszystko uporządkujesz w głowie.

Ok.

Jeszcze jakieś pytania w ramach czatu pojawiły się, to jak prosiłem, albo na slajdo prosimy, albo tutaj ręka do góry, bo czat bardziej do dyskusji, bo to po prostu ciężko śledzić te wszystkie miejsca, a nie chcemy pominąć czegoś. Ale widziałem, że Bartek zadał jakieś pytanie, to chyba był pierwszy i pytał, czy można zadawać pytania o nasze podejście do SEO, a nie do tygodnia pierwszego.

To odpowiadając Ci Bartku, jeżeli zostanie czas, a planujemy tu być maksymalnie 90 minut, to oczywiście możesz pytać nawet o to, jaka jest pogoda w Warszawie.

Nie do niczego się nie ogranicza.

Super, dzięki. Będę rzekał do 90 minuty.

Tak, nuliczamy czas gry.

Możemy jechać dalej.

Agnieszko, prosimy.

Ja jeszcze chciałam w kwestii tego cursora dopytać.

Jeżeli zaciągnę dane z Haba, całe te repozytorium, a wy coś nowego na Habe dodacie, to mi się automatycznie to pobierze do cursora, czy ja muszę odświeżać co jakiś czas?

Najlepiej jak raz w tygodniu, najlepiej jak raz w tygodniu, czyli po udostępnieniu materiałów, poprosisz, napiszesz w cursorze komendę, powierz mi najnowsze zmiany z GitHub'a.

Damian, wyświetlę ekran i pokaż, jak to się robi, bo to bardzo, Agnieszko, fantastyczne pytanie zadawać.

Dobra.

Powinien to Mateusz umieścić w tej lekcji początkowej, ale generalnie...

Ok, generalnie GitHub ma dostęp do twojego terminala, więc jak pobierzesz sobie tą aplikację GitHuba, to możesz po prostu mu napisać pobierz mi.

I to w zasadzie wystarczy. On tu sobie po prostu uruchomi terminal, ty będziesz tylko musiał wywołać tę komendę, czyli tu git pool origin main, czyli po prostu prosisz go o to, żeby z tej gałęzi, my zawsze na tej gałęzi main będziemy wszystko publikować i tylko run.

I tutaj po prostu już to wszystko się zaktualizuje.

I tyle. Raz w tygodniu, albo kiedy chcesz. Generalnie prawdopodobnie raz w tygodniu jest tylko sens, bo raz w tygodniu materiały się udostępniają.

Dzięki wielkie.

No dobra.

Można to gdzieś tam potem jeszcze dodać, bo podejrzewam, że nie ma tu wszystkich osób, a mogą być takie osoby, które nie będą tego wiedziały.

Chociażby klatkę.

Tak, to dodamy to.

Widzisz Agnieszka? Dlatego to było fantastyczne pytanie. Dzięki.

Jakub, miałeś rączkę w górę. Zapraszamy Jakuba.

Ale to pytanie bardziej ogólne, tak, więc nie związane z pierwszym tygodnią.

Okej.

Ale nie ma innych, więc chyba możemy...

Tak, nie ma innych. Jeżeli nie ma innych, to bardzo chętnie się wetnę.

No o co chodzi?

Mieliśmy ostatnio ten raport na temat e-commerce długość opisów kategorii, co nie?

W e-commerce.

I to skłania do zadania sobie pytanie, jak wysterować LLM, żeby pisał powiedzmy w jakimś tam przedziale długości treści. Ja wiem, że to jest bardzo trudne, łamane na niemożliwe, żeby być tak precyzyjnym, nie?

Bo widziałem, tam są rozwiązania, ludzie na przykład stosują fight tuning modeli, żeby one celowały w jakąś długość treści, ale to jest nieelastyczne.

I jak Wy, czy udaje Wam się pisać tak, żeby te gotowe treści były w jakimś tam przedziale długości i jak do tego podchodzicie w praktyce?

Damian, odpowiesz, bo ja jakby nie czytałem tego raportu, informacje w raporcie e-commerce.

A ja mogę odpowiedzieć, bo mam takiego tipa, którego stosuję od dwóch lat.

Może.

Jakby zadając mu na przykład, tworząc najpierw outline, możesz go poprosić, żeby outline był tak długi jak potrzeba i tak krótki jak to możliwe.

I jakby outline'em możesz wystarować ilość na przykład sekcji, które będą.

to jest jakby jeden tick, a drugi tick, bo on i tak nie potrafi tego szacować za bardzo.

Możesz mu przekazywać bardziej opisowo, czyli określić na przykład, że długość to jest krótki, to znaczy będzie miał, nie wiem, od 300 do 500 wyrazów albo od tam 100 do 300 wyrazów, ale jakby musisz to zrobić na tym pierwszym etapie, jak już generujesz outline, żeby ci ten outline nie odjechał, no a już potem jak generujesz treść, no to jakby promptami idzie nie sterować, no a druga opcja to już jest chyba tam o którym zresztą mówił podczas spotkania na live Damian. Także tyle ode mnie. Taki tip, że po prostu w prompcie sterować długością.

To wyrażenie. Tak długi jak to potrzebne i tak krótki jak to możliwe.

Także u mnie działa.

No, dziękuję.

Dziękuję. Roman jest na lotnisko, bo nagrywał dla Was kurs. Status na wczoraj 22.

Informacja, że nagrał 4 godziny.

na jeden tydzień, a dzisiaj nagrało jeszcze dodatkowe, więc spodziewamy się od chwil tego materiału. Damian, jakby rozwiń temat tych długości, bo tutaj mamy też troszeczkę to rozpracowane po naszej stronie.

To może po prostu powiem na początku tło do tego, co Kuba powiedział.

No więc w tym raporcie jest taka teza, że jakby opisy kategorii na sklepach internetowych mają pozytywny wpływ na to, że te kategorie są bardziej widoczne.

No i jest jakiś taki punkt statystyczny, w którym dłuższy tekst powoduje spadek iloczności i prawdopodobnie Google jakby zmienia intencje tej strony, bo jest zbyt dużo tekstu względem listingu produktów.

No i to jest taki punkt na poziomie tam chyba nie wiem, sześciu tysięcy znaków circa about, nie pamiętam już dokładnie.

No więc Kuba po prostu pyta jak osiągnąć te sześć tysięcy znaków, bo zazwyczaj osiąga pewnie dwadzieścia tysięcy, bo tak działa większość procesów.

Najczęściej Kuba po prostu my to sterujemy liczbą nagłówków, bo mniej więcej tutaj jesteśmy w stanie przewidzieć ile dla każdego nagłówka LLM wygeneruje tekstu, bo to jest dość przewidywalne i jesteśmy w stanie po prostu zarządzić liczbą nagłówków i to jest dość, to jest bardziej jakby tutaj LLM precyzyjne, więc jesteś w stanie tutaj ten proces tak przygotować lub właśnie fine tuning, bo jak go wytrenujesz na jakiejś określonej liczbie opisów kategorii, no to on potem będzie po prostu miał skłonność do tego, żeby one były bardzo podobne, więc też podobnej długości.

To ja follow up zrobię do tego pytania jeszcze, bo jak rozumieć ten przedział długości?

Bo czy ten przedział długości to jest jakiś, nie wiem, nazwijmy to sygnał rankingowy, w co wątpię?

Czy to raczej jest, tak się zdarzyło, że w tym przedziale długości jest najwięcej treści, które są po prostu dobre?

No bo w tym przedziale długości najłatwiej spełnić te wszystkie intencje, które powinny być w takiej treści.

I to dlatego w tym przedziale jest najlepiej powiedzmy, ale tu chodzi dalej o to, żeby informacje były kluczowe.

No wydaje mi się, że po prostu działa to tak, że im dłuższy tekst, tym prawdopodobnie więcej słów kluczowych tam umieściłeś, więc prawdopodobieństwo, że na jakieś słowa kluczowe się pojawisz jest wyższe, więc po prostu dłuższy tekst ma większą pojemność tak naprawdę, ale jeżeli on osiągnie jakąś konkretną długość, na przykład 12 tysięcy znaków, no to Google może po prostu przestać go traktować jako kategorię, bo tego tekstu jest zbyt długo względem produktów i on zaczyna po prostu zmieniać intencje tej konkretnej strony i to spada wtedy.

Ale nie ma tak, nie ma czegoś takiego, że jest jakaś uniwersalna najlepsza długość, po prostu statystycznie tak się po prostu wyszło w tym raporcie na bazie tam 90 tysięcy podstron, no ale to jest statystyka, tak?

jakby w praktyce to może być 2000 dobrze, ale może być też 6000 dobrze.

Czyli to nie jest tak, że to jest długość to czynnik rankingowy.

Nie. Myślę, że po prostu długość, widoczność jest skutkiem ubocznym długości.

W takim sensie, że jak masz długi tekst, to możesz na przykład opisać wiele cech danego produktu.

Na przykład masz stronę kategorii o rowerach gravel. To jak masz długi opis, to najczęściej tam umieścisz jakby dużo informacji, a w tych informacjach są jakieś słowa z długiego ogona, które mogą dać Ci dość dalekę pozycję na większą liczbę słów niż krótszy tekst.

Więc to jest po prostu kriaków boczny, a nie czynnik rankingowy.

Okej. Mam jeszcze dwa pytania.

Jeżeli nie ma więcej pytań, to proszę wykorzystać, jeśli mogę.

Wiesz co, to dawaj jedno pytanie, później Karol, bo podniosł rączkę.

Wróć się wniósł do Ciebie, Jakub, dobra?

Ja wiem, że, bo jak oglądam inne Wasze webinary na przykład, że teraz na jednym słyszałem, że w tym procesie generowania treści, które w poprzednim pierwszej kohorcie omawialiśmy, budowaliśmy, zamiast tych trzech etapów, czyli tam generowanie perpleksji, humanizacja, formatowanie, to macie wytrenowany model i żeby to robił naraz.

I jak pozyskać takie dane do trenowania modelu, jak to przygotować?

Wiem, że to pewnie będzie jeszcze poruszane, ale tak wstępnie czy ogólnie, czy dobra koncepcja to jest.

Żeby nam się osoby, które są tu pierwszy raz nie pogubiły, to po prostu zaznaczam, że mówimy o tematach, które są spoza tygodnia pierwszego.

I to po prostu powiedzmy mamy teraz taki free chat.

Jeżeli jakieś są pytania z tygodnia pierwszego, to oczywiście odpowiem.

Kuba, po prostu chodzi generalnie o to, że LLM-y mają bardzo dużą tendencję do tego, żeby nadużywać jakichś konkretnych zwrotów.

Na przykład na Reddicie, jak widzisz podwójny myślnik, to często jest to po prostu LLM wygenerował ten tekst, bo mają tendencję do tego, żeby go nadużywać, bo były trenowane na artykułach naukowych, a tam się często takie znaki stosuje.

Tak samo jest w języku. Na przykład LLM tysiąc razy częściej niż człowiek powie jakieś wyrażenie w danym, na przykład używając określonych dwóch słów.

no i te wszystkie narzędzia, które służą do tego, żeby sprawdzać, czy tekst został wygenerowany przez AI, one w różny sposób działają, ale takie najskuteczniejsze, które istnieje na rynku, czyli CopyLeaks, to on po prostu ma dataset artykułów napisanych przez człowieka, dataset artykułów wygenerowanych i sprawdza te wszystkie pojęcia, nawet ci wylicza, ile razy taki tekst pojawia się w tekstach AI, a ile w ludziach, w tekstach napisanych przez człowieka, no i to porównaje do twojego tekstu.

No i teraz możesz po prostu zaktualizować przez fine tuning wagi modelu w taki sposób, żeby on nie nadużywał tych pojęć, tylko nadużywał takich pojęć, które dać mu w danych treningowych. Więc żeby do tego doprowadzić, to musisz wziąć jakiś model i go wytrenować.

Ten, o którym ja mówiłem, my trenowaliśmy model Gemma 3.27b.

Żeby go wytrenować, potrzebujesz jakiegoś serwera z GPU.

Możesz to na przykład wynająć na jakieś 12 godzin sobie jakąś mocną maszynę na karcie graficznej H100 od nvd.

To będzie ci kosztowało jakieś 6 dolarów za godzinę.

No i po prostu sobie trenujesz ten model, już nie wchodząc w szczegóły, jak to dokładnie zrobić. Ja to zrobiłem po prostu tak, że wziąłem sobie próbkę 50 tysięcy tekstów napisanych przez człowieka z Hugging Face'a, tam można zadać takie data sety i każdy tekst napisany przez człowieka poprosiłem, żeby napisał AI, czyli miałem prompt, weź ten tekst i przepisz go zachowując sens, ale używając innych słów.

Więc ten tekst, który już AI przepisał z ludzkiego na AI-owy, to już był wykrywany jako ten AI-owy, no i później jak w fine-tuningujesz model, to dajesz mu po prostu te 50 tysięcy tekstów w 50 tysięcy promptów, a ten prompt ma na celu tak, przepisz ten tekst i tu jest tekst, który AI mi przed chwilą wygenerował na taki styl i tu jest tekst, który napisany został przez człowieka i dostaje on takich 50 tysięcy próbek, no i później jak pisze tekst, to tam przepisuje tekst, który ja sobie wygenerowałem LLM-em w tym powiedzmy procesie, który był, to jak go przepisuje, to on już bazuje na tych 50 tysięcy tekstów, więc nie używa tego słownictwa po prostu.

Tylko to jest taki już dość zaawansowany temat.

Jest tam dużo takich zagadnień, które należy rozważyć.

Dużo czasu trzeba poświęcić na to, żeby te dane wyczyścić, ale to sobie jeszcze o tym będziemy rozmawiać w kolejnych etapach.

Nie chcę zbyt długo o tym mówić, bo pewnie dla większości osób to jeszcze nie jest temat zainteresowania, ale to będziemy sobie to rozgryzać.

Dobra, to możemy chyba, Robert, lecieć dalej.

Tak, ja szybko zbawiam tutaj Dawid.

Dawid Kotowicz na czacie zapytał.

Z tego to wiem, jeden tydzień poświęcał automatyzacji i blogowaniu. Czy dzięki konfereniu dowiemy się, jak uruchomiliśmy na razie Dadoze, dostaniemy gotowy workflow na Make lub na Siemen?

Tak, to będzie w moim tygodniu i tak dostaniecie wszystkie gotowe workflow, także spokojnie.

wszystko będzie. I wydaje się, że Karol miał podniesioną łapkę jako pierwsza osoba, więc bardzo proszę Karola.

Jasne, ja chciałbym dorzucić kilka groszy, słychać mnie w ogóle?

O, słychać. Tak, doskonale.

Chciałbym dorzucić kilka groszy do pytania Jakuba, właściwie w kierunku odpowiedzi mu. Pamiętam prawdowne czasy, kiedy Rand Fiszkin pracował jeszcze w MOZ-ie, to był 2015 rok, kiedy zrobił takie badanie wśród blogerów, jakiej najlepiej rankuje u nich w Google.

I wtedy wodał magiczną cyfrę 2460 znaków i wszyscy zaczęli w różnych agencjach pisać na 2500 znaków teksty.

Ja myślę, że nie należy trzymać się mocno...

A nie słuch? Ja też kojarzę to właśnie.

Ale dobra, już się spójrzmy, przepraszam.

Dobra, spoko. To już ostatnia myśl.

Ja myślę, że nie należy przewiązywać się do długości tekstu sztywno z takiego powodu, że LLM może mieć ten sam problem, który będzie miał ludzki copywriter, czyli zależnie od tego, jaka jest tematyka tego konkretnego sklepu, na jeden temat da się napisać dużo znaków, na inny niestety mało.

Poza tym zależnie też od tego, jak jest zbudowane drzewko kategorii.

Jeśli będziemy pisać zbyt długi tekst na daną kategorię główną, zaczniemy sobie robić kanibalizację.

Więc raczej nie ma tutaj moim zdaniem uniwersalnej odpowiedzi i trzeba podchodzić do tego tematu dość delikatnie i płynnie, zależnie od tego, w jakim sklepie się siedzi.

Tak, zgadzam się, aczkolwiek pewnie zbyt długie teksty mogą robić problemy.

I to jest trochę tak właśnie, że też ciężko sterować LLM, bo on ma tendencję do tego, żeby jednak bardzo szczegółowo wszystko opisywać, podróżnienie od copywritera, który może używać różnych skrótów myślowych, ale co do zasady z Karolem się oczywiście zgadzam, tutaj jest racja, nie ma sensu się przywiązywać do tej magicznej liczby.

Pamiętaj Kuba, że to jest tylko statystyka i statystycznie tak wychodzi, ale w praktyce to każdy przypadek może nieco inny być.

Natomiast co do zasady wolałbym, żeby te teksty nie były ani zbyt krótkie, ani zbyt długie.

Więc gdzieś w przedziale jakimś dużym, ale nie umieszczałbym tego przedziału w zakresie 5-6 tysięcy, tylko raczej 2 tysiące, 8 tysięcy.

W przypadku pisów kaczynsza.

I tak tych tekstów nie czytasz.

Jak to się Dawian mówi, tak jak tort na weselu.

Nie za słodki, nie za gorzki, taki sam raz.

słuchajcie Państwo, nim przejdziemy dalej z pytaniami, mamy tą matrykację napisaną przez Manusa, no nie powala mnie ona ale to do zasady funkcjonalności które miała spełnić, ma spełnione czyli jak piszemy hej, kolejny raz albo pytanie to przed chwilą to działało są te pytania więc odkłada nam te pytania, więc potencjalnie moglibyśmy zrealizować sobie zrealizować sobie tej aplikacji to zostało wygenerowane.

Teraz trwa jakieś tam, pokażę to jeszcze w manusie, udostępnianie tej aplikacji na jakimś serwerze. Coś tam się teraz jeszcze dzieje w tym manusie, więc cel jakim było, o, jest aplikacja na domenie, zaraz wam ją udostępnię.

Publicity Visit jest udostępniona.

Ta aplikacja zaraz ją udostępnię na czacie.

Możecie zobaczyć, jak to wyszło.

No i dalej możemy w Manusie czatować z naszą aplikacją, kazać mu ją rozwijać, dodawać nowe funkcjonalności.

Tyle w temacie Manusa. Zamykamy.

Jest udostępniony na czacie aplikacja stworzona i pytanie od Grzegorza.

Jeśli chodzi o link, to kiedy warto utapować usługi z linkami nowowolą, tak? Jeżeli mam przykładowo do obliczenia artykułu ze znanego radia jakikolwiek i tylko, że ma tam umieszczony link nowowolą do obliczenia, czy warto takie coś brać i kiedy nie, warto z aplikników przestać. Wiadomo, że to follow jest lepsze.

Chyba nie zrozumiałem do końca pytanie, Damian.

Coś przerywałem.

Coś przerywałem, tak? Właśnie coś to przerywało.

Mniej więcej sens zrozumiałem, tak?

Czy kiedy jest sens kupować link no follow, tak?

Tak, tak.

To może zostawmy to pytanie na koniec, bo to już tak zupełnie odlecieliśmy od tematu.

ale oczywiście odpowiemy, tylko pewnie na sam koniec, dobra?

Tymczasem okazuje się, że aplikacja ma usowa do zadawania pytań, Damian pełni taką samą funkcję jak z Lido i działa.

Ale nie można w górę zostawiać jeszcze.

To muszę mu napisać, żeby dorobił.

Roman Rosenberger, łapka w górze.

To tam, Roman.

No ja chciałem o tym linku tylko powiedzieć, ale powiedzieliście, że na koniec już pewnie będę leciał, ale ktoś to zbadał jakby co to pytania na Discord jak najbardziej.

Że no.

Pa, pa.

Dobra.

Jest kilka pytań na slajdo, Robert.

Czy Google nie preferuje stronę z bardziej obszerną treścią?

Może wynikać z chęci...

Od 18.48 nie odpowiedzieliśmy.

No to Damian przejdzie tutaj, bo...

Szymajcie, tutaj były rodzajki narzędzi, które ułatwiają życie w nawiązaniu do lekcji Damiana z Fireflies i Amy, bo to chyba lekcja z poprzedniej edycji.

Czekaj, zajrzę sobie tylko mojego doka w Macu.

Takie narzędzie, które często teraz wykorzystuję, to jest Super Whisper.

To jest takie narzędzie do komunikacji głosowej z komputerem, bo ma po prostu model Whispera i jest w stanie z głosu przetworzyć tekst, ale to co to narzędzie daje, to można prompt template dla każdej aplikacji zrobić.

To znaczy na przykład, że jeżeli uruchomię tę notatkę głosową w Obsidianie i mam prompta napisanego do Obsidiana, to on mi od razu na przykład zrobi formatkę, informatkę tylko notatkę w formie do Obsidiana. Jak mam na przykład maila i podyktuję maila i mam prompt template do maila, to mnie od razu przetworzy na maila, tak jak bym chciał.

Więc to jest super. To jest takich jeszcze powiedzmy klientów mailowych, które wykorzystuję do AI-a, to jest Sparky Mail.

Ja już korzystam kilka lat, ale oni dodali tam super funkcję AI.

Zamiennie też można używać Superhuman. Widzę, że dużo osób korzysta i sobie chwali.

I z takich aplikacji jeszcze to ostatnio zacząłem sobie dużo agentów tworzyć w Meiku po prostu, żeby nie musieć z aplikacji korzystać zupełnie.

Więc na przykład jakieś automatyzacje, które mi codziennie wysyłają jakiś plan dnia, wyciągając moje eventy z kalendarza, z Notion i tak dalej i model reasoningowy na przykład mi wybiera zadania, które on dzisiaj zrealizować, w jakiej kolejności i tak dalej.

Więc co do zasady chyba Im mniej aplikacji, tym lepiej, więc to wszystko, co potrzebujesz, to kalendarz, e-mail, jakieś właśnie narzędzie typu Make i ono się men, w którym sobie stworzysz agentów, którzy będą ci większość rzeczy robić i jakiś na przykład IDE, czyli cursor.

Ja się więcej do tego ograniczam i ze wszystkimi modelami raczej się przez API komunikuję, no i czasem zamiennie chat GPT i jakiś cloud.

Ja w ramach rozszerzenia doceniam bardzo funkcjonalności Love Bull i funkcjonalności V0, ale to będziemy mieli osobny tydzień o tym, więc nie będę o tym zaraz mówić.

Tylko wkleję zaraz linki na czatach.

Mój największy zachwyt ostatnimi czasy był nad notebookiem LM, ale mamy o tym lekcję w pierwszym tygodniu, więc też nie będziemy w co wchodzić.

Fantastyczne narzędzie i polecam wszystkim.

W stosunku do lekcji nagranej jest aktualizacja, którą umieściłem na GitHubie.

W momencie nagrywania lekcji działało to tylko w języku angielskim.

Od zeszłego tygodnia działa to również w języku polskim.

Po prawej stronie to się wybiera na samej górze, jest opcja ustawień, tam jest opcja wybór języków i jest pięciut języków, tym język polski, więc tyle w stosunku do nagrywanej lekcji dwa tygodnie temu się zmieniło w notebooku, ale polecam nam.

Damian, czy są jeszcze jakieś pytania na slido?

Jakby zapanowałeś nad tym, to pójdźmy dalej.

Jeszcze spięć.

No lećmy.

Jest pytanie od Dawida. Z tego co wiem, jeden tydzień jest poświęcony automatycznemu blogowaniu. Czy dzięki kursom dowiemy się, jak uruchomić tę funkcjonalność od A do Z?

Tak, to już odpowiedziałem tak. Oczywiście w moim tygodniu wszystko będzie przedstawione.

Kolejne pytanie jest, czy Google nie preferuje stron z bardziej obszerną treścią? Może wynikać z chęci optymalizacji kosztów, czyli odrzucania ich w imię oszczędności?

No to też będzie na to trudny tydzień oczywiście, ale odpowiadając w tym momencie, oczywiście, że tak, Google preferuje treści wypełnione maksymalnie bogato wiedzą i tym, czego szukają ludzie, kontekstem, odpowiedziami na pytania i wyczerpaniem tematu, to jest oczywiste, pełnymi strukturami, bo lepiej Google'owi po prostu jest przeszukiwać 10 zaufanych stron, które mają pełną strukturę i znajdą tam wszystko, niż jakieś połowiczne strony, gdzie trochę tego, trochę tego, trochę niczego.

tutaj będziemy mieli kontekst topical authority, który również w tym kursie sobie omówimy dosyć bardzo szczegółowo.

I tak, na koniec dnia właśnie chodzi o to, co było w tym pytaniu, o obniżenie kosztów, bo lepiej jest przetwarzać pełny content i strony lepiej optymalizowane z większą ilością informacji niż 20 stron połowicznie i dokonywać ekstraktizm 20 stron, które można z jednej pełnej, prawda?

Ograniczenie kosztów oczywiście to jest kolejne wyzwanie i masz oczywiście rację w tym pytaniu.

No bo zobacz, teraz mamy kontekst inflacji treści w internecie, nadprodukcji treści, no to Google nie jest w stanie wydać pieniędzy na przetworzenie każdej jednostki treści.

Będzie na to osobne pytanie, nie będę wchodził w głębie, ale oczywiście w tym pytaniu, to co napisałeś, to tak, masz rację, jeśli chodzi o obniżenie kosztów.

Kolejne pytanie jest, czy replit.com w ramach pytania narzędzia AI, czy replit to narzędzie, którym warto się pochylić?

Wiesz co, Replit tak, bo daje od razu też hostowanie tym narzędziom, ale wydaje mi się, że Cursor czy Lovable będą tu lepsze.

Replit się chyba promuje jako takie bardziej narzędzie dostępne dla osób, które nie chcą w ogóle poznać świata technologii, bo nawet aplikację mobilną ostatnio wypuścili do generowania aplikacji.

Ja osobiście nie korzystam, ale jest to narzędzie, które pewnie na liście narzędzi niektórych osób jest, natomiast w większości przypadków to ludzie tam wchodzą raz i już nie wchodzą z powrotem.

Z tego co zauważyłem, nawet dlatego mają taką promocję na rok z góry, chyba jest 10 razy taniej niż za miesiąc.

Dobra, dalej mamy: czy za pomocą AI można stworzyć rozwiązanie dla małego e-commerce'u typu pop-up, zostaw swój numer a oddzwonimy za 5 minut?

No oczywiście, że tak. Damian udostępnij jak krok, pokażę jak to zrobić jakiś pop-up, czy prosty na przykład w K2-sach albo w Klode?

To może nie wyprzedzajmy faktów, bo pewnie pewnie pewnie lepiej to w tygodniu dziewiątym będzie zrobić, bo to pewnie większy temat niż tylko pop up, bo to chyba gdzieś też musi wysyłać te informacje i może uruchomiać jakiś numer telefonu, ale oczywiście tak to jest bardzo proste zadanie.

Jak poznasz to narzędzie to zrobisz to w 30 minut.

30 minut zrobisz, 5 dni będziesz poprawiać.

Dobra.

Make kontra DeFi, różnice podobieństwa zastosowanie.

Będzie od tym oddzielna lekcja, więc nie wiem, czy chcemy wyprzedzać fakty.

Ale tak szybko odpowiadając w jednym zdaniu, Make ma szersze zastosowania.

Da się tam wszystko zautomatyzować.

DeFi jest mocno ograniczony do współpracy z modelami językowymi, ale współpracy z modelami językowymi jest dużo lepszy niż Make.

Tak ogółem dużym.

Pytanie od Pawła, czy powinienem przesiąść się na Gemini 2.5, jeśli jestem przyzwyczajony do OpenAI. Dlaczego tak lub nie?

Moim zdaniem tak, ponieważ Gemini 2.5, zwłaszcza Flash, jest istotnie tańszy. Milion tokenów we Flashu kosztuje 15 centów.

Więc to jest jeden z powodów.

Gemini 2.5 Pro jest znacznie lepszy od doprogramowania niż modele OpenAI chociażby.

Więc pod tym kątem na pewno tak.

No i to, Paweł, jeszcze taka ogólna uwaga.

nie warto się przyzwyczajać do modeli językowych.

Powinieneś stworzyć sobie takie środowisko i taki mindset, że jesteś w stanie się przyłączyć między modelami pięć minut i tak też będziemy tutaj uczyć, żeby model nas nie ogranicza i nie przyzwyczajał, bo tak jak pewnie obserwujesz ten rynek, te modele się wyprzedzają co tydzień, a to jeden, a to drugi. Nie wiem, Robert, czy chcesz tu coś dodać?

W kontekście API chyba wyczerpałeś temat, bo te modele faktycznie są tańsze.

W kontekście użytkowania tego w Gemini jako interfejs chatowy, jeśli jesteś się użytkownikami Google Work Spray czy Google Future, jakkolwiek to się obecnie nazywa, czyli tego konta premium biznesowego Google, macie opty advance w tym pakiecie, więc też mocny argument robię sobie tego poużywać, no bo jest w pakiecie, prawda?

I ja również większość swoich procesów staram się przepinać na modele Google'owe, chociaż ostatnio korzystałem w jakimś tam kontekście tworzenia jakiejś tam estymacji z z z ViPu Google'owego, znacznie lepiej poradziły sobie modele OpenAI z tym zadanie, więc jakby to jak Damian mówi, no nie przyzwyczajajmy się, jeden poradził sobie lepiej z jednym, jeden z drugim zadaniem, no i musiał mieć taki mindset, żeby umieć przeskakiwać.

Tak, chociaż do pisania tekstów akurat modele OpenAI są dobre, więc jeżeli do tego tylko używasz modeli OpenAI, to prawdopodobnie będziesz mógł tam zostać.

Natomiast no widzisz, tu pewnie nie istnieje jeden dobry model do wszystkiego, więc warto jakby być tutaj elastycznym, ale są do tego nasze dział, więc okej.

Pytanie, jak będzie czas? Czy w ramach Westigio na przykład na Dr. Max umieściliście stopki w stopce listing kategorii produktów? Daje to lepsze pozycje, czy tylko indeksowanie? To do Ciebie, Robert, pytanie.

Jeszcze przeczytam, bo to jest takie ciekawe pytanie, Czy te linki w stopce w Dr. Max do produktów i kategorii one zwiększają pozycję, czy poprawiają indeksowanie?

Problem polega na tym, że muszę się do tego pytania przygotować, bo ja nie jestem kompletnie w tym projekcie.

Bo czy nawet nie wiem, co tak się dzieje na tym projekcie, mówię szczerze.

Zaraz sobie przyjrzę się temu i odpowiem. Dobre, damy ją tak, że przejdź do następnego pytania, ja przychodzę na ten projekt, Zobaczy, zwizualizuje sobie problemy.

To jest na produktach, tak?

Co jest na produktach?

I wiesz co?

Ja sobie sprawdzę co jest na tym projekcie po prostu i zaraz odpowiem na to pytanie.

Dobrze.

To jest o Apple Integens. Czy mamy doświadczenie, czy warto używać?

Nie mam doświadczenia, ale to jest kawał gówna.

Z tego co wiem.

Niestety jeszcze to nie działa jak powinno.

AI Overview Google.

Które serwisy powinny walczyć o widoczność?

A może która intencja wyszukiwania? Tylko informacyjna?

No nie tylko informacyjna, bo te AI Overviews pojawiają się też na inne intencje i pewnie za chwilę będą AI Overviews dla produktów chociażby nawet.

Jeżeli masz jakieś na przykład pytania do produktów i pewnie tam też będą reklamy za chwilę.

więc za chwilę wszyscy o to będą walczyć tak naprawdę.

Nie mam jeszcze statystyk na ile procent fras jest AI Overview, ale w Polsce wydaje się, że to jest bardzo, bardzo dużo.

Krzysiu, czy możesz to powiedzieć, bo ostatnio mieliśmy ciekawą rozmowę na temat i miałeś ciekawe wnioski w tym temacie.

Znaczy z tym AI Overview, tak jak mówiłem, że zaczyna się tego coraz więcej pojawiać i sobie jak sobie testowałem to po prostu zaczynają wchodzić już frazy takie zupełnie nie typu jak, gdzie i kiedy.

no tak jak było na live'ie, no to zaczęły mi się pojawiać prawy, które gdzieś tam sprzeczne były poprawiane i no dużo, dużo tego będzie.

Widać, że oni też testują, plus też jest taki, że jakby no coraz większy, znaczy wiesz, mamy te nowsze modele, z tego co widziałem w porównaniu do tego Google'a, znaczy AI US, dużo szybciej nam wchodzą, wiesz, newsy i cały czas to się po prostu rozwija.

Nic więcej tutaj się już chyba nie da, że tak powiem, wyciągać.

Odpowiadając na pytanie w kontekście dr. Max, już sobie wizualizowałem, czy to tam się dzieje na tym projekcie i chyba nie chce się do końca wysprzęglić ze strategii, co tam się dzieje, natomiast jakbyśmy się temu przyjrzeli, albo o stopach trafytem, to jeżeli mamy opalanie kremy FF do twarzy, to pewnie możemy sobie wymyślić, że dochodzi jakiś trend, który nadchodzi albo już się pojawił.

Jeśli chodzi o ten pierwszy produkt, kategorię mleko dla niemowląt, to powiedzmy w ten sposób.

Aktywnie pracujemy nad dodawaniem nowych kategorii, więc jakby to będzie odpowiedź na to pytanie.

Jakbyśmy spojrzeli na probiotyki i magnezy, to możemy sobie spojrzeć na wolumeny tych fraz, więc jakby odpowiedzi, do czego to jest zrobione, jest kilka.

Pierwsza sprawa trendy, opalanie.

Druga sprawa nowości, czyli mleko dla niemowląt.

Trzecia sprawa powiedzmy duże wolumeny.

Jeśli chodzi o produkty, to nie mam bladego pojęcia, ale no dobra.

Powiedzmy, że podobna strategia.

Realizowałem taki projekt też i jakiegoś ogromnego boosta pozycji to nie daje, ale poprawia nas ta indeksacja.

Właśnie miałem wspomnieć, że to indeksacja, nie?

Tak.

Poza tym przy tak dużych e-commerce'ach to chyba ma to kilka znaczeń, nie tylko jedno.

Jest pytanie Bardzo spadają CTR-y po wdrożeniu AI badacie?

Tak, badacie i realnie na projektach dużych w Polsce są to cyfry mniej więcej 30-35% na podstawie polskich danych i to bardzo koreluje do tych wartości, który zaprocentował Ahre w swoim badaniu.

Oni tam chyba powiedzieli o wartości 37%, więc faktycznie to są bardzo zbieżne wartości, które obserwujemy na podstawie Search Console.

Masz wpis Kevina Indiga, on przeanalizował 18 analiz AI Overview i spisał takie meta-wnioski.

Mhm.

No to my w cyfraźnie widzimy 35%, 30-35%.

Liczba.

Okej.

Nie ma więcej pytań.

O, tutaj ktoś odpisał właśnie 30-40 problem zero klików, no to właśnie to jest to.

Czy polecam walediwy?

No, fajnie.

Bardzo fajne miejsce.

Co prawda długa podróż, nie jest za gorąca, 30 stopni, 30 stopni ocean i 30 stopni basen, więc nie ma innego miejsca niż 30 stopni poza klimatyzacją w pokoju, ale taki inny świat, zupełnie jakby się na innej planecie wyglądował.

Także polecam. Pewnie nie co roku, ale raz na 15 lat to taka wycieczka jest fajna.

Tutaj pojawiu się zapytanie, jakie narzędzie polecacie do badania obecności strony Wyszyki Wargrafa Jaita.

chat ITP, polski rynek.

Z polskiego rynku narzędzie nazywa się chatbit.

Tym narzędzie zdaje się od brand24.

Kiedyś chcieliśmy pokazywać to narzędzie i faktycznie umie rozpoznawać obecności powiedzmy w LLMach, jak to było to zapytane, czyli w chatbotach.

I jeśli wydaje mi się, Damian, że również w niektórych narzędziach do skrapowania danych pojawiły się możliwości powiedzmy skrapowania, czyli ściągania danych przez API z chatu, Nie wiem, czy to nie było w Breakday, tak kojarzysz?

Wiesz, to gdzieś to było, na pewno.

Wiesz, to wiedziałem.

A powiem wam, że też wdrażamy takie narzędzie, powinno za jakieś 7 tygodni być.

Będzie Perplex City, ChatGPT, Cloder i DeepSync, jak na razie chyba.

I Copilot.

O, no to to będzie też dodatkowa rzecz.

Czy są jeszcze jakieś pytania?

A było pytanie od...

Paweł podnosi rękę, Ale jeszcze pamiętam, że mamy od Bartka i od Grzegorza pytanie z innej kategorii.

Ale już teraz rozmawiamy o AI Overview, więc możemy powoli przychodzić do tych tematów innych.

Dobra, bo Paweł podniasi rękę, ale nie odzywa się, to nie wiem.

Można?

Tak, tak, tak.

Szybkie pytanie do Ciebie, Damian, bo poruszyłeś temat jeszcze humanizacji i PyTuning. Pytanie jest takie, czy w tym kursie będziemy robić coś takiego zaawansowanego właśnie?

Akurat usunęliśmy PyTuning z tego kursu, ale na pewno Mateusz może udostępnić lekcje z poprzedniego kursu, gdzie PyTuning był omawiany, a jeżeli chodzi o ten, który teraz robiłem, to może na koniec po prostu jakąś tam dodatkową lekcję albo spiszę, albo na githuba jakieś moje takie podstawowe wnioski.

Może byś był ktoś zainteresowany, można by taką opcję po prostu lekcji osobnej zrobić, jeżeli byłoby coś takiego, bo nie ukrywam, że tutaj temat jest dla mnie dość istotny.

Tak, ale to pewnie w kolejnych tygodniach, bo nie chcę wyprzedać faktów, bo...

Jasne, jasne. Dobra, dzięki.

Dopiero wchodzą w świat AI i dużo, które już są dużo dalej, więc żeby zapanować nad tym wszystkim, to musimy dać tym osobom czas, żeby trochę nadgoniły, bo jak zaczniemy zarzucać nowymi pojęciami i rzeczami, to mogą się nam zdemotywować.

Dobra, tutaj Pajrioś, Maslido, wszystkie pytania i udzielę na nie szczytko odpowiedzi.

Czy są jakieś warte uwagi modele poluzowaną cenzurą, które generują treści, na przykład erotyka na czadle ci, ja takie rzeczy robiłem w erotyce, są to modele od Mistrala, one mają poluzowaną cenzurę.

Wszystkie modele open source.

Tak, tak, ale te, znaczy nie do końca wszystkie, ale te są open source'owe i te wyjątkowo mają poluzowaną cenzurę.

Dobra, to może przejdźmy do Bartka. Bartku.

Wiecie co, ja mam takie pytanie, bo ostatnio dość dużo rozmawiałem z osobą, która w Stanach robi SEO i działa na tamtym rynku bezpośrednio.

I właśnie to mnie dość mocno zdziwiło, bo wy macie podejście budowania grafu wiedzy na bazie wektorów, na bazie budowania RACa.

i to jest jakby zbieranie faktów, które są w ramach jakiejś tam jednej struktury.

No i ta osoba mi powiedziała dość długo, siedzi w SEO, że trochę się to zmienia, szczególnie w Stanach i tam bardzo mocno zaczynają wchodzić w Reddita, jakby dając taką tezę, że YouTube, zbieranie danych z YouTube jest trochę już B.

I właśnie skrapowanie budują agentów po to, żeby skrapować dane z Reddita.

Nie wiem, czy słyszeliście, jakiś czas temu była informacja, że Reddit jest bardzo mocno tam, testują użytkowników, budując agentów, którzy losowo wrzucają zapytania po to, żeby wywoływać różnego rodzaju burzę.

I właśnie powiedział mi coś takiego, nie wiem jeszcze jak to interpretować, bo mi się to gdzieś tam w głowie dopiero mierzy, że komunikując się z VC, którzy zarządzają CMO w większych firm w Stanach, jakby nie chcą agencji SEO, jakby to nie jest ich target.

Oni poszukują teraz zamiennika, czegoś, co będzie w stanie dostarczać im dane, używając danych z marketingu, który był przed SEO, czyli przed keywordsami i przed budowaniem topical authority, tylko tym, co jest naprawdę ważne.

Jeżeli szukacie iPada dla dziecka, to dla Was jest ważne, czy jest kontrola bezpieczeństwa dla rodzica, czy są jakieś dwie, trzy dodatkowe rzeczy, a nie specyfikacji i właśnie 8 tysięcy znaków ze specjalami, to co mówił Jakub.

I pytanie, czy Wy mieliście właśnie jakieś podejście na ten temat, albo co Wy o tym sądzicie, bo w sumie Reddit w Europie jest taki dość wyłączony, w sensie, nie wiem, wydaje mi się, że mało osób tutaj korzysta z Reddita i czy to może być jakiś kierunek do budowania grafów wiedzy na autentycznych relacjach ludzi, którzy piszą w jakiejś tematyce?

Nie wiem, może Wykop byłby takim miejscem.

Nie wiem, na ile Wykop jest autentycznym miejscem.

To pewnie trudne pytanie, żeby powiedzieć, bo pewnie trzeba by było brać udział w tej rozmowie, którą odbyłeś, ale wydaje mi się, że to jest mocno powiązane z tym, że po prostu bardzo Google promuje w wynikach wyszkiwania Reddita.

Jak sobie zobaczymy, zaraz ci powiem mniej więcej, na ilu procentach wyników wyszkiwania w Stanach pojawia się Reddit, ale to jest kilka, no zaraz to znajdę.

Już to.

I teraz masz reddita na forums.

No 7% wyników uszukiwania w tym momencie, więc może w tym kontekście on po prostu stara się zakładać wątki na reddicie, żeby pojawiać, no i quora też się pojawiać wyników uszukiwania, bo nie wiem o co może wchodzić więcej.

Wiesz co, no ja to śmiałe tezy, że właśnie budowanie grafów wiedzy i takie podejście odskulowe do SEO umiera, w sensie, że jakby wiadomo, nie chcę używać tego słowa, bo ono już wielokrotnie zostało użyte, ale, że jakby takie podejście, że znowu SEO będzie wracać nie do budowania wiedzy o wszystkim, tylko o tych personalizacjach, chyba ostatnio mówiliście o personalizacji, czyli taka treść zostanie Ci dostarczona, jaką personalizację masz do swojego profilu, tylko nie wiem, czy to będzie z Twojej czy to ty musisz dopasować tą treść pod użytkownikami?

Myślę, że to jest dosyć...

Pytanie klasy filozoficznej, trochę pytanie klasy albo zadanie porównywania owoców do owoców morza.

Przede wszystkim w Polsce nie ma reddita, więc w Polsce oferujemy na polskim bagienku.

Natomiast jeżeli by się pojawił reddit, wykop, check it, orz, jak powiedzmy coś, co jest user generated content, to co do zasad rządzących information retrieval, które stosuje Google, to to, czy ściągniemy dane z Reddita, czy z wykopu, czy z forum optymalizacji, czy z grupy z WSW Polska, to są ciągle informacje, ciąg znaków ze spacją, nieważne z kątemu pochodzi, on może pochodzić z encyklopedii, a może pochodzić z Reddita. Operacje, które wykonuje Google są takie same zawsze.

Ekstrakcje do grafów wiedzy, do grafów informacji, do innych struktur, które da się porównywać i dalej przetwarzać.

To co ty powiedziałaś, to są jakieś takie spersonalizowane wyniki. No to zobacz, jak masz graf wiedzy, czy nawet masz nasz GitHub kursowy, gdzie masz pełną bazę wiedzy, czyli ten knowledge base, to ty sobie możesz w czacie spersonalizować, że ty chcesz jakąś konkretną odpowiedź na konkretne pytanie i z tego grafu eksplacujesz to, czego potrzebuje użytkownik.

Więc fajnie, że ten pan ma takie podejście, że dodał jakieś tam dodatkowe źródło wiedzy w formie Reddita, ale co do logiki tego przedsięwzięcia, które stoi za każdą wyszukiwarką na świecie, to się totalnie nikt nie zmienia.

Tylko po prostu źródło dochodzi nowe, no i potencjalnie można by się zastanawiać, że tam pisze użytkownik, a może jest jakaś informacja, a może jest trochę troli, a może jakichś celowych działań typu Parasite SEO, co tam się dzieje.

Więc jakby no totalnie nie wiem, co to miało na myśli, znaczy wiem, co miało na myśli, ale to jest porównywanie owoców morza do owoców.

Ja bym pewnie się zgodził z takim statementem, że takie SEO, jak my je rozumiemy, to w ostatnich pięciu latach się działo, to pewnie ono mocno jakby dostanie po dupie i tutaj jakby zasady ekonomii wchodzą w gry. W sensie będzie je sens robić, o ile nie będziesz robił tego droga, więc musisz znaleźć rozwiązania, żeby robić to tanie. I wtedy nadal będzie to opłacalne.

Jeżeli będziesz robić to metodami, które masz dostępne teraz w świecie za trzy lata, w którym ten ruch może być połowę mniejszy, no to to będzie tworzyć istotny problem.

Ale tak, Bartku, przyznam, że nie wiem, czy dokładnie odpowiedział na to pytanie, bo szczerze to nie rozumiem za bardzo trochę intencji tego gościa.

Może zabrawa jakichś szczegółów. Nie wiem, czy ktoś może tutaj z uczestników byłby w stanie ewentualnie pociągnąć odpowiedź na to pytanie, ale ja jestem w stanie się zgodzić co do statementu, że takie klasyczne SEO umrze, jeżeli nie dostosujemy się ekonomicznie do tego, czyli nie zmniejszymy istotnie kosztów operacyjnych tego.

Damian, wtrącę tutaj właśnie dwa zdania w temacie tego reddita.

Od dłuższego czasu, z półtorej roku, spotykam się właśnie z takimi wypowiedziami gdzieś na zachodnich, czy to czaterach, czy różne webinary, że Reddit promowany będzie i właśnie, że to jest mega kierunek.

Natomiast to się pojawia tylko i wyłącznie właśnie w Stanach Zjednoczonych i w mojej ocenie tamtego rynku, tak jak obserwuję, jak zachowują się użytkownicy internetu, to wynika właśnie z zachowania użytkowników internetu, gdzie w Stanach Zjednoczonych oni mają specyficzne zachowania w kierunku stron, które generują małą ilość treści, a szybki przekaz informacji.

Tam jest bardzo popularny właśnie Reddit, do tego korzystają z Jelpa, jeśli chodzi o szukanie pewnych informacji, chociażby docenialnicznych, bardzo dużo jest zachowanie również w Pintereście i tutaj słyszałem ostatnio też sporo informacji, żeby pozycjonowanie opierać o wstawiane piny w Pintereście, to ma bardzo duży wpływ, ale to działa tylko i wyłącznie właśnie na rynku amerykańskim ze względu na specyfikę tamtejszych użytkowników.

Oni po prostu lubują się w tego typu miejscach, żeby stamtąd te informacje pozyskiwać, więc Google też stamtąd de facto pozyskuje te informacje. Myślę, że to jest bardziej odpowiedź tutaj tego rozmówcy, z którym Bartek rozmawiał, to była po prostu kwestia jego poglądu na jego podwórko amerykańskie, jak to wygląda, a nie globalnie na samo SEO.

Okej, ja też słyszałem, że chyba Google inwestował, czy miał zainwestować w Reddita, bo Reddita miał ostatnio IPO.

Może to jest jakoś powiązane?

Coś takiego było, ale to tylko jakaś lekka informacja się przewinęła i nie widziałem rozwinięcia tematu.

Też była informacja, że Google płaci Redditowi za ściąganie dania od nich, wydaje się, do searchu i do AIO-RB.

Jak sobie weźmiemy na przykład jak teraz Google jest zaśmiecony tekstami AI, to Google prawdopodobnie szuka nowych źródeł danych do trenowania lepszych wersji modeli, a takim źródłem danych fajnym prawdopodobnie wydaje się Reddick, więc naturalnie będzie się napędzać widoczną średnita w wynikach uszukiwania.

Dobra, dzięki Panowie za odpowiedź.

Mam świadomość, że muszę sam doprecydować sobie to, co miałem tam w głowie, ale dzięki za odpowiedź.

Jeszcze możemy do tego wrócić, jak możesz go dopytać ewentualnie, biorąc pod uwagę to, co my powiedzieliśmy, to może on Ci doprecyzuje, będziemy w stanie lepiej zrozumieć intencję tego zapytania.

Dobra, to może przejdźmy w takim razie do Grzegorza, który miał pytanie o linki, zanim przejdziemy do Mateusza, którego pytanie pojawiało się przed chwilą. Grzegorz, jesteś jeszcze z nami, czy mógłbyś powtórzyć to pytanie, które miałeś?

Jeśli ja mogę pomóc, to chodziło o linki do follow, no follow z serwisów typu radio.

chyba mocno domena.

Tak, ale chyba z nami Grzegorza nie ma, więc nie wiem, czy...

To może...

To jak Grzegorz się pojawi, to Grzegorz prosimy o to pytanie ponownie, bo chyba...

Była taka mowa, że odpowiem na to pytanie na reddicie.

Odpisam nawet na czacie, że na reddicie, okej.

Nie na reddicie, ale właśnie reddit, na discordiu.

Okej, dobra, to Mateusz, proszę.

Siemonko Wam.

Wiecie co, ja odnośnie, W sumie tego reddita bardziej chciałem dodać, że to jest fajne źródło do pozyskiwania właśnie takich mega niszowych tematów.

Jakiś czas temu gadałem nawet z jednym ziomkiem, który pozyskiwał stamtąd tematy z tej branży porno i jakby powiedział, że naprawdę zajebiście to żarło, bo pozyskiwał właśnie takie mega niszowe tematy, więc myślę, że tutaj akurat proces pozyskiwania nasz w Polsce czy w USA jest bardzo zbieżny, bo my jakby samymi chociażby, nie wiem, dajemy na to MBDGami czy samym radiem staramy się po prostu mega to zaprecyzować jakby do zapytania jakiegoś konkretnego usera czy jak najbardziej pokryć jego intencje.

To jest to samo tak naprawdę, co robią na reddicie, że starają się wybadać ewentualnie jaka jest potrzeba usera.

A jeśli chodzi o zmianę, to co powiedziałeś, że user nie będzie chciał wejść i czytać 8 tysięcy znaków, no to jasne, jakby przy prezentacji produktów najważniejszy jest UX, jakby patrzeć na to, żeby guzera zainteresować produktem, a nie to, żeby czytał opis tego produktu.

Więc jakby tutaj myślę, że pod tym kątem na pewno to będzie w sumie już dzisiaj tak naprawdę jesteśmy świadkami tego, że musimy odpowiednio jakby dostosować bezpośrednio sam produkt, żeby użytkownik go kupił, bo tam też pojawiła się w sumie taka adnotacja odnośnie tej ściany tekstu.

W sumie tylko tyle chciałem dodać.

Dzięki Mateusz za komentarz.

Dzięki Mateusz.

Słuchajcie, minęło półtorej godziny, więc chyba tutaj postawimy kropkę.

Dziękujemy wszystkim za udział bardzo.

Teraz w niedzielę Mateusz wrzuci tydzień numer dwa i tutaj też od razu komunikat do osób, które brały udział w poprzednim kursie. Ten materiał będzie dość nowy, bo to jest tydzień, którego nie było w poprzedniej edycji i wiele nowych zagadnień się tam pojawia i już zaczyna się jakiś bardziej w sensie głupiego dosałożyłem.

Bardziej techniczna wiedza, więc przygotujcie, może nie oglądajcie tego od 23 zmęczeni, tylko trochę na świeższej głowie.

Dobra, dzięki Robert. Coś jeszcze z tym powiedzieć?

Tak, w międzyczasie zamykając klamrą manusową rozwinąłem, szanowni Państwo, naszą aplikację.

Być może będziemy ją wykorzystywać.

To jest ta wersja? Chyba ta.

Spraśćmy.

zobacz Damian dodaliśmy łapki w górę i możemy sobie odznaczać odpowiedzi na pytania czego nie mamy na slido więc jeszcze chwileczkę i może przeskoczymy na to w kolejnych live'ach super miłego wieczoru wszystkim, życie do zobaczenia za tydzień czwartek o 18 i na Discordzie do zobaczenia, dziękujemy, cześć dzięki, cześć wam cześć cześć, dzięki dzięki, cześć

