# Transkrypcja lekcji: Pamięć długoterminowa modeli językowych i reranking

Cześć, w tej lekcji mówimy sobie temat pamięci długoterminowej modeli językowych i takim skrótem, który nazywamy RAC, czyli Retrieval Augment Generation. I to jest kolejny zestaw takich nowych pojęć, które są kluczowe w kontekście zrozumienia tego, jak działają modele językowe, ale też jak działają nowoczesne wyszukiwarki, bo nowoczesne wyszukiwarki utylizują wszystkie te metody, o których sobie tutaj powiemy i będziesz mógł z tych metod korzystać, będziesz mogła z tych metod korzystać, żeby wzmocnić swoje procesy.

No więc przechodząc do pamięci długoterminowej, generalnie ukuło się coś takiego, przeświadczenie pewnego rodzaju, że modele potrafią zapamiętywać jakieś informacje. Otóż jest to nieprawda. Modele czegoś takiego nie potrafią. Modele są po prostu trenowane, wypuszczane i w takiej formie po prostu możemy z nich korzystać. One nie aktualizują swoich informacji na bieżąco. Przynajmniej na razie. Przynajmniej na razie technologia na to nie zezwala i nie umożliwia tego. Nie wiem jak będzie w przyszłości, natomiast w tym momencie po prostu pamięć długoterminowa nie istnieje. To jest pewien skut myślowy, który odnosi się do pewnego rodzaju technologii i procesów, które pozwalają zasilić model językowy kontekstem, w ten sposób wzmocnić powiedzmy jego pamięć.

No więc możesz wiedzieć, że na przykład chat GPT promuje coś takiego, że posiada pewnego rodzaju wbudowaną pamięć. No i tak naprawdę to nie działa tak, że model po prostu zapamiętuje wszystkie nasze konwersacje i zawsze jest w stanie do nich wrócić, tylko pewnego rodzaju dokonuje procesu przetwarzania tych konwersacji i zapisywania pewnych kluczowych informacji. I kiedy nasze aktualne konwersacje skojarzą się z poprzednimi zapisanymi tymi informacjami w bazie tego modelu, znaczy nie w bazie modelu, tylko w zewnętrznej bazie jakiejś, to po prostu są te informacje ładowane do modelu jako kontekst.

Tutaj widzimy na przykład, co chat GPT o nas wie. Zresztą przejdziemy sobie do chatu GPT i pokażę Ci to. Zobacz, możemy wpisać taki prompt, I jak uruchomimy ten prąg, musisz tu wybrać model GPT-4O, ponieważ w przypadku GPT-3 O-3 nie będzie to działało. Więc zobacz, to jest podsumowanie wszystkich informacji, które czat GPT o mnie zebra, czyli moje imię i nazwisko, moja lokalizacja, preferowany format odpowiedzi, zainteresowania, moje ekspertyza, moje projekty, tematy, którymi się interesuje itd. Generalnie ChatGPT ma taką po prostu możliwość, żeby do tych konwersacji, które dotyczyły tych konkretnych elementów wracać, natomiast ma też zawsze tego JSON-a na podorędziu, więc jest w stanie nawiązać np. do moich zainteresowań itd. Natomiast to nie jest tak, że model GPT 4.0 ma te informacje zapamiętane, to jest jakiś zewnętrzny proces, który po prostu przywołuje te informacje na potrzeby danej odpowiedzi.

I wracając do prezentacji, tak jak powiedziałem, modele językowe mają coś takiego jak Knowledge Cut-Off i każdy model językowy jest w ten sposób oznaczony, to na przykład mamy Gemini 2.5 Flash Preview z 20 maja i on ma Knowledge Cut-Off na January 2025, czyli styczeń 2025 roku, więc wiemy, że ten model posiada wiedzę do stycznia 2025 roku, więc nie będzie w stanie z nam powiedzieć żadnej informacji, która po tej dacie powstała, to znaczy może próbować, bo tak jak wiesz modele mogą halucynować, natomiast jedyne co będziemy mogli zrobić to dostarczyć mu tę informację jako kontekst i wtedy na podstawie tego kontekstu on będzie w stanie na tę informację odpowiedzieć.

No i tutaj właśnie pojawia się pojęcie Retrieval Augment Generation, czyli RAG, czyli to jest ta technologia, która pozwala wtłaczać odpowiedni kontekst do modelu językowego, który później pozwala modelowi językowemu bazować na informacjach, które mu dostarczyliśmy. No i to jest szerokie pojęcie pod parasolem, którego można schować różne tak naprawdę rzeczy. Zarówno ta pamięć czata GPG wykorzystuje w jakiś sposób technologię RAC, zarówno chatboty wykorzystują technologię RAC i dużo, dużo innych narzędzi i technologii tą technologię RAC wykorzystuje.

I jak taki proces RAC-owy działa? No więc wyobraź sobie, że posiadasz jakiś zbiór dokumentów, na przykład są to PDF-y, JSON-y, obrazki i tak dalej, i tak dalej. No i to jest powiedzmy twoja taka baza wiedzy, załóżmy, że to jest coś, co chciałbyś, żeby model zapamiętał, żeby to było pamięcią modelu językowego. Więc w takiej technice RAC stosuje się takiego jak chunking, czyli dzielenie dokumentów na mniejsze kawałki. No i to wynika z tego, że po pierwsze modele embeddingowe, które później służą do tego, żeby dopasowywać te chunking, one zawodzą przy długich tekstach, ponieważ ten powiedzmy kontekst cały się rozmywa. Jak jest długi tekst, to modelem bedingowy stara się go uśrednić do jakiegoś wielowymiarowego wektora, co zawsze powoduje, że to powiedzmy semantyczność się trochę spłaszcza i ta powiedzmy wieloznaczeniowość tego tekstu się rozmywa. Więc po prostu ładujemy, więc w pierwszym elemencie procesu RUG dzielimy duże kawałki tekstu na mniejsze tak zwane czanki. To samo robi Google. Jeżeli Google crawluje twoją stronę, to nie jest tak, że w całym swoim indeksie przechowuje jako w jednym polu cały twój artykuł, tylko dzieli go na kawałki, według jakiegoś swojego procesu.

Czyli na przykład, załóżmy, mam jakąś PDF-a, który zawiera jakieś instrukcje na temat, albo zawiera jakieś pytania i odpowiedzi na temat senuto. Więc tego PDF-a po prostu dzielę na przykład na 30 mniejszych części. No i każde z tych małych części, przy wykorzystaniu modelu embeddingowego, zamieniam na wektory. No i właśnie transformuję taką wiedzę na wektory. Następnie te wektory zapisuję do bazy wektorowej. Na przykład przy wykorzystaniu, nie wiem, subabase czy kwadranta, o którym sobie mówiliśmy.

I w praktyce jak już potem wykorzystuję sobie te czanki, czyli zbudowałem bazę wiedzy, która zawiera jakieś informacje, które chciałem, żeby model zapamiętał, znaczy on ich nie zapamiętuje, tylko za pomocą właśnie tego procesu jestem w stanie mu je dostarczyć. I teraz mamy tutaj taki proces, wyobraź sobie, że to jest jakiś chatbot albo na przykład wyszukiwarka, no i użytkownik zadaje pytanie, na przykład ile kosztuje pakiet Basic w Senuta? No i to pytanie też właśnie zamieniam na wektory, czyli wektoryzuję za pomocą modelu embeddingowego, a następnie dokonuję wyszukiwania w bazie wektorowej, co zresztą robiliśmy w jednym z collabów, czyli wyszukuję takie czanki kontentu, które z dużym prawdopodobieństwem zawierają jakąś odpowiedź na to pytanie i załóżmy, że ta odpowiedź zawierała się w jakimś PDF-ie, powiedzmy w jego połowie, ale tylko ten konkretny czank odpowiada na to pytanie. Więc najczęściej w takim po prostu procesie w bazie wektorowej wybieramy kilka czanków, które potencjalnie zawierają odpowiedź na to pytanie.

Następnie wykonujemy prompt do modelu językowego i ten prompt wygląda mniej więcej tak. Okej, tu są kilka, tu jest kilka czanków, które, tu jest pytanie użytkownika, które brzmi, ile kosztuje pakiet Basic Senu, to odpowiedz na to pytanie w oparciu o te kawałki treści, które powiedzmy ci dostarczam. No i wtedy model po prostu ma te kawałki treści i na podstawie tego wnioskuje i udziela odpowiedzi. No i model tak naprawdę nie posiada tej wiedzy, To znaczy nie wiedział ile kosztuje ten bucket basic w sumie. To być może wiedział, ale na przykład jakbyśmy zmienili cennik po tym, powiedzmy, momencie, kiedy jego knowledge cut-off występuje, no to by tego nie wiedział. Natomiast jako, że mamy ten proces ragowy, dostarczyliśmy mu tą informację i on jest w stanie dowiedzieć, więc LLM wtedy odpowiada.

I to jest taki schemat działania, powiedzmy, takiego chatbota. Natomiast musisz wiedzieć, że Google też poniekąd staje się chatbotem takim, który udziela odpowiedzi i trochę konwersuje z użytkownikiem. Zwłaszcza w przyszłości będziemy mogli mocniej zaobserwować ten kierunek rozwoju wyżki barki.

I jest jeszcze pojęcie, które chciałbym tutaj wprowadzić, które też jest kluczowe z punktu widzenia rozumienia procesów fragowych i w ogóle rozumienia tego, jak działa wyszukiwarka. Jest też coś takiego jak proces rerankingowy. To znaczy, tak jak powiedziałem, że czasem embeddingi mogą zawodzić, ponieważ przy dłuższych tekstach albo przy powiedzmy bardziej skomplikowanym kontekście albo przy bardzo podobnych tekstach możemy mieć problem z rozróżnieniem tego, co zawiera odpowiedź na nasze pytanie. To znaczy, że jak mam na przykład dwa długie teksty, zamienię je na wektory za pomocą modele embeddingowego, no i model embeddingowy postara się je po prostu uśrednić do jakiegoś wektora. No i to porównanie może nie być takie powiedzmy skuteczne.

Dlatego najczęściej embeddingi w procesach ragowych wykorzystuje się do takiego szybkiego wyszukiwania takich najbardziej dopasowanych dokumentów. A później jeszcze używa się modelu rerankingowego, czyli takiego modelu, który zawiera trochę więcej elementów z modelu językowego, czyli zawiera powiedzmy model embeddingowy, to jest tylko ten pierwszy etap modelu językowego, Model rankingowy zawiera też jeszcze powiedzmy ten enkoder, dekoder i nie generuje żadnej odpowiedzi, ale jest wytrenowany do tego, żeby szeregować po prostu dokumenty według powiedzmy dopasowania do zapytania. No i on po prostu bardziej rozumie ten cały kontekst, bardziej rozumie znaczenie tego, z czym mamy do czynienia.

I taki model re-rankingowy też stosowany jest w wyszukiwarce. Nie wiem, czy pamiętasz, ale było coś takiego jak neural matching zdaje się w 2017 lub 2018 roku i wtedy podniosło się taki powiedzmy rumor w branży SEO, ponieważ wszyscy zaczęli twierdzić, że Google nie będzie używał linków do ustalenia rankingu, ponieważ Google zakomunikował, że jest w stanie ustalać ranking tylko na podstawie treści strony internetowej.

No i co do zasady jest to prawda, ponieważ powiedzmy jest coś takiego jak Initial Ranking i powiedzmy, że Google nie wykorzystuje tutaj akurat być może embeddingów, natomiast jest jakiś taki ranking inicjalny, który układa Google w oparciu o wszystkie swoje czynniki, czyli na przykład ustala ten ranking 10 najlepszych wyników, ale później może zrobić re-ranking tylko w oparciu o same treści, wykorzystując taki model re-rankingowy.

Więc rzeczywiście może być tak, że dojście do top 10 wymaga wszystkich spełnienia wszystkich wymagań przez Google natomiast to, na którym miejscu jesteśmy już w top 10 może wyłącznie zależeć od tego jaką treść mamy natomiast nie chciałbym, żebyś uznał to jako taką prawdę objawioną być może tak jest, być może tak nie jest tego do końca nie wiemy, nie wiemy też czy Google nadal używa neural matching być może ma jakąś bardziej zaawansowaną technologię tylko to był taki początek używania modeli rankingowych przez Google i to powiedzmy początkowy etap.

Więc odwołując się na przykład do chatbota, model na przykład, wracając do tego pytania, ile kosztuje pakiet Basic Zenuta, to powiedzmy model dostarczył, to retrieval na embeddingach dostarczyłby na przykład 20 chunków, które zawierają odpowiedź, a model rankingowy mógłby jeszcze je uszeregować według tych, które tą odpowiedź na 100% zawierają.

No i po co komu wykorzystywać tę metodę retrieval algorithm generation? No generalnie jest kilka ważnych elementów, na które należy zwrócić uwagę. Po pierwsze to ile danych możemy przesłać w jednym prompcie jest ograniczone przez liczbę tokenów. I aktualnie nowoczesne modele mają okno kontekstowe nawet na 10 milionów tokenów. Co oznacza, że możemy tam załadować bardzo dużo wiedzy. W przeliczeniu na książki będzie to kilkadziesiąt książek w jednym prompcie. I wiele osób jakby wieszczy w związku z tym taką koniec wykorzystywania tej metody RAG, ponieważ nie musimy już się martwić o to, o to kontekstowe, więc możemy całą wiedzę zawsze ładować do modelu.

Natomiast to nie jest do końca prawda, ponieważ zazwyczaj możemy mieć wiedzę bardzo długą, na przykład wiedza o Senuto, o wewnętrznych procesach i tak dalej, pewnie zawiera więcej nawet niż 10 milionów tokenów, gdybym wszystko to zebrał. A poza tym należy pamiętać, że za każdy token też płacimy. To znaczy, że za każdy token pobiera od nas dostawca modelu językowego opłatę i wyobraź sobie, żebym tworzył chatbota dla Senuto i nawet korzystał z modelu, który ma autokontekstowe 10 milion tokenów. Jeżeli użytkownik zapytałby mnie o cenę pakietu Basic, to bym za każdym razem ładował mu na przykład 60 książek wiedzy i kazał na podstawie tego odpowiedzieć, co by już po prostu nie miało ekonomicznego uzasadnienia, ponieważ wymagałoby to po prostu wysokich nakładów finansowych. Każde pytanie kosztowałoby na przykład kilkadziesiąt centów, żeby na nie odpowiedzieć. Natomiast w przypadku procesu ragowego ten koszt na przykład możemy zniwelować do 99%.

Co też jeszcze bardzo ważne jest, to tak jak też tłumaczyłem już pewnie w tygodniu numer dwa albo trzy, to modele językowe często mają skłonność do pomijania informacji w przypadku, kiedy dostają dużo tych informacji. Czyli jakbym na przykład modelowi załadował na przykład 60 PDF-ów, to pomimo, że jego okno kontekstowe no to zezwala, to on mógłby ich nie przetworzyć, po prostu gdzieś mógłby się zgubić w połowie albo na przykład stracić fokus. Więc po prostu zawsze wykorzystujemy rak do tego, żeby dostarczyć taki kontekst, jakiego model aktualnie potrzebuje do wykonania swojego zadania.

Nie wiem, czy pamiętasz serial Black Mirror i śmieszne jest to, że pewnie jeszcze kilka lat temu patrzyłem na ten serial jak na science fiction, a teraz wszystko nagle się urzeczywistnia i wydaje się, że ten serial nie był żadnym science fiction, tylko poniekąd przewidział naszą przyszłość. Natomiast był tam taki odcinek, gdzie ludzie mieli taki chip w mózgu i on zapisywał wszystko, co ludzie robili, mówili, później oni mogli to odtwarzać na telewizorze. No i właśnie na przykład do tego można byłoby wykorzystać technologię RAC.

Wyobraź sobie, że masz takie urządzenie, które cały czas słucha tego, co mówisz, zapisuje to, co mówisz, przechowuje to w bazie wektorowej i zawsze za pomocą takiej techniki RAC możesz mieć czarbot do swojego mózgu. To znaczy zawsze możesz zapytać na przykład o czym rozmawiałem wczoraj na przykład z Mateuszem nagrywając film. Albo jesteś na super imprezie, czegoś zapomniałeś, no czasem się to zdarza, nawet najlepszym i chcesz przywołać te wspomnienia, więc po prostu jesteś w stanie zapytać i mieć taką po prostu bazę wektorową do swojego mózgu.

Myślę, że to będzie w przyszłości dość powszechnie wykorzystywane. Na podstawie tych wszystkich informacji mógłbym na przykład stworzyć jakiś, nie wiem, model mentalny mojej osoby i gdybym zapisał odpowiednio dużo moich takich informacji bieżących, na przykład nie wiem jak podejmuję decyzje, o ile jestem racjonalny, to model językowy by potrafił naśladować, powiedzmy model podejmowania moich decyzji i na przykład trochę sklonować moją osobowość w ten sposób, że mógłby zarządzać moją firmą w moim imieniu, wykorzystując historię i przeszłość moich decyzji i procesów myślech, które podejmowałem, ponieważ ludzie co do zasady są dość przewidywalni, chyba, że są szaleni, tylko złudzenie, że jest inaczej.

I nawet jest takie urządzenie, które możecie już zamówić, nazywa się OMI. Co prawda noszenie czegoś na skronie nie wydaje mi się zbyt odpowiedzialne, natomiast jest też wersja, która działa jako naszyjnik. No i właśnie ona działa w taki sposób, że za każdym razem po prostu mogę przekazać jakąś myśl do powiedzmy tego urządzenia, a tą myśl zwektoryzuję, zapiszę w bazie wektorowej, no a później mogę po prostu czatować na przykład.

Okej, nie wiem, załóżmy, że już abstrakcyjne przykłady, załóżmy, że jesteś jakimś wyznawcą filozofii buddystycznej, no to jeżeli nagrasz swoje całe życie, to możesz sobie stworzyć swojego agenta, który powiedzmy ma, nie wiem, w grane nauki jakiegoś buddyjskiego filozofa, nie wiem, czy to akurat filozofią można nazwać, no ale jest w stanie się na przykład udzielić feedbacku na temat tego, co myślałeś, albo jak sposób decyzji podejmowałeś.

Załóżmy, że interesuje cię jakiś mark, powiedzmy filozofia Marka Aureliusza, więc powiedzmy jest jakiś agent, który ma całą wiedzę Marka Aureliusza, tu jest baza wektorowa, która zabiera wszystkie twoje, powiedzmy, To, co powiedziałeś, to, co mówiłaś, to, co przetwarzałeś, przetwarzałeś jako myśli, no i jesteś w stanie komunikować się ze swoimi myślami, ale z Markiem Aureliuszem i jego powiedzmy wskazówkami co do tego, w jaki sposób należałoby myśleć.

Więc wskaczamy w po prostu w fazę science fiction i wydaje mi się, że RAG właśnie między innymi do tego w przyszłości będzie wykorzystywany. To znaczy, żeby osiągnąć taką pamięć absolutną i nawet wydaje mi się, że bez takiego urządzenia jak tutaj można osiągnąć to, o czym mówię, ponieważ mamy w ręku każdy, w kieszeni każdy telefon i przy wykorzystaniu tych narzędzi, o których będziemy tutaj mówić w kolejnych tygodniach, czyli na przykład N8N, jesteś dokładnie w stanie to osiągnąć.

To znaczy będzie można bardzo prosty sposób stworzyć automatyzację, gdzie po prostu będziesz wysyłać notarkę głosową, która będzie się zapisywać w bazie Quadrant i ona zawsze będzie tam zwektoryzowana, będziesz zawsze mógł zadać pytanie. Na przykład ja mam taki problem, zawsze jak syna odprawiam do przedszkola, zapominam kodu do bramy tam, który wymagany jest, żeby wejść, więc mogę to zapisać po prostu w bazie wektorowej jako notatkę i zawsze wysłać notatkę głosową do telefonu, żeby mi przypomniał, jaki jest kod do bramy w przedszkolu. Więc to jest jakiś taki przykład wykorzystania.

No i gdzie wykorzystuje się RAC? Dość szeroko wykorzystuje się RAC. Głównie w chatbotach i asystentach konwersacyjnych. Nie wiem, czy korzystałeś z naszej strony Sensei Academy. Tam był taki chatbot głosowy, który rozmawiał moim głosem. No i on właśnie miał wgraną bazę wiedzy na temat Sensei. Czyli ja pobrałem całą zawartość strony Sensei. Przeformatowałem za pomocą modelu językowego tą bazę wiedzy na format Q&A. No i wgrałem do tego modelu. I kiedy z nim rozmawiałeś i zadawałeś mu pytanie, no to on właśnie wykonywał cały ten proces, o którym powiedziałem. To znaczy wektoryzował twoje pytanie, w bazie wektorowej miał wszystkie te Q&A zapisane no i dobierał te odpowiedzi, które najlepiej pasowały do tego pytania, które zadałeś i na podstawie tego generował model językowy, transkrypcję tego, co ma ten model głosowy powiedzieć i w ten sposób się wypowiadał.

Tak samo działają chatboty, to znaczy jeżeli korzystasz z jakiegoś chatbota, jakiejś firmy, to prawdopodobnie jest tam jakaś baza wektorowa z jakąś wiedzą wgraną, podzieloną i wgraną i jak zadajesz pytanie, ono jest wektoryzowane, następnie z tej bazy wiedzy ekstraktowane i na podstawie źródeł z tej bazy wiedzy po prostu udzielana jest Tobie odpowiedź.

No, wyszukiwarki semantyczne i to jest kluczowe, bo Google też przy korystaniu z AI Overview czy za chwilę z AI Mode też wykorzystuje system ragowy i jeżeli będziemy umieli wszystkie te koncepty połączyć w swojej głowie i tak naprawdę wykorzystać je, to będziemy w stanie na przykład za pomocą inżynierii wstecznej trochę odtworzyć to, jak działa AI Mode i AI Overview, co zresztą postaramy się uczynić do tych wyszukiwarek semantycznych też się dopasować.

Generowanie treści. No i tutaj opinii jest wiele, nawet moja i Roberta się różni, więc będziesz w stanie poznać różne perspektywy. Ja już nie wykorzystuję raga przy generowaniu treści, natomiast w przypadku, kiedy bym miał jakiś taki duży serwis, na przykład Onet i tam robił system do generowania treści, to bym ten raga wykorzystywał, bo wszystkie artykuły poprzednio Onetu, wszystkie fakty na temat różnych osób, wszystkie jakieś wewnętrzne dokumenty mogłyby wykorzystane być do tego, żeby uzupełnić jakąś wiedzę z artykułu.

Natomiast jeszcze dwa lata temu, co w modelach językowych jest, można mówić o tym w latach świetlnych, te modele językowe miały okno kontekstowe na 8 tysięcy tokenów i to suma była wejścia i wyjścia. Więc wykorzystywaliśmy systemy ragowe do tego, kiedy pisaliśmy na przykład treść na temat, na przykład jakiegoś nagłówka, no to nie byliśmy w stanie załączyć całego kontekstu naszej konkurencji na ten temat, tylko właśnie wykorzystywaliśmy system ragowy do tego, żeby wyciągnąć z treści konkurencji taki kontekst, taki, który pasuje aktualnie najbardziej do danego nagłówka.

W tym momencie, powiedzmy, można pominąć ten proces w tym procesie generowania treści, ponieważ te modele, na przykład Gemini 2.5, one mają okno kontekstowe na milion wierszy, na milion tokenów, więc zazwyczaj po prostu możemy ten proces pominąć. Koszt jest tak niski, że w zasadzie pomijalny, więc ten rag nie znajduje aż takiego zastosowania, a czykolwiek możesz go zastosować, jeżeli posiadasz na przykład jakąś dużą witrynę, duży zbiór informacji o produktach na przykład w e-commerce, no to znajdzie to zastosowanie, natomiast jeżeli chcesz taki uniwersalny proces do generowania artykułów wytworzyć, który będzie bazował na tym, co w wynikach wyszukiwania się znajduje, to w zasadzie tego procesu ragowego już nie potrzebujesz, zwłaszcza, że modele rozwijają się w takim kierunku, że po pierwsze będą w stanie przetwarzać więcej danych w oknach kontekstowych.

Zazwyczaj mogę założyć się, że do końca roku wszystkie topowe modele będą miały okno kontekstowe o wielkości 10 milionów tokenów, a w przyszłym roku jeszcze więcej. Być może w przyszłości nawet ono nie będzie w żaden sposób ograniczane. Druga bardzo ważna informacja. Modele coraz lepiej radzą sobie ze skupieniem. To znaczy, że kiedyś prawdopodobnie gdybyśmy wrzucili dużo danych do okna kontekstowego, prawdopodobieństwo pominięcia informacji byłoby znacznie wyższe niż aktualnie jest. No i cena, ona też idzie, dąży do zera. W przyszłości modele językowe co do zasady będą praktycznie darmowe, więc te procesy reagowe nie będą aż tak kluczowe, natomiast aktualnie są jeszcze bardzo ważne.

Natomiast nawet jeżeli nie będziesz ich wykorzystywać, to warto, żebyś je rozumiał, rozumiała, ponieważ Google je będzie wykorzystywał i w ten sposób będzie generował swój ranking, bo ranking Google'a jest tak duży i indeks Google'a jest tak duży, że on zawsze będzie potrzebał tych procesów.

I asystenci osobiści, wydaje mi się, że właśnie system ragowy świetnie się będzie odnajdywał w takich systemach, o których powiedziałem, na przykład tak jak w tym produkcie OMI.

Z takich najpopularniejszych narzędzi, które możesz skojarzyć z systemem ragowym, to jest na przykład Notebook LM. Właśnie on wykorzystuje system ragowy do tego, żeby podpowiadać. Czyli sprytny sposób wykorzystania modelu językowego. Tak naprawdę notebook LM to jest model językowy, system ragowy i przyjazny interfejs. No bo co robi notebook LM? Wrzucasz mu na przykład 50 PDF-ów, jak zadajesz mu pytanie, to on jest w stanie wyekstraktować wiedzę z tych PDF-ów właśnie wykorzystując te metody.

Więc najbardziej taki przykład, który przychodzi do głowy, drugi najbardziej przykład, który przychodzi do głowy to są modele GPT, czyli GPT-sy tak zwane, czyli jak tworzysz customowe GPT-s i tam możesz wgrywać pliki, na przykład do tych plików można wgrać serię PDF-ów, to kiedy zadajesz pytanie do tego GPT-su, to właśnie też się odbywa proces ragowy. To znaczy, że OpenAI właśnie twoje pliki, które dasz do GPT-sa zapisuje w jakiejś bazie wektorowej i każde twoje pytanie też jest wektoryzowane i właśnie następuje ten retribal z tych PDF-ów, tych informacji, które aktualnie tobie są potrzebne do tego, żeby na to pytanie odpowiedzieć.

I jak wdrożyć rag, bo jest wiele metod na to. Pierwsza najprostsza wydaje się to jest API od OpenAI i jest coś takiego jak Assistant API i to jest taki rodzaj API, który właśnie służy do tego, żeby tworzyć asystentów, czyli powiedzmy dzięki temu API możemy stworzyć na przykład powiedzmy chatbota, ale też dużo różnych pewnie narzędzi można było na podstawie tego stworzyć, na przykład można by było stworzyć narzędzie, które było takim alternatywnym interfejsem dla aplikacji jakiejś Senuto na przykład, ponieważ to asystent API ma możliwość korzystania z funkcji, ma możliwość wyszukiwania aplików, interpretowania kodu. Jeżeli włączymy te wszystkie funkcje to może to działać tak, że ten asystent ma dostęp do API Senuto i jeżeli w API Senuto opiszemy wszystkie endpointy, które mamy i wypiszemy do czego one służą, włączymy ten file search, czyli gramy mu bazę wiedzy na temat tego co Senuto robi i jeszcze włączymy mu code interpreter, to do tego asystenta możemy wysłać takie zapytanie, okej, wybierz mi wszystkie słowa kluczowe, na które widoczne jest onet. No i on przeszuka tą dokumentację API, wybierze ten endpoint, który na podstawie opisu prawdopodobnie udzieli tej odpowiedzi, wykona funkcję, czyli odpyta to API, to API odpowie z tymi danymi, a dzięki temu, że mamy kod interpreter, to jeszcze mogę napisać, wybierz te słowa kluczowe, ale jeszcze je zkategoryzuj. Więc on jeszcze napisze skrypt Pythona, który wykona tą operację i w praktyce zwróci te dane. Więc moglibyśmy stworzyć takiego inteligentnego asystenta SEO w oparciu właśnie o to API.

I pokażę Ci, jak ono w praktyce działa, jak działa w najprostszy sposób. Musimy po prostu przejść do playground. Pierwszym krokiem, jaki musimy wykonać, żeby zrobić asystenta OpenAI, to jest utworzenie VectorStore, czyli bazy wektorowej. I tutaj po prostu płacimy OpenAI za hostowanie tej bazy wektorowej 10 centów za gigabyte za dzień. Czyli otworzymy wektor store sensor AI 0.3.0 i klikamy create assistant tutaj, czyli otworzymy w tej bazie wektorowej asystenta i mamy tutaj ułożony untitled assistant, więc przechodzę sobie do zakładki asystentu. On tutaj się nam pojawił na górze. Tutaj możemy nazwać go sensor AI 3.0. Tu możemy nadać mu tą instrukcję systemową. Na przykład jesteś inteligentnym asystentem senuto, odpowiadać na pytania użytkowników i tak dalej, i tak dalej. Tutaj wgrywam mu plik, który chce, żeby podzielił na czanki i na bazie tego pliku dokonywał tego retribalu. Tutaj akurat wgrywam mu dokument naukowy attentional unit, czyli tego artykułu naukowego na temat technologii transformerów. Mogę też wybrać, jaki jest chunk size i chunk overlap, więc mam jakąś kontrolę nad tym, jak on podzieli ten dokument, natomiast nie taką dokładną. Mogę wybrać różne ustawienia modelu, tak jak zawsze. I tutaj mogę to przejść do playgroundu, czyli już wykorzystywać tego asystenta. I mamy tutaj to, że on ma file search, mógłbym tutaj właśnie code interpreter i tak jak powiedziałem tą funkcję uruchomić. Natomiast zadam mu pytanie, what is encoder? Tak w zasadzie też działa notebook.lm, tak jak tutaj ten asystent, tylko tutaj możemy jeszcze wykonywać funkcje. Mamy do tego API, więc jesteśmy w stanie stworzyć tego, powiedzmy, asystenta firmowego.

To, czego nie mamy tutaj, to nie mamy kontroli nad tym, jak on do końca dzieli te pliki, ani co w tej bazie wektorowej jest, ani jak ona działa i tak dalej. Natomiast to jest takie rozwiązanie trochę no-brainer, ponieważ po prostu tutaj to konfigurujemy w panelu i prostym endpointem API jesteśmy w stanie się komunikować.

Ten cały proces reglowy nas interesuje. I widzisz, że na podstawie tego dokumentu udzielił odpowiedzi. Ta odpowiedź co do zasady jest taka sama, jak znajduje się w tym dokumencie, więc możemy to wgrać. Mamy tutaj możliwość komunikowania się przez API z tym konkretnie asystentem, więc to jest takie najprostsze rozwiązanie.

Są też inne rozwiązania oczywiście. Mamy te narzędzia, o których będziemy mówić w kolejnych tygodniach. N8N, Make i DeFi. W tych wszystkich narzędziach można utworzyć proces ragowy. W przypadku N8N i Make to jest trochę tak, że musisz zrobić to samodzielnie, to znaczy musisz samodzielnie mieć jakąś usługę bazy wektorowej, na przykład konto w quadrancie, wtedy je tam podłączamy. W przypadku DeFi przychodzi już z zainstalowaną bazą wektorową i z wszystkimi tymi funkcjami reagowymi, więc jeszcze będziemy to przerabiać.

Więc mamy powiedzmy taką no-brainer, czyli usługę Assistant AI od OpenAI, gdzie możemy po prostu wgrać pliki, łączyć się z nimi przez API i zrobić jakiś prosty interfejs. Mamy usługi no-code, które będziemy omawiać. No i możemy po prostu też stworzyć to sami samodzielnie w oparciu o jakiś powiedzmy język na przykład Python i stworzyć jakiś frontend na przykład w jakimś Next.js więc dróg do tego, żeby system ragowy zaprojektować jest wiele natomiast nie jest to nic takiego skomplikowanego będziemy jeszcze to robić i właśnie w kolejnej lekcji zajmiemy się trochę praktyką ale pod kątem tego, żeby pokazać tobie w jaki sposób działa model rankingowy ponieważ chciałbym, żeby nasz model mentalny budował się jak klocki Lego po kolei więc tutaj wprowadzimy nowe pojęcie modelu rankingowego, żebyś zobaczył, zobaczyła jak ten model rankingowy potrafi wpływać na wyniki.

Natomiast przy okazji omawiania AI Overview i AI Mode już ten cały proces ragowy sobie powiedzmy w ramach jakiegoś notebooka zaprojektujemy tak, żebyś mógł, mogła go spokojnie zrozumieć. Ja też wiem, że prawdopodobnie albo skorzystać z Assistant AI, albo z tych narzędzi NoCode, więc tam się skupimy, żeby go wdrożyć produkcyjnie. Tu się skupimy w tym tygodniu, żeby zrozumieć go, jak działa mechanika i jak wszystkie te procesy zachodzą.

I w tej lekcji to wszystko. Dziękuję i zapraszam Cię do kolejnej. 